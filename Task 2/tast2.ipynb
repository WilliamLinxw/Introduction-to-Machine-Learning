{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed24906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature of the training set\n",
    "def get_features(filename):\n",
    "    features = pd.read_csv('./'+filename)\n",
    "    print(features.head(12))\n",
    "    # train = train.sample(frac=1).reset_index(drop=True)\n",
    "    # print(train.head())\n",
    "    return features\n",
    "\n",
    "# Get the labels of the training set\n",
    "def get_labels(filename):\n",
    "    labels = pd.read_csv('./'+filename)\n",
    "    print(labels.head(12))\n",
    "    # train = train.sample(frac=1).reset_index(drop=True)\n",
    "    # print(train.head())\n",
    "    return labels\n",
    "\n",
    "# Get the features of the testing set\n",
    "def get_test_feature(filename):\n",
    "    test_features = pd.read_csv('./'+filename)\n",
    "    print(test_features.head(12))\n",
    "    # train = train.sample(frac=1).reset_index(drop=True)\n",
    "    # print(train.head())\n",
    "    return test_features\n",
    "\n",
    "train_features = get_features('train_features.csv')\n",
    "labels = get_labels('train_labels.csv')\n",
    "test_features = get_test_feature('test_features.csv')\n",
    "print(train_features.shape)\n",
    "print(labels.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.drop(columns = ['pid', 'Time'])\n",
    "test_features = test_features.drop(columns = ['pid', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60516980",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ec997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average multiple tests on one patient in the training set\n",
    "average_train_features = train_features.groupby(np.arange(len(train_features))//12).mean()\n",
    "print(average_train_features.shape)\n",
    "\n",
    "# Check now how many are still NaN\n",
    "for index, row in average_train_features.iteritems():\n",
    "    print(average_train_features[index].isnull().sum())\n",
    "average_train_features.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3542c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average multiple tests on one patient in the test set, also impute it using the same KNN imputer\n",
    "average_test_features = test_features.groupby(np.arange(len(test_features))//12).mean()\n",
    "print(average_test_features.shape)\n",
    "average_test_features.head(12)\n",
    "\n",
    "# Check now how many are still NaN\n",
    "for index, row in average_test_features.iteritems():\n",
    "    print(average_test_features[index].isnull().sum())\n",
    "average_test_features.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since the HistGradBoosting method can handle the missing data and with better performance, skip the imputation part and\n",
    "jump directly to the cell without imputation \"# Not imputed\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f721970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KNN to impute data\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8b63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute the training features\n",
    "imputer = KNNImputer(n_neighbors=1000)\n",
    "imputed = imputer.fit_transform(average_train_features)\n",
    "imputed_train_features = pd.DataFrame(imputed, columns=average_train_features.columns)\n",
    "\n",
    "imputed_train_features.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45260f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also impute the test features\n",
    "imputed_test = imputer.transform(average_test_features)\n",
    "imputed_test_features = pd.DataFrame(imputed_test, columns=average_test_features.columns)\n",
    "\n",
    "imputed_test_features.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of NaN in train_features\n",
    "print(imputed_train_features.shape)\n",
    "print(imputed_train_features.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c18e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of NaN in the test_features\n",
    "print(imputed_test_features.shape)\n",
    "print(imputed_test_features.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bfcd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the labels\n",
    "print(labels.shape)\n",
    "labels.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delect the columns with too many NaN\n",
    "# for index, row in average_features.iteritems():\n",
    "#     if average_features[index].isnull().sum() >= 0.7 * len(average_features[index]):\n",
    "#         average_features = average_features.drop(columns=index)\n",
    "#         average_test_features = average_test_features.drop(columns=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputed\n",
    "X_train = imputed_train_features\n",
    "X_test = imputed_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047534d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "START FROM HERE WITHOUT IMPUTATION\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not imputed\n",
    "X_train = average_train_features\n",
    "X_test = average_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_tests = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', \n",
    "                 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "disease = ['LABEL_Sepsis']\n",
    "vital_signs = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f84122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training data\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc007060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import validation methods\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a096c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram-Based Gradient Boosting Classifier and Regressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64eeac2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "for index, row in labels.iteritems():\n",
    "    if index in medical_tests:\n",
    "        hgbC = HistGradientBoostingClassifier()\n",
    "        y = labels[index]\n",
    "        fit = hgbC.fit(X_train, y)\n",
    "        proba = fit.predict_proba(X_test)[:,1]\n",
    "        output_dict[index] = proba\n",
    "    elif index in disease:\n",
    "        hgbC = HistGradientBoostingClassifier()\n",
    "        y = labels[index]\n",
    "        fit = hgbC.fit(X_train, y)\n",
    "        proba = fit.predict_proba(X_test)[:,1]\n",
    "        output_dict[index] = proba\n",
    "    elif index in vital_signs:\n",
    "        hgbR = HistGradientBoostingRegressor()\n",
    "        y = labels[index]\n",
    "        fit = hgbR.fit(X_train, y)\n",
    "        values = fit.predict(X_test)\n",
    "        output_dict[index] = values\n",
    "    print(output_dict)\n",
    "output = pd.DataFrame(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('prediction_9.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbC = HistGradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924edbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgbR = HistGradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00cb40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in medical_tests + disease:\n",
    "    y = labels[item]\n",
    "    score = cross_validate(hgbC, X_train, y, cv=10, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "    avgScore = {}\n",
    "    for k,v in score.items():\n",
    "        # v is the list of scores for fold k\n",
    "        avgScore[k] = sum(v)/ float(len(v))\n",
    "    print(avgScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in vital_signs:\n",
    "    y = labels[item]\n",
    "    regression_score = cross_validate(hgbR, X_train, y, cv=10, scoring=('r2'), return_train_score=True)\n",
    "    avgScore = {}\n",
    "    for k,v in regression_score.items():\n",
    "        # v is the list of scores for fold k\n",
    "        avgScore[k] = sum(v)/ float(len(v))\n",
    "    print(avgScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tried to select HistGradientBoostingClassifier's parameters and find there are not that much difference.\n",
    "'''\n",
    "# # Grid Search Cross Validation\n",
    "# parameters = {\n",
    "#  'max_iter': [1000,1200,1500],\n",
    "#  'learning_rate': [0.1],\n",
    "#  'max_depth': [25, 50, 75],\n",
    "#  'l2_regularization': [1.5],\n",
    "#  'random_state': [2022],\n",
    "#  }\n",
    "# tuning = GridSearchCV(estimator =HistGradientBoostingClassifier(), \n",
    "#             param_grid = parameters, scoring='roc_auc',n_jobs=4, cv=5)\n",
    "# tuning.fit(X_train,y)\n",
    "# tuning.cv_results_, tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8023959",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Code below is production code during model selection and testing, including Logistic Regression, SGD classifier, SVM with kernels, KNN. \n",
    "They are either too slow or with bad performance so I do not use them afterall.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c91eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db4e1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec5c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_BaseExcess\n",
    "model = svm.SVC(C=0.8, kernel='rbf', gamma=0.04, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24120b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels['LABEL_Fibrinogen']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f109aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360028dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9898b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_Fibrinogen\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_AST\n",
    "y = labels['LABEL_AST']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.02, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237386f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_Alkalinephos\n",
    "y = labels['LABEL_Alkalinephos']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_Bilirubin_total\n",
    "y = labels['LABEL_Bilirubin_total']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=0.8, kernel='rbf', gamma=0.075, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7733cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_Lactate\n",
    "y = labels['LABEL_Lactate']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_TroponinI\n",
    "y = labels['LABEL_TroponinI']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_SaO2\n",
    "y = labels['LABEL_SaO2']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_Bilirubin_direct\n",
    "y = labels['LABEL_Bilirubin_direct']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_EtCO2\n",
    "y = labels['LABEL_EtCO2']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=1, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LABEL_Sepsis\n",
    "y = labels['LABEL_Sepsis']\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the training features and test features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "print(y_train.mean())\n",
    "\n",
    "model = svm.SVC(C=1, kernel='rbf', gamma=1, class_weight='balanced', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "auroc_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print('y_pred_train mean: ', y_pred_train.mean())\n",
    "print('y_test mean: ', y_test.mean())\n",
    "print('y_pred_test mean: ', y_pred.mean())\n",
    "print('train accuracy: ', score)\n",
    "print('test accuracy: ', test_score)\n",
    "print('auroc score: ', auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83804d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fd304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fec484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X = ss.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a18551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=0.1, kernel='rbf', gamma=0.01, class_weight='balanced')\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, scoring=('roc_auc', 'accuracy'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=0.2, kernel='rbf', gamma=0.02, class_weight='balanced')\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=('roc_auc', 'accuracy'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7cdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross validation to find the best C and gamma\n",
    "def optimize_C_gamma(X, y, index):\n",
    "    print('Scores for label: ', index, 'Start! \\n')\n",
    "    scores = []\n",
    "    for c in np.arange(0.1, 1.1, 0.3):\n",
    "        for g in np.arange(0.01, 0.25, 0.04):\n",
    "            print('Current C: ', c)\n",
    "            print('Current Gamma: ', g)\n",
    "            # Train the SVM model\n",
    "            model = svm.SVC(C=c, kernel='rbf', gamma=g, class_weight='balanced')\n",
    "            score = cross_validate(model, X, y, cv=5, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "            avgScore = {}\n",
    "            avgScore['LABEL'] = index\n",
    "            for k,v in score.items():\n",
    "                # v is the list of grades for student k\n",
    "                avgScore[k] = sum(v)/ float(len(v))\n",
    "            avgScore.pop('fit_time')\n",
    "            avgScore.pop('score_time')\n",
    "        \n",
    "            scores.append(avgScore)\n",
    "            print(avgScore, '\\n')\n",
    "            print('Interate once!\\n')\n",
    "    print('Scores for label: ', index, 'Finished! \\n')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_C_gamma(X, y, index):\n",
    "    print('Scores for label: ', index, 'Start! \\n')\n",
    "    params_grid = {'C': np.arange(0.1, 1, 0.3), 'gamma': np.arange(0.01, 0.2, 0.03)}\n",
    "    model = svm.SVC()\n",
    "    grid_search = GridSearchCV(model, param_grid = params_grid, scoring='roc_auc')\n",
    "    grid_search.fit(X, y)\n",
    "    score = grid_search.score(X, y)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48714320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use KNN to classify\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c742cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X\n",
    "y = labels['LABEL_BaseExcess']\n",
    "\n",
    "def search_knn(X, y, index):\n",
    "    print('Scores for label: ', index, 'Start! \\n')\n",
    "    scores = []\n",
    "    for k in range(1,20,2):\n",
    "        print('K: ', k)\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        score = cross_validate(knn, X, y, cv=5, scoring=['roc_auc', 'accuracy'], return_train_score=True)\n",
    "        avgScore = {}\n",
    "        avgScore['LABEL'] = index\n",
    "        for k,v in score.items():\n",
    "            # v is the list of grades for student k\n",
    "            avgScore[k] = sum(v)/ float(len(v))\n",
    "        avgScore.pop('fit_time')\n",
    "        avgScore.pop('score_time')\n",
    "        scores.append(avgScore)\n",
    "        print(avgScore, '\\n')\n",
    "        print('Interate once!\\n')\n",
    "    print('Scores for label: ', index, 'Finished! \\n')\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ceeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_knn(X, y, index):\n",
    "    print('Scores for label: ', index, 'Start! \\n')\n",
    "    scores = []\n",
    "    for k in range(201,400,4):\n",
    "        print('K: ', k)\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        score = cross_validate(knn, X, y, cv=5, scoring=['roc_auc', 'accuracy'], return_train_score=True)\n",
    "        avgScore = {}\n",
    "        avgScore['LABEL'] = index\n",
    "        for k,v in score.items():\n",
    "            # v is the list of grades for student k\n",
    "            avgScore[k] = sum(v)/ float(len(v))\n",
    "        avgScore.pop('fit_time')\n",
    "        avgScore.pop('score_time')\n",
    "        scores.append(avgScore)\n",
    "        print(avgScore, '\\n')\n",
    "        print('Interate once!\\n')\n",
    "    print('Scores for label: ', index, 'Finished! \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X\n",
    "y = labels['LABEL_Sepsis']\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ce691",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10000)\n",
    "score_test = cross_validate(knn, X, y, cv=5, scoring=['roc_auc', 'accuracy'], return_train_score=True)\n",
    "avgScore_test={}\n",
    "avgScore_test['LABEL'] = 'LABEL_Sepsis'\n",
    "for k,v in score_test.items():\n",
    "    # v is the list of output for item k\n",
    "    avgScore_test[k] = sum(v)/ float(len(v))\n",
    "# avgScore_test.pop('fit_time')\n",
    "# avgScore_test.pop('score_time')\n",
    "print(avgScore_test, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453332fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_knn(X, labels['LABEL_Sepsis'], 'LABEL_Sepsis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each medical test\n",
    "medical_tests = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "for index, row in labels.iteritems():\n",
    "    if index in medical_tests:\n",
    "        scores_for_label = search_knn(X, labels[index], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = optimize_C_gamma(X, y, 'LABEL_BaseExcess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(C=0.8, kernel='rbf', gamma=0.04, class_weight='balanced')\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d373350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each medical test\n",
    "medical_tests = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "# Iterate the medical tests\n",
    "for index, row in labels.iteritems():\n",
    "    if index in medical_tests:\n",
    "        scores_for_label = optimize_C_gamma(X, labels[index], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbb0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Classifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X\n",
    "y = labels['LABEL_Sepsis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e783f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(sgd, X, y, cv=5, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
