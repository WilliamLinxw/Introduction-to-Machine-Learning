{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVAaWUZmJuEf",
        "outputId": "4ad645e5-4726-44d8-9e0c-43603abf0dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BVAaWUZmJuEf"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "38209c7b"
      },
      "outputs": [],
      "source": [
        "# Initialization: importing the packages that we will use\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Google colab offers time limited use of GPU for free\n",
        "\n",
        "# Training parameters \n",
        "BATCH_SIZE = 64"
      ],
      "id": "38209c7b"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aa85839b"
      },
      "outputs": [],
      "source": [
        "# For loading the data\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "id": "aa85839b"
    },
    {
      "cell_type": "code",
      "source": [
        "# For constructing the network\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data\n",
        "import torch.backends.cudnn as cudnn"
      ],
      "metadata": {
        "id": "qglrJZFAcYGG"
      },
      "id": "qglrJZFAcYGG",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "hIf_DSCgj5FT"
      },
      "id": "hIf_DSCgj5FT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9cafface"
      },
      "outputs": [],
      "source": [
        "# Solve the imshow dead kernel problem\n",
        "import os    \n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "id": "9cafface"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc461e5c",
        "outputId": "25f3aea3-a38f-49b9-c075-cbf8af2ff404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START LOADING DATA ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start loading the data\n",
        "'''\n",
        "print('================== START LOADING DATA ==================')"
      ],
      "id": "bc461e5c"
    },
    {
      "cell_type": "code",
      "source": [
        "path_drive = '/content/drive/My Drive/'"
      ],
      "metadata": {
        "id": "ckGiS8tn0UxL"
      },
      "id": "ckGiS8tn0UxL",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Task4/pretrain_features.csv /content\n",
        "!cp /content/drive/MyDrive/Task4/pretrain_labels.csv /content\n",
        "!cp /content/drive/MyDrive/Task4/test_features.csv /content\n",
        "!cp /content/drive/MyDrive/Task4/train_features.csv /content\n",
        "!cp /content/drive/MyDrive/Task4/train_labels.csv /content"
      ],
      "metadata": {
        "id": "ljA0haO40cAM"
      },
      "id": "ljA0haO40cAM",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1 = pd.read_csv('pretrain_features.csv')  # features\n",
        "r2 = pd.read_csv('pretrain_labels.csv')  # labels\n",
        "\n",
        "\n",
        "print(r1.shape)\n",
        "print(r2.shape)\n",
        "\n",
        "# Merge two dataframes\n",
        "all_data_st = pd.merge(r2, r1, on='Id')\n",
        "print(all_data_st.shape)\n",
        "print(all_data_st.head(10))\n",
        "\n",
        "# # Output to csv\n",
        "# all_data_st.to_csv(\"training_set.csv\", index=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3I-LGWBXWPF",
        "outputId": "57b731d6-3ea2-460e-9ee2-fbeb80b8fb54"
      },
      "id": "X3I-LGWBXWPF",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 1002)\n",
            "(50000, 2)\n",
            "(50000, 1003)\n",
            "   Id  lumo_energy                                             smiles  \\\n",
            "0   0    -3.111521  c1occ2c1c1ccc3cscc3c1c1ncc3cc(ccc3c21)-c1cccc2...   \n",
            "1   1    -3.219118  C1C=c2c(cc3ncc4c5[SiH2]C=Cc5oc4c3c2=C1)-c1scc2...   \n",
            "2   2    -3.114145  C1C=c2c3cccnc3c3c4c[nH]cc4c4cc(cnc4c3c2=C1)-c1...   \n",
            "3   3    -3.161867  [SiH2]1C=Cc2c1csc2-c1cnc2c(c1)c1ccccc1c1cc3ccc...   \n",
            "4   4    -3.687744        c1occ2c1c(cc1[se]c3ccncc3c21)-c1cccc2nsnc12   \n",
            "5   5    -2.791261  [SiH2]1C=Cc2[nH]c3c(oc4cc(sc34)-c3scc4cc[se]c3...   \n",
            "6   6    -3.688235          c1ccc(nc1)-c1cc2ncc3c4cnccc4sc3c2c2nsnc12   \n",
            "7   7    -3.243368  C1C=c2c3cccnc3c3c4cocc4c4C=C(Cc4c3c2=C1)c1scc2...   \n",
            "8   8    -3.508069  c1cc2csc(-c3cc4c5cscc5c5c6occc6c6cscc6c5c4c4ns...   \n",
            "9   9    -3.440629  [SiH2]1C=c2c3cc(oc3c3c4cocc4c4ccc5cscc5c4c3c2=...   \n",
            "\n",
            "   feature_0000  feature_0001  feature_0002  feature_0003  feature_0004  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           0.0           0.0           0.0           1.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           0.0           0.0           0.0   \n",
            "5           0.0           0.0           0.0           1.0           0.0   \n",
            "6           0.0           0.0           0.0           1.0           0.0   \n",
            "7           0.0           0.0           0.0           0.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0005  feature_0006  ...  feature_0990  feature_0991  feature_0992  \\\n",
            "0           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "1           0.0           1.0  ...           0.0           1.0           0.0   \n",
            "2           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "3           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "4           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "5           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "6           0.0           1.0  ...           0.0           0.0           0.0   \n",
            "7           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "8           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "9           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0993  feature_0994  feature_0995  feature_0996  feature_0997  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           0.0           0.0           0.0           0.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           1.0           0.0           0.0   \n",
            "5           0.0           0.0           0.0           0.0           0.0   \n",
            "6           0.0           0.0           1.0           0.0           0.0   \n",
            "7           0.0           0.0           0.0           0.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           1.0   \n",
            "\n",
            "   feature_0998  feature_0999  \n",
            "0           0.0           0.0  \n",
            "1           0.0           0.0  \n",
            "2           0.0           0.0  \n",
            "3           0.0           0.0  \n",
            "4           0.0           0.0  \n",
            "5           0.0           0.0  \n",
            "6           0.0           0.0  \n",
            "7           0.0           0.0  \n",
            "8           0.0           0.0  \n",
            "9           0.0           0.0  \n",
            "\n",
            "[10 rows x 1003 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_data_st = all_data_st.sample(frac=1)\n",
        "# print(all_data_st.head(10))\n",
        "all_data_st_noidsmiles = all_data_st.drop(['Id', 'smiles'], axis=1)\n",
        "print(all_data_st_noidsmiles.head(10))\n",
        "all_data_array = np.array(all_data_st_noidsmiles)\n",
        "all_data_tensor = torch.tensor(all_data_array)\n",
        "print(all_data_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyMGg_ii7dEN",
        "outputId": "49b3bfb8-c3e4-4360-e7cc-508b67a3843b"
      },
      "id": "UyMGg_ii7dEN",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   lumo_energy  feature_0000  feature_0001  feature_0002  feature_0003  \\\n",
            "0    -3.111521           0.0           0.0           0.0           0.0   \n",
            "1    -3.219118           0.0           0.0           0.0           0.0   \n",
            "2    -3.114145           0.0           0.0           0.0           0.0   \n",
            "3    -3.161867           0.0           0.0           0.0           0.0   \n",
            "4    -3.687744           0.0           0.0           0.0           0.0   \n",
            "5    -2.791261           0.0           0.0           0.0           1.0   \n",
            "6    -3.688235           0.0           0.0           0.0           1.0   \n",
            "7    -3.243368           0.0           0.0           0.0           0.0   \n",
            "8    -3.508069           0.0           0.0           0.0           0.0   \n",
            "9    -3.440629           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0004  feature_0005  feature_0006  feature_0007  feature_0008  ...  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "1           1.0           0.0           1.0           0.0           0.0  ...   \n",
            "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "3           0.0           0.0           0.0           1.0           0.0  ...   \n",
            "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "5           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "6           0.0           0.0           1.0           0.0           0.0  ...   \n",
            "7           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "8           0.0           0.0           0.0           0.0           1.0  ...   \n",
            "9           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "\n",
            "   feature_0990  feature_0991  feature_0992  feature_0993  feature_0994  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           1.0           0.0           0.0           0.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           0.0           0.0           0.0   \n",
            "5           0.0           0.0           0.0           0.0           0.0   \n",
            "6           0.0           0.0           0.0           0.0           0.0   \n",
            "7           0.0           0.0           0.0           0.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0995  feature_0996  feature_0997  feature_0998  feature_0999  \n",
            "0           0.0           0.0           0.0           0.0           0.0  \n",
            "1           0.0           0.0           0.0           0.0           0.0  \n",
            "2           0.0           0.0           0.0           0.0           0.0  \n",
            "3           0.0           0.0           0.0           0.0           0.0  \n",
            "4           1.0           0.0           0.0           0.0           0.0  \n",
            "5           0.0           0.0           0.0           0.0           0.0  \n",
            "6           1.0           0.0           0.0           0.0           0.0  \n",
            "7           0.0           0.0           0.0           0.0           0.0  \n",
            "8           0.0           0.0           0.0           0.0           0.0  \n",
            "9           0.0           0.0           1.0           0.0           0.0  \n",
            "\n",
            "[10 rows x 1001 columns]\n",
            "tensor([[-3.1115,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-3.2191,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-3.1141,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [-2.7010,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-3.5168,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-3.4101,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(all_data_st_noidsmiles.iloc[0][1:], dtype=torch.double)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNsPQgI3D_Bg",
        "outputId": "d17c7db3-c64c-4585-e1ba-d29b878253ac"
      },
      "id": "GNsPQgI3D_Bg",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For reproducbility\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "np.random.seed(1998)\n",
        "torch.manual_seed(1998)\n",
        "torch.cuda.manual_seed(1998)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(1998)"
      ],
      "metadata": {
        "id": "zc7esgtsNpGD"
      },
      "id": "zc7esgtsNpGD",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87028c17",
        "outputId": "9edd3671-b00a-4afa-b421-9c2fe78ffe05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START CONSTRUCTING NETWORK ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start constructing the network\n",
        "'''\n",
        "print('================== START CONSTRUCTING NETWORK ==================')"
      ],
      "id": "87028c17"
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtract(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder_hidden_layer_1 = nn.Linear(\n",
        "            in_features=kwargs[\"input_shape\"], out_features=512\n",
        "        )\n",
        "        self.encoder_hidden_layer_2 = nn.Linear(\n",
        "            in_features=512, out_features=256\n",
        "        )\n",
        "        self.encoder_hidden_layer_3 = nn.Linear(\n",
        "            in_features=256, out_features=128\n",
        "        )\n",
        "        self.encoder_hidden_layer_4 = nn.Linear(\n",
        "            in_features=128, out_features=64\n",
        "        )\n",
        "        self.prediction_layer = nn.Linear(\n",
        "            in_features=64, out_features=1\n",
        "        )\n",
        " \n",
        "    def forward(self, features):\n",
        "        activation_1 = self.encoder_hidden_layer_1(features)\n",
        "        activation_1 = torch.relu(activation_1)\n",
        "        activation_2 = self.encoder_hidden_layer_2(activation_1)\n",
        "        activation_2 = torch.relu(activation_2)\n",
        "        activation_3 = self.encoder_hidden_layer_3(activation_2)\n",
        "        activation_3 = torch.relu(activation_3)\n",
        "        activation_4 = self.encoder_hidden_layer_4(activation_3)\n",
        "        activation_4 = torch.relu(activation_4)\n",
        "        output = self.prediction_layer(activation_4)\n",
        "        return output"
      ],
      "metadata": {
        "id": "RmsOJBPA4hM_"
      },
      "id": "RmsOJBPA4hM_",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeatureExtract(input_shape=1000).to(device)"
      ],
      "metadata": {
        "id": "BTYNgw7-770D"
      },
      "id": "BTYNgw7-770D",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5fa4121e"
      },
      "outputs": [],
      "source": [
        "# Construct the loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                            lr=0.0005,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=2e-3,#The value used in the paper is 1e-3\n",
        "                            nesterov=True)"
      ],
      "id": "5fa4121e"
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_data_st_noidsmiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bom4YyJpW_Bo",
        "outputId": "85652268-7302-4340-b722-c6f22b6042c7"
      },
      "id": "bom4YyJpW_Bo",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training = all_data_st_noidsmiles.sample(frac=0.95, random_state=1998)\n",
        "\n",
        "validation = all_data_st_noidsmiles.drop(training.index)\n",
        "\n",
        "print(len(training))\n",
        "print(len(validation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-3354dLaa_j",
        "outputId": "07e1c162-acf5-45d9-b84e-8eb17aa97585"
      },
      "id": "3-3354dLaa_j",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47500\n",
            "2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(training.iloc[0][1:], dtype=torch.float).to(device)\n",
        "result = model(input)\n",
        "loss = criterion(result, torch.tensor(training.iloc[1][0:1], dtype=torch.float).to(device))\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lnrQwic53nu",
        "outputId": "048df940-c158-48ec-da2b-39eb720e3739"
      },
      "id": "3lnrQwic53nu",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.0797, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def val(model, criterion, valset, datapoint):\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  validation_loss_sum = 0\n",
        "\n",
        "  for i in range(len(valset)):\n",
        "\n",
        "    # Get one input from the validation set\n",
        "    input = torch.tensor(valset.iloc[i][1:], dtype=torch.float).to(device)\n",
        "    \n",
        "    # Calculate the output\n",
        "    output = model(input)\n",
        "\n",
        "    # Calculate the MSE loss\n",
        "    validation_loss_point = criterion(output, torch.tensor(valset.iloc[i][0:1], dtype=torch.float).to(device))\n",
        "\n",
        "    # update validation_loss\n",
        "    validation_loss_sum += validation_loss_point.item()\n",
        "\n",
        "  print(f'{datapoint} Validation Loss: {validation_loss_sum / len(valset):.3f}')\n",
        "\n",
        "  torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "IRLjt9cfcs3F"
      },
      "id": "IRLjt9cfcs3F",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, epochs, training_set, validation_set):\n",
        "  \n",
        "  # Empty the cache of CUDA  \n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  print('================== START TRAINING ==================')\n",
        "\n",
        "  # Change to train mode\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    for data_point in range(len(training_set)):\n",
        "\n",
        "      # Get one input from the training set\n",
        "      input = torch.tensor(training_set.iloc[data_point][1:], dtype=torch.float).to(device)\n",
        "\n",
        "      # Calculate its corresponding output\n",
        "      result = model(input)\n",
        "      # print(result)\n",
        "\n",
        "      # Calculate the MSE loss\n",
        "      loss = criterion(result, torch.tensor(training_set.iloc[data_point][0:1], dtype=torch.float).to(device))\n",
        "      if ((data_point+1) % 100) == 0:\n",
        "        print(f'training {data_point+1} loss: {loss}')\n",
        "\n",
        "      # Zero the gradient\n",
        "      optimizer.zero_grad()\n",
        "                \n",
        "      # Back prop and update\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # Validation\n",
        "      if ((data_point+1) % 5000) == 0:\n",
        "        # Change to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        val(model, criterion, validation_set, data_point+1)\n",
        "\n",
        "        # Change back to training mode\n",
        "        model.train()\n",
        "\n",
        "    print(f'[{epoch + 1}] average loss per epoch: {running_loss / len(training_set):.3f}')\n",
        "\n",
        "    save_path = f'/content/drive/My Drive/Task4/test5/model_epoch{epoch+1}.pt'\n",
        "    torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict()}, save_path)\n",
        "    print(f'Saved model checkpoint to {save_path}')"
      ],
      "metadata": {
        "id": "yO3HXJwwWsQg"
      },
      "id": "yO3HXJwwWsQg",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, criterion, optimizer, 5, training, validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmioLEsSsOkB",
        "outputId": "9d35d1db-5bab-47db-c50b-cbd1d648d397"
      },
      "id": "lmioLEsSsOkB",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START TRAINING ==================\n",
            "training 100 loss: 0.006629479117691517\n",
            "training 200 loss: 0.1727408468723297\n",
            "training 300 loss: 0.17348702251911163\n",
            "training 400 loss: 0.05562751740217209\n",
            "training 500 loss: 0.3524382710456848\n",
            "training 600 loss: 0.05999352037906647\n",
            "training 700 loss: 0.0006808076286688447\n",
            "training 800 loss: 0.0036711804568767548\n",
            "training 900 loss: 0.010571641847491264\n",
            "training 1000 loss: 0.06037874147295952\n",
            "training 1100 loss: 0.07002365589141846\n",
            "training 1200 loss: 0.0031184267718344927\n",
            "training 1300 loss: 0.004192217253148556\n",
            "training 1400 loss: 0.00508978171274066\n",
            "training 1500 loss: 0.015186920762062073\n",
            "training 1600 loss: 0.006078209728002548\n",
            "training 1700 loss: 0.010147289372980595\n",
            "training 1800 loss: 0.06858671456575394\n",
            "training 1900 loss: 0.0006138374446891248\n",
            "training 2000 loss: 0.001844224170781672\n",
            "training 2100 loss: 0.0010619936510920525\n",
            "training 2200 loss: 0.015589496120810509\n",
            "training 2300 loss: 0.00087946024723351\n",
            "training 2400 loss: 0.10230690240859985\n",
            "training 2500 loss: 0.007880859076976776\n",
            "training 2600 loss: 0.018354035913944244\n",
            "training 2700 loss: 0.00013127847341820598\n",
            "training 2800 loss: 0.00652968930080533\n",
            "training 2900 loss: 0.20930558443069458\n",
            "training 3000 loss: 0.15841367840766907\n",
            "training 3100 loss: 0.0012893832754343748\n",
            "training 3200 loss: 0.05788278952240944\n",
            "training 3300 loss: 1.6311992112605367e-07\n",
            "training 3400 loss: 0.015274899080395699\n",
            "training 3500 loss: 0.032073408365249634\n",
            "training 3600 loss: 0.12558239698410034\n",
            "training 3700 loss: 0.05700906738638878\n",
            "training 3800 loss: 0.0002477149828337133\n",
            "training 3900 loss: 0.007723001763224602\n",
            "training 4000 loss: 0.04729120060801506\n",
            "training 4100 loss: 0.029611622914671898\n",
            "training 4200 loss: 0.007535622920840979\n",
            "training 4300 loss: 0.00022567599080502987\n",
            "training 4400 loss: 0.06548231840133667\n",
            "training 4500 loss: 0.04480021819472313\n",
            "training 4600 loss: 0.0009336231742054224\n",
            "training 4700 loss: 0.011241700500249863\n",
            "training 4800 loss: 0.005744330119341612\n",
            "training 4900 loss: 0.002070600865408778\n",
            "training 5000 loss: 2.2203930711839348e-05\n",
            "5000 Validation Loss: 0.019\n",
            "training 5100 loss: 0.04650207981467247\n",
            "training 5200 loss: 0.0008932161726988852\n",
            "training 5300 loss: 0.00048473788774572313\n",
            "training 5400 loss: 0.030322322621941566\n",
            "training 5500 loss: 0.0026506625581532717\n",
            "training 5600 loss: 9.476303262090369e-07\n",
            "training 5700 loss: 0.06909127533435822\n",
            "training 5800 loss: 0.0022078282199800014\n",
            "training 5900 loss: 0.006548776291310787\n",
            "training 6000 loss: 0.003082794602960348\n",
            "training 6100 loss: 0.008791834115982056\n",
            "training 6200 loss: 0.0024963964242488146\n",
            "training 6300 loss: 0.00494740204885602\n",
            "training 6400 loss: 0.032726138830184937\n",
            "training 6500 loss: 0.0021310080774128437\n",
            "training 6600 loss: 3.7037571019027382e-06\n",
            "training 6700 loss: 0.008615185506641865\n",
            "training 6800 loss: 0.00777900405228138\n",
            "training 6900 loss: 0.12087365239858627\n",
            "training 7000 loss: 0.0658068060874939\n",
            "training 7100 loss: 0.014644590206444263\n",
            "training 7200 loss: 0.019468538463115692\n",
            "training 7300 loss: 0.06817784160375595\n",
            "training 7400 loss: 0.005982527509331703\n",
            "training 7500 loss: 0.0003770309849642217\n",
            "training 7600 loss: 0.028957609087228775\n",
            "training 7700 loss: 0.00010569092410150915\n",
            "training 7800 loss: 0.026653822511434555\n",
            "training 7900 loss: 0.0035372122656553984\n",
            "training 8000 loss: 0.004029092378914356\n",
            "training 8100 loss: 0.04882664978504181\n",
            "training 8200 loss: 0.00034625225816853344\n",
            "training 8300 loss: 0.007987255230545998\n",
            "training 8400 loss: 0.010443920269608498\n",
            "training 8500 loss: 0.02751481719315052\n",
            "training 8600 loss: 0.0002036688383668661\n",
            "training 8700 loss: 0.0003297114744782448\n",
            "training 8800 loss: 0.020795658230781555\n",
            "training 8900 loss: 0.014049595221877098\n",
            "training 9000 loss: 9.725135896587744e-06\n",
            "training 9100 loss: 0.00111247634049505\n",
            "training 9200 loss: 0.00474121468141675\n",
            "training 9300 loss: 0.0024328839499503374\n",
            "training 9400 loss: 0.026196185499429703\n",
            "training 9500 loss: 0.08950697630643845\n",
            "training 9600 loss: 0.006927037611603737\n",
            "training 9700 loss: 0.02706950530409813\n",
            "training 9800 loss: 0.009218554943799973\n",
            "training 9900 loss: 0.021413689479231834\n",
            "training 10000 loss: 0.015260346233844757\n",
            "10000 Validation Loss: 0.016\n",
            "training 10100 loss: 0.016563326120376587\n",
            "training 10200 loss: 0.007604035083204508\n",
            "training 10300 loss: 0.003908277023583651\n",
            "training 10400 loss: 0.047598741948604584\n",
            "training 10500 loss: 0.04260830953717232\n",
            "training 10600 loss: 0.010411247611045837\n",
            "training 10700 loss: 0.029865052551031113\n",
            "training 10800 loss: 0.006563563831150532\n",
            "training 10900 loss: 0.005403219256550074\n",
            "training 11000 loss: 0.006053995806723833\n",
            "training 11100 loss: 0.00020009161380585283\n",
            "training 11200 loss: 0.01659850962460041\n",
            "training 11300 loss: 0.0017737312009558082\n",
            "training 11400 loss: 0.0013064589584246278\n",
            "training 11500 loss: 0.005983966402709484\n",
            "training 11600 loss: 0.060622818768024445\n",
            "training 11700 loss: 0.016684258356690407\n",
            "training 11800 loss: 0.017740124836564064\n",
            "training 11900 loss: 0.00011332939175190404\n",
            "training 12000 loss: 0.014924337156116962\n",
            "training 12100 loss: 0.0022445227950811386\n",
            "training 12200 loss: 0.012810558080673218\n",
            "training 12300 loss: 0.006085237953811884\n",
            "training 12400 loss: 0.10009752959012985\n",
            "training 12500 loss: 0.0053938296623528\n",
            "training 12600 loss: 0.028911538422107697\n",
            "training 12700 loss: 0.002081007231026888\n",
            "training 12800 loss: 0.009882975369691849\n",
            "training 12900 loss: 0.005372838117182255\n",
            "training 13000 loss: 0.0004464301164261997\n",
            "training 13100 loss: 0.0007551573216915131\n",
            "training 13200 loss: 0.005183317232877016\n",
            "training 13300 loss: 0.031434860080480576\n",
            "training 13400 loss: 8.639677980681881e-05\n",
            "training 13500 loss: 0.007137442473322153\n",
            "training 13600 loss: 2.5840168746071868e-05\n",
            "training 13700 loss: 0.006006598938256502\n",
            "training 13800 loss: 0.010146137326955795\n",
            "training 13900 loss: 0.0004391955735627562\n",
            "training 14000 loss: 0.007809693459421396\n",
            "training 14100 loss: 0.009009066969156265\n",
            "training 14200 loss: 0.04656502977013588\n",
            "training 14300 loss: 0.001091487123630941\n",
            "training 14400 loss: 0.007507419213652611\n",
            "training 14500 loss: 0.0003526344080455601\n",
            "training 14600 loss: 0.0008501678821630776\n",
            "training 14700 loss: 1.8554628695710562e-05\n",
            "training 14800 loss: 0.013853762298822403\n",
            "training 14900 loss: 0.0009426783653907478\n",
            "training 15000 loss: 0.023491328582167625\n",
            "15000 Validation Loss: 0.012\n",
            "training 15100 loss: 0.04869440197944641\n",
            "training 15200 loss: 0.010264347307384014\n",
            "training 15300 loss: 5.928214159212075e-05\n",
            "training 15400 loss: 0.021978093311190605\n",
            "training 15500 loss: 0.0014080494875088334\n",
            "training 15600 loss: 0.005650176666676998\n",
            "training 15700 loss: 0.12352612614631653\n",
            "training 15800 loss: 0.0318351648747921\n",
            "training 15900 loss: 0.028252199292182922\n",
            "training 16000 loss: 0.004039207939058542\n",
            "training 16100 loss: 0.014735731296241283\n",
            "training 16200 loss: 0.02602928690612316\n",
            "training 16300 loss: 0.0010807226644828916\n",
            "training 16400 loss: 0.0005857305368408561\n",
            "training 16500 loss: 0.060373704880476\n",
            "training 16600 loss: 0.0018009863561019301\n",
            "training 16700 loss: 0.002082573715597391\n",
            "training 16800 loss: 0.03201877698302269\n",
            "training 16900 loss: 0.009274311363697052\n",
            "training 17000 loss: 0.015227965079247952\n",
            "training 17100 loss: 0.005108202807605267\n",
            "training 17200 loss: 0.0009460339788347483\n",
            "training 17300 loss: 0.0003781057894229889\n",
            "training 17400 loss: 0.002853830810636282\n",
            "training 17500 loss: 0.0012497801799327135\n",
            "training 17600 loss: 0.004588637966662645\n",
            "training 17700 loss: 0.0067227124236524105\n",
            "training 17800 loss: 0.04102145507931709\n",
            "training 17900 loss: 0.013674522750079632\n",
            "training 18000 loss: 0.00708123529329896\n",
            "training 18100 loss: 0.009663589298725128\n",
            "training 18200 loss: 0.03017520159482956\n",
            "training 18300 loss: 0.008220593445003033\n",
            "training 18400 loss: 0.02143176645040512\n",
            "training 18500 loss: 0.027154143899679184\n",
            "training 18600 loss: 0.0014369681011885405\n",
            "training 18700 loss: 0.00112862978130579\n",
            "training 18800 loss: 3.373697109054774e-05\n",
            "training 18900 loss: 0.003299933625385165\n",
            "training 19000 loss: 0.027400800958275795\n",
            "training 19100 loss: 0.0014960007974877954\n",
            "training 19200 loss: 3.5926461805502186e-08\n",
            "training 19300 loss: 0.00045737746404483914\n",
            "training 19400 loss: 0.010626230388879776\n",
            "training 19500 loss: 0.002158302115276456\n",
            "training 19600 loss: 0.008266397751867771\n",
            "training 19700 loss: 0.009589108638465405\n",
            "training 19800 loss: 0.005646735895425081\n",
            "training 19900 loss: 0.026578130200505257\n",
            "training 20000 loss: 0.0251670740544796\n",
            "20000 Validation Loss: 0.011\n",
            "training 20100 loss: 0.03248737379908562\n",
            "training 20200 loss: 0.0011650099186226726\n",
            "training 20300 loss: 0.00097675621509552\n",
            "training 20400 loss: 0.0025628269650042057\n",
            "training 20500 loss: 0.00321545684710145\n",
            "training 20600 loss: 0.0006623342633247375\n",
            "training 20700 loss: 0.01678146794438362\n",
            "training 20800 loss: 0.013911349698901176\n",
            "training 20900 loss: 0.00534595875069499\n",
            "training 21000 loss: 0.001306786434724927\n",
            "training 21100 loss: 0.005174052435904741\n",
            "training 21200 loss: 0.05207141116261482\n",
            "training 21300 loss: 0.001056763343513012\n",
            "training 21400 loss: 4.242356226313859e-06\n",
            "training 21500 loss: 0.004423416685312986\n",
            "training 21600 loss: 0.0008598162676207721\n",
            "training 21700 loss: 1.2388805771479383e-05\n",
            "training 21800 loss: 0.0020980960689485073\n",
            "training 21900 loss: 0.007232344709336758\n",
            "training 22000 loss: 0.007175966165959835\n",
            "training 22100 loss: 0.001356989610940218\n",
            "training 22200 loss: 0.00013628057786263525\n",
            "training 22300 loss: 0.005176144652068615\n",
            "training 22400 loss: 0.006014103069901466\n",
            "training 22500 loss: 0.0025653380434960127\n",
            "training 22600 loss: 0.006321798078715801\n",
            "training 22700 loss: 0.0006732640322297812\n",
            "training 22800 loss: 0.0018052587984129786\n",
            "training 22900 loss: 0.0019344817847013474\n",
            "training 23000 loss: 0.0020569968037307262\n",
            "training 23100 loss: 0.012174520641565323\n",
            "training 23200 loss: 0.0049032289534807205\n",
            "training 23300 loss: 0.0001057791814673692\n",
            "training 23400 loss: 0.06371185928583145\n",
            "training 23500 loss: 0.0011713660787791014\n",
            "training 23600 loss: 0.05777351185679436\n",
            "training 23700 loss: 0.0033419488463550806\n",
            "training 23800 loss: 0.0022895438596606255\n",
            "training 23900 loss: 0.006680203601717949\n",
            "training 24000 loss: 2.4458135158056393e-05\n",
            "training 24100 loss: 0.0028262846171855927\n",
            "training 24200 loss: 0.026876229792833328\n",
            "training 24300 loss: 0.0027576531283557415\n",
            "training 24400 loss: 0.01884167082607746\n",
            "training 24500 loss: 0.0018033143132925034\n",
            "training 24600 loss: 0.06357640773057938\n",
            "training 24700 loss: 0.12316741049289703\n",
            "training 24800 loss: 0.0018666945397853851\n",
            "training 24900 loss: 0.004288379102945328\n",
            "training 25000 loss: 0.004197498317807913\n",
            "25000 Validation Loss: 0.009\n",
            "training 25100 loss: 0.006478121038526297\n",
            "training 25200 loss: 0.021718015894293785\n",
            "training 25300 loss: 0.0293106772005558\n",
            "training 25400 loss: 0.005394179839640856\n",
            "training 25500 loss: 0.00047498196363449097\n",
            "training 25600 loss: 0.06366540491580963\n",
            "training 25700 loss: 0.0006618680199608207\n",
            "training 25800 loss: 0.0007252556388266385\n",
            "training 25900 loss: 0.004909875802695751\n",
            "training 26000 loss: 0.006465116981416941\n",
            "training 26100 loss: 0.00014320774062070996\n",
            "training 26200 loss: 0.0002585709444247186\n",
            "training 26300 loss: 0.02514362893998623\n",
            "training 26400 loss: 0.0007280706195160747\n",
            "training 26500 loss: 0.014835923910140991\n",
            "training 26600 loss: 0.010132933035492897\n",
            "training 26700 loss: 0.00935553852468729\n",
            "training 26800 loss: 0.02518712356686592\n",
            "training 26900 loss: 0.0004875239683315158\n",
            "training 27000 loss: 0.009716818109154701\n",
            "training 27100 loss: 0.002091068774461746\n",
            "training 27200 loss: 0.0025534210726618767\n",
            "training 27300 loss: 0.00048790304572321475\n",
            "training 27400 loss: 0.03092428296804428\n",
            "training 27500 loss: 0.0027532477397471666\n",
            "training 27600 loss: 0.0036170650273561478\n",
            "training 27700 loss: 0.00046978972386568785\n",
            "training 27800 loss: 0.007391032762825489\n",
            "training 27900 loss: 0.004304131492972374\n",
            "training 28000 loss: 0.002710877452045679\n",
            "training 28100 loss: 0.0008790926076471806\n",
            "training 28200 loss: 0.0019311276264488697\n",
            "training 28300 loss: 0.0010298413690179586\n",
            "training 28400 loss: 0.006491714157164097\n",
            "training 28500 loss: 0.0005070889019407332\n",
            "training 28600 loss: 0.009870512410998344\n",
            "training 28700 loss: 0.013086630031466484\n",
            "training 28800 loss: 0.02419973537325859\n",
            "training 28900 loss: 0.003269735025241971\n",
            "training 29000 loss: 0.007432906422764063\n",
            "training 29100 loss: 0.0006815792294219136\n",
            "training 29200 loss: 0.0005282217171043158\n",
            "training 29300 loss: 1.6294663510052487e-05\n",
            "training 29400 loss: 0.08517132699489594\n",
            "training 29500 loss: 0.016385531052947044\n",
            "training 29600 loss: 0.14768318831920624\n",
            "training 29700 loss: 0.0130089046433568\n",
            "training 29800 loss: 0.014997475780546665\n",
            "training 29900 loss: 2.4075256078504026e-05\n",
            "training 30000 loss: 0.002633994910866022\n",
            "30000 Validation Loss: 0.010\n",
            "training 30100 loss: 0.031010543927550316\n",
            "training 30200 loss: 0.023319168016314507\n",
            "training 30300 loss: 0.00971272960305214\n",
            "training 30400 loss: 0.00031113156001083553\n",
            "training 30500 loss: 0.00693763792514801\n",
            "training 30600 loss: 0.011174408718943596\n",
            "training 30700 loss: 0.0012346211588010192\n",
            "training 30800 loss: 0.0074732075445353985\n",
            "training 30900 loss: 0.008135342039167881\n",
            "training 31000 loss: 0.009575105272233486\n",
            "training 31100 loss: 0.00043894577538594604\n",
            "training 31200 loss: 0.0002653313858900219\n",
            "training 31300 loss: 0.0034833934623748064\n",
            "training 31400 loss: 0.01828562282025814\n",
            "training 31500 loss: 0.02915072627365589\n",
            "training 31600 loss: 0.0004786992212757468\n",
            "training 31700 loss: 0.00017410273721907288\n",
            "training 31800 loss: 0.015299661085009575\n",
            "training 31900 loss: 0.0019596042111516\n",
            "training 32000 loss: 0.01404185313731432\n",
            "training 32100 loss: 0.0001671704085310921\n",
            "training 32200 loss: 0.001185246161185205\n",
            "training 32300 loss: 0.0059439013712108135\n",
            "training 32400 loss: 0.00695548253133893\n",
            "training 32500 loss: 0.0002424003032501787\n",
            "training 32600 loss: 9.874406714516226e-06\n",
            "training 32700 loss: 0.01650562882423401\n",
            "training 32800 loss: 5.5209580750670284e-05\n",
            "training 32900 loss: 0.022275002673268318\n",
            "training 33000 loss: 0.010161224752664566\n",
            "training 33100 loss: 0.010666772723197937\n",
            "training 33200 loss: 0.0023790663108229637\n",
            "training 33300 loss: 3.255109550082125e-05\n",
            "training 33400 loss: 0.0005579668213613331\n",
            "training 33500 loss: 0.006297329906374216\n",
            "training 33600 loss: 0.009014091454446316\n",
            "training 33700 loss: 0.01507787685841322\n",
            "training 33800 loss: 0.0802656039595604\n",
            "training 33900 loss: 0.058098554611206055\n",
            "training 34000 loss: 0.016333507373929024\n",
            "training 34100 loss: 0.009915378876030445\n",
            "training 34200 loss: 0.02509072981774807\n",
            "training 34300 loss: 0.000787120487075299\n",
            "training 34400 loss: 0.016370519995689392\n",
            "training 34500 loss: 0.018282979726791382\n",
            "training 34600 loss: 0.04690541326999664\n",
            "training 34700 loss: 0.025128962472081184\n",
            "training 34800 loss: 0.0052450536750257015\n",
            "training 34900 loss: 0.003549417480826378\n",
            "training 35000 loss: 0.010239965282380581\n",
            "35000 Validation Loss: 0.009\n",
            "training 35100 loss: 1.216326290887082e-05\n",
            "training 35200 loss: 0.0032566601876169443\n",
            "training 35300 loss: 0.0017573419027030468\n",
            "training 35400 loss: 0.00012541754404082894\n",
            "training 35500 loss: 1.3924011909693945e-05\n",
            "training 35600 loss: 0.0028492221608757973\n",
            "training 35700 loss: 3.3416457881685346e-05\n",
            "training 35800 loss: 0.01313216332346201\n",
            "training 35900 loss: 0.0006747372099198401\n",
            "training 36000 loss: 0.0004852839629165828\n",
            "training 36100 loss: 0.0008759285556152463\n",
            "training 36200 loss: 0.023329071700572968\n",
            "training 36300 loss: 0.0026955066714435816\n",
            "training 36400 loss: 0.0030407372396439314\n",
            "training 36500 loss: 0.017068063840270042\n",
            "training 36600 loss: 0.008419226855039597\n",
            "training 36700 loss: 0.010417233221232891\n",
            "training 36800 loss: 0.008670375682413578\n",
            "training 36900 loss: 0.030586443841457367\n",
            "training 37000 loss: 0.005651538725942373\n",
            "training 37100 loss: 0.02440609037876129\n",
            "training 37200 loss: 0.00027112592943012714\n",
            "training 37300 loss: 0.01229932252317667\n",
            "training 37400 loss: 0.0011187833733856678\n",
            "training 37500 loss: 0.011056669987738132\n",
            "training 37600 loss: 0.00913241133093834\n",
            "training 37700 loss: 0.006799403578042984\n",
            "training 37800 loss: 0.00033145409543067217\n",
            "training 37900 loss: 0.004778026137501001\n",
            "training 38000 loss: 0.012968364171683788\n",
            "training 38100 loss: 0.005868009757250547\n",
            "training 38200 loss: 0.0062521169893443584\n",
            "training 38300 loss: 0.00075540627585724\n",
            "training 38400 loss: 0.01666412316262722\n",
            "training 38500 loss: 0.0011262120679020882\n",
            "training 38600 loss: 0.00016244471771642566\n",
            "training 38700 loss: 0.004950521979480982\n",
            "training 38800 loss: 0.0013807533541694283\n",
            "training 38900 loss: 0.017284037545323372\n",
            "training 39000 loss: 0.0003851947549264878\n",
            "training 39100 loss: 0.0007181589026004076\n",
            "training 39200 loss: 0.01327764242887497\n",
            "training 39300 loss: 0.0009271508315578103\n",
            "training 39400 loss: 0.010404243133962154\n",
            "training 39500 loss: 0.025898976251482964\n",
            "training 39600 loss: 5.801500810775906e-05\n",
            "training 39700 loss: 0.0007157202926464379\n",
            "training 39800 loss: 0.00020322675118222833\n",
            "training 39900 loss: 0.016664184629917145\n",
            "training 40000 loss: 0.0020959561225026846\n",
            "40000 Validation Loss: 0.009\n",
            "training 40100 loss: 7.733984966762364e-05\n",
            "training 40200 loss: 0.027133876457810402\n",
            "training 40300 loss: 1.955574407475069e-05\n",
            "training 40400 loss: 0.01048220880329609\n",
            "training 40500 loss: 0.0051629794761538506\n",
            "training 40600 loss: 0.0300893671810627\n",
            "training 40700 loss: 0.00827194843441248\n",
            "training 40800 loss: 0.002402264392003417\n",
            "training 40900 loss: 0.006779758259654045\n",
            "training 41000 loss: 0.06506944447755814\n",
            "training 41100 loss: 8.996534597827122e-05\n",
            "training 41200 loss: 0.0012728282017633319\n",
            "training 41300 loss: 7.980395457707345e-05\n",
            "training 41400 loss: 0.00578382657840848\n",
            "training 41500 loss: 0.003256306517869234\n",
            "training 41600 loss: 0.006596209481358528\n",
            "training 41700 loss: 0.0011712518753483891\n",
            "training 41800 loss: 0.009240864776074886\n",
            "training 41900 loss: 0.0010989513248205185\n",
            "training 42000 loss: 0.01635606400668621\n",
            "training 42100 loss: 0.004693104885518551\n",
            "training 42200 loss: 0.0019465177319943905\n",
            "training 42300 loss: 0.0019384475890547037\n",
            "training 42400 loss: 0.001983996480703354\n",
            "training 42500 loss: 0.012837179005146027\n",
            "training 42600 loss: 2.9017930501140654e-05\n",
            "training 42700 loss: 0.0016768488567322493\n",
            "training 42800 loss: 0.019832495599985123\n",
            "training 42900 loss: 0.001297683804295957\n",
            "training 43000 loss: 8.940989937400445e-05\n",
            "training 43100 loss: 0.009472636505961418\n",
            "training 43200 loss: 0.0002068729227175936\n",
            "training 43300 loss: 0.03465469181537628\n",
            "training 43400 loss: 0.017942344769835472\n",
            "training 43500 loss: 0.05182470753788948\n",
            "training 43600 loss: 0.000294267141725868\n",
            "training 43700 loss: 0.024821657687425613\n",
            "training 43800 loss: 0.13942469656467438\n",
            "training 43900 loss: 0.026244211941957474\n",
            "training 44000 loss: 0.0011580382706597447\n",
            "training 44100 loss: 0.03266431763768196\n",
            "training 44200 loss: 0.021048855036497116\n",
            "training 44300 loss: 0.005759084597229958\n",
            "training 44400 loss: 0.0010621179826557636\n",
            "training 44500 loss: 0.0024083212483674288\n",
            "training 44600 loss: 0.009790848940610886\n",
            "training 44700 loss: 0.00031210799352265894\n",
            "training 44800 loss: 0.01434075366705656\n",
            "training 44900 loss: 0.0023245627526193857\n",
            "training 45000 loss: 0.015190270729362965\n",
            "45000 Validation Loss: 0.009\n",
            "training 45100 loss: 0.00870192889124155\n",
            "training 45200 loss: 0.012674860656261444\n",
            "training 45300 loss: 0.0030495256651192904\n",
            "training 45400 loss: 0.006648828275501728\n",
            "training 45500 loss: 0.008998253382742405\n",
            "training 45600 loss: 0.01694909855723381\n",
            "training 45700 loss: 0.00029040270601399243\n",
            "training 45800 loss: 0.00046420469880104065\n",
            "training 45900 loss: 0.024066472426056862\n",
            "training 46000 loss: 0.0004659940314013511\n",
            "training 46100 loss: 0.010337909683585167\n",
            "training 46200 loss: 0.0068360986188054085\n",
            "training 46300 loss: 0.0013321961741894484\n",
            "training 46400 loss: 0.00013254904479254037\n",
            "training 46500 loss: 0.000717239105142653\n",
            "training 46600 loss: 3.939280941267498e-05\n",
            "training 46700 loss: 0.0004735281690955162\n",
            "training 46800 loss: 0.018474064767360687\n",
            "training 46900 loss: 0.01206046063452959\n",
            "training 47000 loss: 0.0004832901759073138\n",
            "training 47100 loss: 0.0003974665014538914\n",
            "training 47200 loss: 0.0002243955823360011\n",
            "training 47300 loss: 0.00044978130608797073\n",
            "training 47400 loss: 0.007969111204147339\n",
            "training 47500 loss: 0.027482792735099792\n",
            "[1] average loss per epoch: 0.024\n",
            "Saved model checkpoint to /content/drive/My Drive/Task4/test5/model_epoch1.pt\n",
            "training 100 loss: 0.0006597719038836658\n",
            "training 200 loss: 0.0030489463824778795\n",
            "training 300 loss: 0.12706515192985535\n",
            "training 400 loss: 0.006155968178063631\n",
            "training 500 loss: 0.002383580431342125\n",
            "training 600 loss: 0.0005507142632268369\n",
            "training 700 loss: 0.0017323051579296589\n",
            "training 800 loss: 0.003476192709058523\n",
            "training 900 loss: 0.014134673401713371\n",
            "training 1000 loss: 0.007069643586874008\n",
            "training 1100 loss: 0.006205602083355188\n",
            "training 1200 loss: 3.904628101736307e-06\n",
            "training 1300 loss: 0.007971026934683323\n",
            "training 1400 loss: 0.019923456013202667\n",
            "training 1500 loss: 8.860464004101232e-05\n",
            "training 1600 loss: 2.5669578462839127e-08\n",
            "training 1700 loss: 0.0016875662840902805\n",
            "training 1800 loss: 0.047305408865213394\n",
            "training 1900 loss: 0.013709395192563534\n",
            "training 2000 loss: 0.0013038060860708356\n",
            "training 2100 loss: 0.004628940485417843\n",
            "training 2200 loss: 5.470388941830606e-08\n",
            "training 2300 loss: 2.9689049370063003e-06\n",
            "training 2400 loss: 0.08441375941038132\n",
            "training 2500 loss: 0.001033425098285079\n",
            "training 2600 loss: 0.006339326035231352\n",
            "training 2700 loss: 0.004373545292764902\n",
            "training 2800 loss: 0.0014036333886906505\n",
            "training 2900 loss: 0.0005584286991506815\n",
            "training 3000 loss: 0.025487693026661873\n",
            "training 3100 loss: 0.00015810440527275205\n",
            "training 3200 loss: 7.560748144896934e-06\n",
            "training 3300 loss: 0.00017367515829391778\n",
            "training 3400 loss: 0.0001706158509477973\n",
            "training 3500 loss: 0.08077909797430038\n",
            "training 3600 loss: 0.012834964320063591\n",
            "training 3700 loss: 0.0033240551128983498\n",
            "training 3800 loss: 0.00024318044597748667\n",
            "training 3900 loss: 0.009299354627728462\n",
            "training 4000 loss: 0.02798856794834137\n",
            "training 4100 loss: 0.023750025779008865\n",
            "training 4200 loss: 0.002054748358204961\n",
            "training 4300 loss: 0.0017857003258541226\n",
            "training 4400 loss: 0.026501767337322235\n",
            "training 4500 loss: 0.015751797705888748\n",
            "training 4600 loss: 0.0002091147907776758\n",
            "training 4700 loss: 0.009173879399895668\n",
            "training 4800 loss: 0.00043229764560237527\n",
            "training 4900 loss: 0.014145673252642155\n",
            "training 5000 loss: 0.016375644132494926\n",
            "5000 Validation Loss: 0.008\n",
            "training 5100 loss: 0.032037295401096344\n",
            "training 5200 loss: 0.000790709862485528\n",
            "training 5300 loss: 0.002182204043492675\n",
            "training 5400 loss: 0.0010598036460578442\n",
            "training 5500 loss: 0.00451955059543252\n",
            "training 5600 loss: 0.01458060834556818\n",
            "training 5700 loss: 0.010637834668159485\n",
            "training 5800 loss: 0.0005496405647136271\n",
            "training 5900 loss: 0.0018144279019907117\n",
            "training 6000 loss: 0.00017280277097597718\n",
            "training 6100 loss: 0.017216399312019348\n",
            "training 6200 loss: 0.0005558175616897643\n",
            "training 6300 loss: 0.0007413573912344873\n",
            "training 6400 loss: 0.010638818144798279\n",
            "training 6500 loss: 0.00020631776715163141\n",
            "training 6600 loss: 0.0007715460960753262\n",
            "training 6700 loss: 4.576372157316655e-05\n",
            "training 6800 loss: 0.01420335192233324\n",
            "training 6900 loss: 0.06570030748844147\n",
            "training 7000 loss: 0.018990082666277885\n",
            "training 7100 loss: 0.007326893974095583\n",
            "training 7200 loss: 0.012842636555433273\n",
            "training 7300 loss: 0.018196426331996918\n",
            "training 7400 loss: 0.0009987708181142807\n",
            "training 7500 loss: 0.004257489927113056\n",
            "training 7600 loss: 0.0065603191033005714\n",
            "training 7700 loss: 0.013184946961700916\n",
            "training 7800 loss: 0.0011350787244737148\n",
            "training 7900 loss: 0.0031258338131010532\n",
            "training 8000 loss: 0.0275145024061203\n",
            "training 8100 loss: 0.02023358829319477\n",
            "training 8200 loss: 0.013518672436475754\n",
            "training 8300 loss: 0.0008182041929103434\n",
            "training 8400 loss: 0.02032763883471489\n",
            "training 8500 loss: 0.0062923738732934\n",
            "training 8600 loss: 0.005678995046764612\n",
            "training 8700 loss: 0.0004545468545984477\n",
            "training 8800 loss: 0.015300014987587929\n",
            "training 8900 loss: 0.006923188455402851\n",
            "training 9000 loss: 0.0029174829833209515\n",
            "training 9100 loss: 0.008493814617395401\n",
            "training 9200 loss: 0.03207571431994438\n",
            "training 9300 loss: 5.6231165217468515e-06\n",
            "training 9400 loss: 0.016793763265013695\n",
            "training 9500 loss: 0.057543713599443436\n",
            "training 9600 loss: 0.0007566912099719048\n",
            "training 9700 loss: 0.0043052262626588345\n",
            "training 9800 loss: 0.0008425935520790517\n",
            "training 9900 loss: 0.004308981820940971\n",
            "training 10000 loss: 0.012143814004957676\n",
            "10000 Validation Loss: 0.007\n",
            "training 10100 loss: 0.0011612046509981155\n",
            "training 10200 loss: 0.0006110643153078854\n",
            "training 10300 loss: 0.002892399439588189\n",
            "training 10400 loss: 0.0026116501539945602\n",
            "training 10500 loss: 0.00797685980796814\n",
            "training 10600 loss: 0.0036287177354097366\n",
            "training 10700 loss: 0.00014297387679107487\n",
            "training 10800 loss: 2.5168319552903995e-05\n",
            "training 10900 loss: 0.00016646215226501226\n",
            "training 11000 loss: 0.0038246645126491785\n",
            "training 11100 loss: 6.853140803286806e-05\n",
            "training 11200 loss: 0.0023434299509972334\n",
            "training 11300 loss: 0.0012604736257344484\n",
            "training 11400 loss: 0.00018907115736510605\n",
            "training 11500 loss: 0.0030003006104379892\n",
            "training 11600 loss: 0.021628722548484802\n",
            "training 11700 loss: 0.0003365433367434889\n",
            "training 11800 loss: 0.004220297560095787\n",
            "training 11900 loss: 2.7480500648380257e-06\n",
            "training 12000 loss: 0.000883665110450238\n",
            "training 12100 loss: 0.0020054206252098083\n",
            "training 12200 loss: 0.011964820325374603\n",
            "training 12300 loss: 0.007735955994576216\n",
            "training 12400 loss: 0.06655152142047882\n",
            "training 12500 loss: 1.7121012206189334e-05\n",
            "training 12600 loss: 0.00877320021390915\n",
            "training 12700 loss: 0.0007692431681789458\n",
            "training 12800 loss: 7.5581260716717225e-06\n",
            "training 12900 loss: 0.013154303655028343\n",
            "training 13000 loss: 0.00010983386891894042\n",
            "training 13100 loss: 0.0018770303577184677\n",
            "training 13200 loss: 0.007982354611158371\n",
            "training 13300 loss: 0.024726789444684982\n",
            "training 13400 loss: 0.00048409769078716636\n",
            "training 13500 loss: 0.003283546306192875\n",
            "training 13600 loss: 0.007521307561546564\n",
            "training 13700 loss: 0.0006068047950975597\n",
            "training 13800 loss: 0.006209020968526602\n",
            "training 13900 loss: 0.0001069594727596268\n",
            "training 14000 loss: 0.0018923283787444234\n",
            "training 14100 loss: 0.005886726547032595\n",
            "training 14200 loss: 0.016643386334180832\n",
            "training 14300 loss: 3.7756819892820204e-08\n",
            "training 14400 loss: 0.0011784431990236044\n",
            "training 14500 loss: 0.0016146213747560978\n",
            "training 14600 loss: 0.00025046939845196903\n",
            "training 14700 loss: 3.0683172553835902e-06\n",
            "training 14800 loss: 0.006039831321686506\n",
            "training 14900 loss: 0.0005503562279045582\n",
            "training 15000 loss: 0.00040290364995598793\n",
            "15000 Validation Loss: 0.007\n",
            "training 15100 loss: 0.051762089133262634\n",
            "training 15200 loss: 0.004192896652966738\n",
            "training 15300 loss: 0.0010609682649374008\n",
            "training 15400 loss: 0.0021001496352255344\n",
            "training 15500 loss: 0.0014206921914592385\n",
            "training 15600 loss: 0.0016923296498134732\n",
            "training 15700 loss: 0.1328684538602829\n",
            "training 15800 loss: 0.03378506377339363\n",
            "training 15900 loss: 0.016654768958687782\n",
            "training 16000 loss: 0.0038335754070430994\n",
            "training 16100 loss: 0.004065069369971752\n",
            "training 16200 loss: 0.016856294125318527\n",
            "training 16300 loss: 0.000620364211499691\n",
            "training 16400 loss: 0.00036267098039388657\n",
            "training 16500 loss: 0.026606278494000435\n",
            "training 16600 loss: 0.00112517224624753\n",
            "training 16700 loss: 0.0004527086275629699\n",
            "training 16800 loss: 0.0045858281664550304\n",
            "training 16900 loss: 0.020853735506534576\n",
            "training 17000 loss: 0.017002839595079422\n",
            "training 17100 loss: 0.001498380908742547\n",
            "training 17200 loss: 0.0062134177424013615\n",
            "training 17300 loss: 0.0003295902570243925\n",
            "training 17400 loss: 0.0017113317735493183\n",
            "training 17500 loss: 3.3649960187176475e-06\n",
            "training 17600 loss: 4.180707037448883e-06\n",
            "training 17700 loss: 0.01179906539618969\n",
            "training 17800 loss: 0.010445626452565193\n",
            "training 17900 loss: 0.009889376349747181\n",
            "training 18000 loss: 0.001692486577667296\n",
            "training 18100 loss: 0.0028148633427917957\n",
            "training 18200 loss: 0.004040632396936417\n",
            "training 18300 loss: 0.00862173642963171\n",
            "training 18400 loss: 0.011647534556686878\n",
            "training 18500 loss: 0.01674584485590458\n",
            "training 18600 loss: 0.0005180245498195291\n",
            "training 18700 loss: 0.0019685854203999043\n",
            "training 18800 loss: 0.0006119015160948038\n",
            "training 18900 loss: 0.0017721048789098859\n",
            "training 19000 loss: 0.003125673858448863\n",
            "training 19100 loss: 0.00011204875045223162\n",
            "training 19200 loss: 7.669965270906687e-06\n",
            "training 19300 loss: 0.001987396040931344\n",
            "training 19400 loss: 0.014313699677586555\n",
            "training 19500 loss: 0.003357956185936928\n",
            "training 19600 loss: 0.005940409377217293\n",
            "training 19700 loss: 0.004842614755034447\n",
            "training 19800 loss: 0.002429310232400894\n",
            "training 19900 loss: 0.013597069308161736\n",
            "training 20000 loss: 0.006485760677605867\n",
            "20000 Validation Loss: 0.007\n",
            "training 20100 loss: 0.032805632799863815\n",
            "training 20200 loss: 0.0009139853646047413\n",
            "training 20300 loss: 0.0010014248546212912\n",
            "training 20400 loss: 0.0006747372099198401\n",
            "training 20500 loss: 0.0043481336906552315\n",
            "training 20600 loss: 2.1129744709469378e-05\n",
            "training 20700 loss: 0.017924657091498375\n",
            "training 20800 loss: 0.0022917119786143303\n",
            "training 20900 loss: 4.965597327100113e-05\n",
            "training 21000 loss: 0.004588315263390541\n",
            "training 21100 loss: 0.004507601261138916\n",
            "training 21200 loss: 0.03516849875450134\n",
            "training 21300 loss: 0.00035883995587937534\n",
            "training 21400 loss: 0.0004865137452725321\n",
            "training 21500 loss: 0.007098660804331303\n",
            "training 21600 loss: 1.8341630493523553e-05\n",
            "training 21700 loss: 0.0010668629547581077\n",
            "training 21800 loss: 0.0004990462912246585\n",
            "training 21900 loss: 0.0034337863326072693\n",
            "training 22000 loss: 0.0004659528494812548\n",
            "training 22100 loss: 0.0007756176055409014\n",
            "training 22200 loss: 0.0017798012122511864\n",
            "training 22300 loss: 0.0039411066100001335\n",
            "training 22400 loss: 0.010846439749002457\n",
            "training 22500 loss: 0.004621676169335842\n",
            "training 22600 loss: 0.004810018464922905\n",
            "training 22700 loss: 0.002435330767184496\n",
            "training 22800 loss: 1.6091271390905604e-05\n",
            "training 22900 loss: 0.00012101393076591194\n",
            "training 23000 loss: 0.001689505996182561\n",
            "training 23100 loss: 0.012628627941012383\n",
            "training 23200 loss: 0.002283296547830105\n",
            "training 23300 loss: 0.00040385176544077694\n",
            "training 23400 loss: 0.01629604957997799\n",
            "training 23500 loss: 0.0005104014999233186\n",
            "training 23600 loss: 0.045965030789375305\n",
            "training 23700 loss: 0.006077763624489307\n",
            "training 23800 loss: 0.0002296330058015883\n",
            "training 23900 loss: 0.007285646162927151\n",
            "training 24000 loss: 0.00021273666061460972\n",
            "training 24100 loss: 0.003705700160935521\n",
            "training 24200 loss: 0.022176118567585945\n",
            "training 24300 loss: 0.014123166911303997\n",
            "training 24400 loss: 0.012432042509317398\n",
            "training 24500 loss: 0.000534167280420661\n",
            "training 24600 loss: 0.0154820391908288\n",
            "training 24700 loss: 0.06912048161029816\n",
            "training 24800 loss: 0.0004230774939060211\n",
            "training 24900 loss: 0.0011930896434932947\n",
            "training 25000 loss: 0.001294044777750969\n",
            "25000 Validation Loss: 0.006\n",
            "training 25100 loss: 0.0007487240945920348\n",
            "training 25200 loss: 0.006598300766199827\n",
            "training 25300 loss: 0.012897110544145107\n",
            "training 25400 loss: 0.0008067870512604713\n",
            "training 25500 loss: 0.0016909567639231682\n",
            "training 25600 loss: 0.030928056687116623\n",
            "training 25700 loss: 0.00031801569275557995\n",
            "training 25800 loss: 0.0019521177746355534\n",
            "training 25900 loss: 0.0075487918220460415\n",
            "training 26000 loss: 0.0028737345710396767\n",
            "training 26100 loss: 0.00575485173612833\n",
            "training 26200 loss: 0.0009026610059663653\n",
            "training 26300 loss: 0.01828368753194809\n",
            "training 26400 loss: 0.000884232169482857\n",
            "training 26500 loss: 0.0022140166256576777\n",
            "training 26600 loss: 0.00011979481496382505\n",
            "training 26700 loss: 0.00200947979465127\n",
            "training 26800 loss: 0.019191093742847443\n",
            "training 26900 loss: 0.005783355329185724\n",
            "training 27000 loss: 0.007230641320347786\n",
            "training 27100 loss: 0.002163555473089218\n",
            "training 27200 loss: 0.0024088595528155565\n",
            "training 27300 loss: 0.0010654461802914739\n",
            "training 27400 loss: 0.018000643700361252\n",
            "training 27500 loss: 0.0001899507624329999\n",
            "training 27600 loss: 0.0019494642037898302\n",
            "training 27700 loss: 0.003568875603377819\n",
            "training 27800 loss: 0.006271020974963903\n",
            "training 27900 loss: 0.00018036807887256145\n",
            "training 28000 loss: 0.0018348573939874768\n",
            "training 28100 loss: 0.0026673332322388887\n",
            "training 28200 loss: 0.000535347091499716\n",
            "training 28300 loss: 0.0018465183675289154\n",
            "training 28400 loss: 8.363577944692224e-05\n",
            "training 28500 loss: 0.0028723543509840965\n",
            "training 28600 loss: 0.009299769066274166\n",
            "training 28700 loss: 0.016583215445280075\n",
            "training 28800 loss: 0.018699057400226593\n",
            "training 28900 loss: 0.0024961105082184076\n",
            "training 29000 loss: 0.002641635946929455\n",
            "training 29100 loss: 0.0017747756792232394\n",
            "training 29200 loss: 0.005975374951958656\n",
            "training 29300 loss: 0.000224295596126467\n",
            "training 29400 loss: 0.04845394939184189\n",
            "training 29500 loss: 0.0059485710225999355\n",
            "training 29600 loss: 0.11772976815700531\n",
            "training 29700 loss: 0.015042122453451157\n",
            "training 29800 loss: 0.008294384926557541\n",
            "training 29900 loss: 0.001803577528335154\n",
            "training 30000 loss: 0.0022840711753815413\n",
            "30000 Validation Loss: 0.008\n",
            "training 30100 loss: 0.014224102720618248\n",
            "training 30200 loss: 0.009549972601234913\n",
            "training 30300 loss: 0.01152896136045456\n",
            "training 30400 loss: 0.0007089878199622035\n",
            "training 30500 loss: 0.0004605749563779682\n",
            "training 30600 loss: 0.004342350177466869\n",
            "training 30700 loss: 0.0005954761290922761\n",
            "training 30800 loss: 0.010798817500472069\n",
            "training 30900 loss: 0.005497810896486044\n",
            "training 31000 loss: 0.005987286567687988\n",
            "training 31100 loss: 0.0009791719494387507\n",
            "training 31200 loss: 0.00035681045847013593\n",
            "training 31300 loss: 0.01134121511131525\n",
            "training 31400 loss: 0.014897552318871021\n",
            "training 31500 loss: 0.004337104503065348\n",
            "training 31600 loss: 1.0264775482937694e-05\n",
            "training 31700 loss: 0.0034022845793515444\n",
            "training 31800 loss: 0.016410382464528084\n",
            "training 31900 loss: 0.002042446518316865\n",
            "training 32000 loss: 0.0065654185600578785\n",
            "training 32100 loss: 0.0004551875463221222\n",
            "training 32200 loss: 0.0011771504068747163\n",
            "training 32300 loss: 0.0022119751665741205\n",
            "training 32400 loss: 0.009254850447177887\n",
            "training 32500 loss: 0.0009282546234317124\n",
            "training 32600 loss: 0.004782543051987886\n",
            "training 32700 loss: 0.007496969774365425\n",
            "training 32800 loss: 9.997597226174548e-05\n",
            "training 32900 loss: 0.016008857637643814\n",
            "training 33000 loss: 0.0029051846358925104\n",
            "training 33100 loss: 0.01260880846530199\n",
            "training 33200 loss: 0.0003744521818589419\n",
            "training 33300 loss: 0.00011965393787249923\n",
            "training 33400 loss: 0.0018900473369285464\n",
            "training 33500 loss: 0.0007720229914411902\n",
            "training 33600 loss: 0.004612571559846401\n",
            "training 33700 loss: 0.006349998991936445\n",
            "training 33800 loss: 0.07112318277359009\n",
            "training 33900 loss: 0.0228786189109087\n",
            "training 34000 loss: 0.008832165040075779\n",
            "training 34100 loss: 0.02429603971540928\n",
            "training 34200 loss: 0.02255406230688095\n",
            "training 34300 loss: 0.0008465429418720305\n",
            "training 34400 loss: 0.023141035810112953\n",
            "training 34500 loss: 0.003920478746294975\n",
            "training 34600 loss: 0.02266785316169262\n",
            "training 34700 loss: 0.00840558111667633\n",
            "training 34800 loss: 0.0017427003476768732\n",
            "training 34900 loss: 0.001776764984242618\n",
            "training 35000 loss: 0.003947635181248188\n",
            "35000 Validation Loss: 0.006\n",
            "training 35100 loss: 0.0011496965307742357\n",
            "training 35200 loss: 0.003712001722306013\n",
            "training 35300 loss: 8.154104762070347e-06\n",
            "training 35400 loss: 1.0642752386047505e-06\n",
            "training 35500 loss: 2.744722405623179e-05\n",
            "training 35600 loss: 0.00349254603497684\n",
            "training 35700 loss: 0.0005760652711614966\n",
            "training 35800 loss: 0.0044996971264481544\n",
            "training 35900 loss: 9.160548506770283e-05\n",
            "training 36000 loss: 0.0012856362154707313\n",
            "training 36100 loss: 0.00228081364184618\n",
            "training 36200 loss: 0.013536198064684868\n",
            "training 36300 loss: 0.0009154563886113465\n",
            "training 36400 loss: 0.00017477029177825898\n",
            "training 36500 loss: 0.009927063249051571\n",
            "training 36600 loss: 0.019334373995661736\n",
            "training 36700 loss: 0.01092783734202385\n",
            "training 36800 loss: 0.014232235960662365\n",
            "training 36900 loss: 0.011962264776229858\n",
            "training 37000 loss: 0.0051692514680325985\n",
            "training 37100 loss: 0.02099161222577095\n",
            "training 37200 loss: 0.002804323798045516\n",
            "training 37300 loss: 0.011319339275360107\n",
            "training 37400 loss: 0.00011861302482429892\n",
            "training 37500 loss: 0.007166073191910982\n",
            "training 37600 loss: 0.0036637012381106615\n",
            "training 37700 loss: 0.0016937422333285213\n",
            "training 37800 loss: 0.0004744520992971957\n",
            "training 37900 loss: 3.26218651025556e-05\n",
            "training 38000 loss: 0.009700327180325985\n",
            "training 38100 loss: 0.00330590782687068\n",
            "training 38200 loss: 0.0070139444433152676\n",
            "training 38300 loss: 0.0033950847573578358\n",
            "training 38400 loss: 0.02049044705927372\n",
            "training 38500 loss: 0.005414301063865423\n",
            "training 38600 loss: 0.0003068484365940094\n",
            "training 38700 loss: 0.0006717059877701104\n",
            "training 38800 loss: 0.0003785880981013179\n",
            "training 38900 loss: 0.006396211683750153\n",
            "training 39000 loss: 4.113246632186929e-07\n",
            "training 39100 loss: 0.0001677936816122383\n",
            "training 39200 loss: 0.01600300706923008\n",
            "training 39300 loss: 0.001200578990392387\n",
            "training 39400 loss: 0.010366340167820454\n",
            "training 39500 loss: 0.024419277906417847\n",
            "training 39600 loss: 0.0006936955614946783\n",
            "training 39700 loss: 0.000646101136226207\n",
            "training 39800 loss: 6.066873083909741e-06\n",
            "training 39900 loss: 0.01868823543190956\n",
            "training 40000 loss: 0.00013418453454505652\n",
            "40000 Validation Loss: 0.007\n",
            "training 40100 loss: 0.00027592069818638265\n",
            "training 40200 loss: 0.009686618112027645\n",
            "training 40300 loss: 0.0008394683245569468\n",
            "training 40400 loss: 0.009015087969601154\n",
            "training 40500 loss: 0.008864639326930046\n",
            "training 40600 loss: 0.04319259524345398\n",
            "training 40700 loss: 0.004538355395197868\n",
            "training 40800 loss: 0.0008828148129396141\n",
            "training 40900 loss: 0.00875587947666645\n",
            "training 41000 loss: 0.047788165509700775\n",
            "training 41100 loss: 0.00017082768317777663\n",
            "training 41200 loss: 0.00010855827713385224\n",
            "training 41300 loss: 0.0018928261706605554\n",
            "training 41400 loss: 0.003482830710709095\n",
            "training 41500 loss: 0.013286764733493328\n",
            "training 41600 loss: 0.004942607134580612\n",
            "training 41700 loss: 0.0005095400847494602\n",
            "training 41800 loss: 0.003517218865454197\n",
            "training 41900 loss: 0.00018075252592097968\n",
            "training 42000 loss: 0.01461984496563673\n",
            "training 42100 loss: 0.0021203237120062113\n",
            "training 42200 loss: 0.0038902328815311193\n",
            "training 42300 loss: 0.0010390739189460874\n",
            "training 42400 loss: 0.003894278546795249\n",
            "training 42500 loss: 0.007841033861041069\n",
            "training 42600 loss: 1.2424075976014137e-05\n",
            "training 42700 loss: 5.124779272591695e-05\n",
            "training 42800 loss: 0.011629993095993996\n",
            "training 42900 loss: 0.007694784086197615\n",
            "training 43000 loss: 0.00036179975722916424\n",
            "training 43100 loss: 0.011453412473201752\n",
            "training 43200 loss: 0.0002988986379932612\n",
            "training 43300 loss: 0.025174185633659363\n",
            "training 43400 loss: 0.01219420600682497\n",
            "training 43500 loss: 0.029888130724430084\n",
            "training 43600 loss: 1.1803427696577273e-05\n",
            "training 43700 loss: 0.01085970364511013\n",
            "training 43800 loss: 0.06750506162643433\n",
            "training 43900 loss: 0.009927015751600266\n",
            "training 44000 loss: 0.0007085815886966884\n",
            "training 44100 loss: 0.022044451907277107\n",
            "training 44200 loss: 0.017843926325440407\n",
            "training 44300 loss: 0.004725467413663864\n",
            "training 44400 loss: 0.00464160181581974\n",
            "training 44500 loss: 0.003826316213235259\n",
            "training 44600 loss: 0.005959756672382355\n",
            "training 44700 loss: 6.730142922606319e-05\n",
            "training 44800 loss: 0.011658035218715668\n",
            "training 44900 loss: 0.0036153446417301893\n",
            "training 45000 loss: 0.012513894587755203\n",
            "45000 Validation Loss: 0.007\n",
            "training 45100 loss: 0.004763732198625803\n",
            "training 45200 loss: 0.021634193137288094\n",
            "training 45300 loss: 0.005272440146654844\n",
            "training 45400 loss: 0.007317019626498222\n",
            "training 45500 loss: 0.0056320903822779655\n",
            "training 45600 loss: 0.010369835421442986\n",
            "training 45700 loss: 0.0013537945924326777\n",
            "training 45800 loss: 5.69917174289003e-05\n",
            "training 45900 loss: 0.016233230009675026\n",
            "training 46000 loss: 2.8776979888789356e-05\n",
            "training 46100 loss: 0.008276155218482018\n",
            "training 46200 loss: 0.007244271691888571\n",
            "training 46300 loss: 0.0023977323435246944\n",
            "training 46400 loss: 0.0006329813622869551\n",
            "training 46500 loss: 0.0028233956545591354\n",
            "training 46600 loss: 0.0004054632445331663\n",
            "training 46700 loss: 0.002002517692744732\n",
            "training 46800 loss: 0.0029426487162709236\n",
            "training 46900 loss: 0.01108938455581665\n",
            "training 47000 loss: 5.870712993782945e-05\n",
            "training 47100 loss: 0.0006590862176381052\n",
            "training 47200 loss: 4.067197733093053e-05\n",
            "training 47300 loss: 0.0013480460038408637\n",
            "training 47400 loss: 0.007706123404204845\n",
            "training 47500 loss: 0.019908180460333824\n",
            "[2] average loss per epoch: 0.008\n",
            "Saved model checkpoint to /content/drive/My Drive/Task4/test5/model_epoch2.pt\n",
            "training 100 loss: 7.631586777279153e-05\n",
            "training 200 loss: 0.004348731134086847\n",
            "training 300 loss: 0.09355415403842926\n",
            "training 400 loss: 0.0012402236461639404\n",
            "training 500 loss: 0.003438398241996765\n",
            "training 600 loss: 3.553566421032883e-05\n",
            "training 700 loss: 0.0018066366901621222\n",
            "training 800 loss: 0.0016157136997208\n",
            "training 900 loss: 0.01117602176964283\n",
            "training 1000 loss: 0.00436973012983799\n",
            "training 1100 loss: 0.008973934687674046\n",
            "training 1200 loss: 0.0007604866987094283\n",
            "training 1300 loss: 0.004705951549112797\n",
            "training 1400 loss: 0.012631414458155632\n",
            "training 1500 loss: 8.812950545689091e-05\n",
            "training 1600 loss: 7.131844176910818e-05\n",
            "training 1700 loss: 0.0024713247548788786\n",
            "training 1800 loss: 0.055254314094781876\n",
            "training 1900 loss: 0.016443079337477684\n",
            "training 2000 loss: 7.011548586888239e-05\n",
            "training 2100 loss: 0.005798413883894682\n",
            "training 2200 loss: 0.00012846906611230224\n",
            "training 2300 loss: 0.0008483058190904558\n",
            "training 2400 loss: 0.04131470248103142\n",
            "training 2500 loss: 0.0018511931411921978\n",
            "training 2600 loss: 0.002889938186854124\n",
            "training 2700 loss: 0.0008881358080543578\n",
            "training 2800 loss: 0.004531098064035177\n",
            "training 2900 loss: 0.0012357775121927261\n",
            "training 3000 loss: 0.012556016445159912\n",
            "training 3100 loss: 0.0002747338730841875\n",
            "training 3200 loss: 0.0002687755913939327\n",
            "training 3300 loss: 0.000574681325815618\n",
            "training 3400 loss: 0.0003144796355627477\n",
            "training 3500 loss: 0.04411844164133072\n",
            "training 3600 loss: 0.009349497966468334\n",
            "training 3700 loss: 0.0024410285986959934\n",
            "training 3800 loss: 0.0013448790414258838\n",
            "training 3900 loss: 0.007685793563723564\n",
            "training 4000 loss: 0.031216274946928024\n",
            "training 4100 loss: 0.02398834563791752\n",
            "training 4200 loss: 0.0010970236035063863\n",
            "training 4300 loss: 0.002042661886662245\n",
            "training 4400 loss: 0.017465874552726746\n",
            "training 4500 loss: 0.0064431666396558285\n",
            "training 4600 loss: 0.0012260410003364086\n",
            "training 4700 loss: 0.00861735362559557\n",
            "training 4800 loss: 0.00016649907047394663\n",
            "training 4900 loss: 0.01689916104078293\n",
            "training 5000 loss: 0.014885856769979\n",
            "5000 Validation Loss: 0.006\n",
            "training 5100 loss: 0.04183340072631836\n",
            "training 5200 loss: 0.0010738519486039877\n",
            "training 5300 loss: 0.002094231778755784\n",
            "training 5400 loss: 0.001123173744417727\n",
            "training 5500 loss: 0.004765279125422239\n",
            "training 5600 loss: 0.010275364853441715\n",
            "training 5700 loss: 0.004957301542162895\n",
            "training 5800 loss: 4.09552849305328e-05\n",
            "training 5900 loss: 0.0006314946222119033\n",
            "training 6000 loss: 2.445431164233014e-06\n",
            "training 6100 loss: 0.005402202717959881\n",
            "training 6200 loss: 0.0017689147498458624\n",
            "training 6300 loss: 0.0008944992441684008\n",
            "training 6400 loss: 0.0019331607036292553\n",
            "training 6500 loss: 9.567501547280699e-05\n",
            "training 6600 loss: 4.224696112942183e-06\n",
            "training 6700 loss: 8.770028944127262e-05\n",
            "training 6800 loss: 0.005199499428272247\n",
            "training 6900 loss: 0.054202280938625336\n",
            "training 7000 loss: 0.013586952351033688\n",
            "training 7100 loss: 0.002441264223307371\n",
            "training 7200 loss: 0.011580165475606918\n",
            "training 7300 loss: 0.009080173447728157\n",
            "training 7400 loss: 8.208189683500677e-09\n",
            "training 7500 loss: 0.0008746166131459177\n",
            "training 7600 loss: 0.0027850144542753696\n",
            "training 7700 loss: 0.014258245006203651\n",
            "training 7800 loss: 0.0023067796137183905\n",
            "training 7900 loss: 0.0018544361228123307\n",
            "training 8000 loss: 0.025032000616192818\n",
            "training 8100 loss: 0.01276930421590805\n",
            "training 8200 loss: 0.021601803600788116\n",
            "training 8300 loss: 0.0006661193328909576\n",
            "training 8400 loss: 0.012498590163886547\n",
            "training 8500 loss: 0.00023715793213341385\n",
            "training 8600 loss: 0.005535423289984465\n",
            "training 8700 loss: 0.0005481324042193592\n",
            "training 8800 loss: 0.010904819704592228\n",
            "training 8900 loss: 0.005181223154067993\n",
            "training 9000 loss: 0.0020313633140176535\n",
            "training 9100 loss: 0.008014978840947151\n",
            "training 9200 loss: 0.02625943347811699\n",
            "training 9300 loss: 3.1473337003262714e-06\n",
            "training 9400 loss: 0.0063085732981562614\n",
            "training 9500 loss: 0.05384119972586632\n",
            "training 9600 loss: 4.212943167658523e-06\n",
            "training 9700 loss: 0.003482802538201213\n",
            "training 9800 loss: 7.79701440478675e-05\n",
            "training 9900 loss: 0.0028597181662917137\n",
            "training 10000 loss: 0.014543436467647552\n",
            "10000 Validation Loss: 0.006\n",
            "training 10100 loss: 6.677047349512577e-05\n",
            "training 10200 loss: 7.256817866618803e-07\n",
            "training 10300 loss: 0.0031233816407620907\n",
            "training 10400 loss: 0.000617564539425075\n",
            "training 10500 loss: 0.003928573802113533\n",
            "training 10600 loss: 0.002552192425355315\n",
            "training 10700 loss: 0.001572403241880238\n",
            "training 10800 loss: 0.00023783399956300855\n",
            "training 10900 loss: 0.00039742846274748445\n",
            "training 11000 loss: 0.002610480645671487\n",
            "training 11100 loss: 0.0003431272052694112\n",
            "training 11200 loss: 0.0016075016465038061\n",
            "training 11300 loss: 0.0017284173518419266\n",
            "training 11400 loss: 0.0015656035393476486\n",
            "training 11500 loss: 0.0008213443215936422\n",
            "training 11600 loss: 0.01602316088974476\n",
            "training 11700 loss: 0.0013257123064249754\n",
            "training 11800 loss: 0.005504530388861895\n",
            "training 11900 loss: 1.5023647392808925e-06\n",
            "training 12000 loss: 0.00045893905917182565\n",
            "training 12100 loss: 0.0009829954942688346\n",
            "training 12200 loss: 0.005570529960095882\n",
            "training 12300 loss: 0.006295513827353716\n",
            "training 12400 loss: 0.03677073493599892\n",
            "training 12500 loss: 0.000526852672919631\n",
            "training 12600 loss: 0.006878348533064127\n",
            "training 12700 loss: 0.0009410101338289678\n",
            "training 12800 loss: 0.0016339350258931518\n",
            "training 12900 loss: 0.009970772080123425\n",
            "training 13000 loss: 0.0009923916077241302\n",
            "training 13100 loss: 0.0032713438849896193\n",
            "training 13200 loss: 0.006767749320715666\n",
            "training 13300 loss: 0.027141574770212173\n",
            "training 13400 loss: 0.0008084809524007142\n",
            "training 13500 loss: 0.0009564168285578489\n",
            "training 13600 loss: 0.011764802038669586\n",
            "training 13700 loss: 0.0005518002435564995\n",
            "training 13800 loss: 0.0032217870466411114\n",
            "training 13900 loss: 1.6953714521150687e-07\n",
            "training 14000 loss: 0.0008703770581632853\n",
            "training 14100 loss: 0.0041521773673594\n",
            "training 14200 loss: 0.011502147652208805\n",
            "training 14300 loss: 0.0004257789987605065\n",
            "training 14400 loss: 0.00013769256474915892\n",
            "training 14500 loss: 0.0009854937670752406\n",
            "training 14600 loss: 6.525567005155608e-05\n",
            "training 14700 loss: 9.180640336126089e-05\n",
            "training 14800 loss: 0.005519115831702948\n",
            "training 14900 loss: 0.0006175526650622487\n",
            "training 15000 loss: 0.0038822959177196026\n",
            "15000 Validation Loss: 0.006\n",
            "training 15100 loss: 0.04099683091044426\n",
            "training 15200 loss: 0.0015780997928231955\n",
            "training 15300 loss: 0.0038448027335107327\n",
            "training 15400 loss: 0.00038707812200300395\n",
            "training 15500 loss: 0.0019195780623704195\n",
            "training 15600 loss: 0.0011872005416080356\n",
            "training 15700 loss: 0.09680650383234024\n",
            "training 15800 loss: 0.025626813992857933\n",
            "training 15900 loss: 0.014257789589464664\n",
            "training 16000 loss: 0.0031781704165041447\n",
            "training 16100 loss: 0.003406569128856063\n",
            "training 16200 loss: 0.016872022300958633\n",
            "training 16300 loss: 0.0008750397246330976\n",
            "training 16400 loss: 0.0008237649453803897\n",
            "training 16500 loss: 0.01390280295163393\n",
            "training 16600 loss: 0.0002938828256446868\n",
            "training 16700 loss: 0.0006706312415190041\n",
            "training 16800 loss: 0.0010528143029659986\n",
            "training 16900 loss: 0.020881563425064087\n",
            "training 17000 loss: 0.019089434295892715\n",
            "training 17100 loss: 0.0035641200374811888\n",
            "training 17200 loss: 0.008963367901742458\n",
            "training 17300 loss: 0.00019230418547522277\n",
            "training 17400 loss: 0.0024243539664894342\n",
            "training 17500 loss: 4.2067404137924314e-05\n",
            "training 17600 loss: 4.7206622184603475e-06\n",
            "training 17700 loss: 0.017041224986314774\n",
            "training 17800 loss: 0.005332231055945158\n",
            "training 17900 loss: 0.0070846062153577805\n",
            "training 18000 loss: 0.0009942100150510669\n",
            "training 18100 loss: 0.00019148510182276368\n",
            "training 18200 loss: 0.0008207567734643817\n",
            "training 18300 loss: 0.004624140448868275\n",
            "training 18400 loss: 0.009593872353434563\n",
            "training 18500 loss: 0.012551635503768921\n",
            "training 18600 loss: 0.003104728413745761\n",
            "training 18700 loss: 0.0008192411623895168\n",
            "training 18800 loss: 0.0010882285423576832\n",
            "training 18900 loss: 0.00048372006858699024\n",
            "training 19000 loss: 0.00010385088535258546\n",
            "training 19100 loss: 0.00035947252763435245\n",
            "training 19200 loss: 0.0001348869554931298\n",
            "training 19300 loss: 0.00012084613263141364\n",
            "training 19400 loss: 0.009006442502140999\n",
            "training 19500 loss: 0.013712354004383087\n",
            "training 19600 loss: 0.008032192476093769\n",
            "training 19700 loss: 0.0026957541704177856\n",
            "training 19800 loss: 0.0021917258854955435\n",
            "training 19900 loss: 0.009865065105259418\n",
            "training 20000 loss: 0.001452900469303131\n",
            "20000 Validation Loss: 0.006\n",
            "training 20100 loss: 0.026869507506489754\n",
            "training 20200 loss: 0.002487636636942625\n",
            "training 20300 loss: 0.0011730965925380588\n",
            "training 20400 loss: 0.0007219591061584651\n",
            "training 20500 loss: 0.0038514286279678345\n",
            "training 20600 loss: 0.00015015466487966478\n",
            "training 20700 loss: 0.015976596623659134\n",
            "training 20800 loss: 1.2503335256042192e-08\n",
            "training 20900 loss: 9.798823884921148e-05\n",
            "training 21000 loss: 0.004011556506156921\n",
            "training 21100 loss: 0.0020906764548271894\n",
            "training 21200 loss: 0.02632739767432213\n",
            "training 21300 loss: 0.000678495445754379\n",
            "training 21400 loss: 0.0016393749974668026\n",
            "training 21500 loss: 0.006653961259871721\n",
            "training 21600 loss: 1.779640297172591e-05\n",
            "training 21700 loss: 0.0010900225024670362\n",
            "training 21800 loss: 0.0002397793432464823\n",
            "training 21900 loss: 0.0010214117355644703\n",
            "training 22000 loss: 0.0004934589378535748\n",
            "training 22100 loss: 3.725290298461914e-05\n",
            "training 22200 loss: 0.000957242795266211\n",
            "training 22300 loss: 0.002327782567590475\n",
            "training 22400 loss: 0.010096198879182339\n",
            "training 22500 loss: 0.0028330571949481964\n",
            "training 22600 loss: 0.0020773978903889656\n",
            "training 22700 loss: 0.002419566735625267\n",
            "training 22800 loss: 8.286569936899468e-05\n",
            "training 22900 loss: 0.0006588291726075113\n",
            "training 23000 loss: 0.000660507008433342\n",
            "training 23100 loss: 0.008742944337427616\n",
            "training 23200 loss: 0.001918157795444131\n",
            "training 23300 loss: 0.0004768856451846659\n",
            "training 23400 loss: 0.010358087718486786\n",
            "training 23500 loss: 0.0004765941121149808\n",
            "training 23600 loss: 0.03743377700448036\n",
            "training 23700 loss: 0.0062657734379172325\n",
            "training 23800 loss: 0.00057178147835657\n",
            "training 23900 loss: 0.008014637045562267\n",
            "training 24000 loss: 6.273437611525878e-05\n",
            "training 24100 loss: 0.0022321376018226147\n",
            "training 24200 loss: 0.007916712202131748\n",
            "training 24300 loss: 0.018179964274168015\n",
            "training 24400 loss: 0.005821530241519213\n",
            "training 24500 loss: 0.0008062048000283539\n",
            "training 24600 loss: 0.009717006236314774\n",
            "training 24700 loss: 0.04507172107696533\n",
            "training 24800 loss: 0.0002346531837247312\n",
            "training 24900 loss: 0.0002550178032834083\n",
            "training 25000 loss: 0.000911824288778007\n",
            "25000 Validation Loss: 0.005\n",
            "training 25100 loss: 0.0071497345343232155\n",
            "training 25200 loss: 0.003828351618722081\n",
            "training 25300 loss: 0.008422333747148514\n",
            "training 25400 loss: 0.00011382232332834974\n",
            "training 25500 loss: 0.003097930457442999\n",
            "training 25600 loss: 0.02625379152595997\n",
            "training 25700 loss: 0.00016115885227918625\n",
            "training 25800 loss: 0.004053403157740831\n",
            "training 25900 loss: 0.010138501413166523\n",
            "training 26000 loss: 0.003958187997341156\n",
            "training 26100 loss: 0.009517149068415165\n",
            "training 26200 loss: 0.0023860489018261433\n",
            "training 26300 loss: 0.012186940759420395\n",
            "training 26400 loss: 4.131922833039425e-05\n",
            "training 26500 loss: 0.00030961944139562547\n",
            "training 26600 loss: 4.6155787458701525e-06\n",
            "training 26700 loss: 0.00017491531616542488\n",
            "training 26800 loss: 0.022630034014582634\n",
            "training 26900 loss: 0.005528755486011505\n",
            "training 27000 loss: 0.006042425520718098\n",
            "training 27100 loss: 0.0023092536721378565\n",
            "training 27200 loss: 0.0012175905285403132\n",
            "training 27300 loss: 0.001431009266525507\n",
            "training 27400 loss: 0.011906891129910946\n",
            "training 27500 loss: 0.00013641420810017735\n",
            "training 27600 loss: 0.0002869837044272572\n",
            "training 27700 loss: 0.002361701102927327\n",
            "training 27800 loss: 0.007224722765386105\n",
            "training 27900 loss: 0.0002783497329801321\n",
            "training 28000 loss: 0.00048388788127340376\n",
            "training 28100 loss: 0.003621454117819667\n",
            "training 28200 loss: 0.0009357807575725019\n",
            "training 28300 loss: 0.00138348329346627\n",
            "training 28400 loss: 9.460527508053929e-05\n",
            "training 28500 loss: 0.004589348565787077\n",
            "training 28600 loss: 0.0036050875205546618\n",
            "training 28700 loss: 0.01970524527132511\n",
            "training 28800 loss: 0.012841232120990753\n",
            "training 28900 loss: 0.0003615367750171572\n",
            "training 29000 loss: 0.002435424830764532\n",
            "training 29100 loss: 0.0007380244205705822\n",
            "training 29200 loss: 0.0054417382925748825\n",
            "training 29300 loss: 0.00042182300239801407\n",
            "training 29400 loss: 0.02714133821427822\n",
            "training 29500 loss: 0.0029061613604426384\n",
            "training 29600 loss: 0.11421486735343933\n",
            "training 29700 loss: 0.01684880442917347\n",
            "training 29800 loss: 0.0054323854856193066\n",
            "training 29900 loss: 0.0004933424643240869\n",
            "training 30000 loss: 0.001711706630885601\n",
            "30000 Validation Loss: 0.006\n",
            "training 30100 loss: 0.007179278880357742\n",
            "training 30200 loss: 0.0055260611698031425\n",
            "training 30300 loss: 0.00956932082772255\n",
            "training 30400 loss: 0.0005027494626119733\n",
            "training 30500 loss: 6.66536288917996e-05\n",
            "training 30600 loss: 0.002650466049090028\n",
            "training 30700 loss: 0.004323799628764391\n",
            "training 30800 loss: 0.011827155947685242\n",
            "training 30900 loss: 0.008739066310226917\n",
            "training 31000 loss: 0.003363235853612423\n",
            "training 31100 loss: 0.0018438146216794848\n",
            "training 31200 loss: 3.053476757486351e-05\n",
            "training 31300 loss: 0.01415843702852726\n",
            "training 31400 loss: 0.01519955787807703\n",
            "training 31500 loss: 0.0017744542565196753\n",
            "training 31600 loss: 0.0003559553006198257\n",
            "training 31700 loss: 0.0051158396527171135\n",
            "training 31800 loss: 0.01898515410721302\n",
            "training 31900 loss: 0.001460052328184247\n",
            "training 32000 loss: 0.0033275200985372066\n",
            "training 32100 loss: 0.0015457430854439735\n",
            "training 32200 loss: 0.000322529929690063\n",
            "training 32300 loss: 0.0002758415066637099\n",
            "training 32400 loss: 0.010094378143548965\n",
            "training 32500 loss: 0.000774064683355391\n",
            "training 32600 loss: 0.00707353325560689\n",
            "training 32700 loss: 0.00216555199585855\n",
            "training 32800 loss: 0.0012231875443831086\n",
            "training 32900 loss: 0.012823298573493958\n",
            "training 33000 loss: 0.0018849510233849287\n",
            "training 33100 loss: 0.014525845646858215\n",
            "training 33200 loss: 0.00023339851759374142\n",
            "training 33300 loss: 0.00044216852984391153\n",
            "training 33400 loss: 0.0033308216370642185\n",
            "training 33500 loss: 0.0007167922449298203\n",
            "training 33600 loss: 0.0021495604887604713\n",
            "training 33700 loss: 0.004869796335697174\n",
            "training 33800 loss: 0.06521683186292648\n",
            "training 33900 loss: 0.017663994804024696\n",
            "training 34000 loss: 0.008587059564888477\n",
            "training 34100 loss: 0.028470221906900406\n",
            "training 34200 loss: 0.01380239985883236\n",
            "training 34300 loss: 0.00021889393974561244\n",
            "training 34400 loss: 0.02183884009718895\n",
            "training 34500 loss: 0.002886837115511298\n",
            "training 34600 loss: 0.01726410910487175\n",
            "training 34700 loss: 0.00553719699382782\n",
            "training 34800 loss: 0.0002662253682501614\n",
            "training 34900 loss: 0.0014556280802935362\n",
            "training 35000 loss: 0.0019006346119567752\n",
            "35000 Validation Loss: 0.005\n",
            "training 35100 loss: 0.0006365012959577143\n",
            "training 35200 loss: 0.002469997387379408\n",
            "training 35300 loss: 3.769952309085056e-05\n",
            "training 35400 loss: 3.864223253913224e-05\n",
            "training 35500 loss: 1.430017437087372e-05\n",
            "training 35600 loss: 0.0055657620541751385\n",
            "training 35700 loss: 0.000942136743105948\n",
            "training 35800 loss: 0.0010828555095940828\n",
            "training 35900 loss: 0.0011983495205640793\n",
            "training 36000 loss: 0.0013320394791662693\n",
            "training 36100 loss: 0.00411095330491662\n",
            "training 36200 loss: 0.007203257642686367\n",
            "training 36300 loss: 0.00039519768324680626\n",
            "training 36400 loss: 0.00011511304182931781\n",
            "training 36500 loss: 0.005981421563774347\n",
            "training 36600 loss: 0.02272496372461319\n",
            "training 36700 loss: 0.009044449776411057\n",
            "training 36800 loss: 0.017448484897613525\n",
            "training 36900 loss: 0.007415731903165579\n",
            "training 37000 loss: 0.0035942161921411753\n",
            "training 37100 loss: 0.014535673893988132\n",
            "training 37200 loss: 0.0028703613206744194\n",
            "training 37300 loss: 0.012114983052015305\n",
            "training 37400 loss: 0.001046526595018804\n",
            "training 37500 loss: 0.003345395438373089\n",
            "training 37600 loss: 0.0035047587007284164\n",
            "training 37700 loss: 0.0007189641473814845\n",
            "training 37800 loss: 0.002755324821919203\n",
            "training 37900 loss: 0.0010323984315618873\n",
            "training 38000 loss: 0.00524046178907156\n",
            "training 38100 loss: 0.0044973623007535934\n",
            "training 38200 loss: 0.005532620940357447\n",
            "training 38300 loss: 0.0030193976126611233\n",
            "training 38400 loss: 0.026979908347129822\n",
            "training 38500 loss: 0.0066294013522565365\n",
            "training 38600 loss: 0.0004922838998027146\n",
            "training 38700 loss: 4.6333212821991765e-07\n",
            "training 38800 loss: 0.00017533179197926074\n",
            "training 38900 loss: 0.0018618563190102577\n",
            "training 39000 loss: 0.00029936045757494867\n",
            "training 39100 loss: 8.28309784992598e-05\n",
            "training 39200 loss: 0.01428974885493517\n",
            "training 39300 loss: 0.0002502128772903234\n",
            "training 39400 loss: 0.008634182624518871\n",
            "training 39500 loss: 0.02047666162252426\n",
            "training 39600 loss: 0.001715338439680636\n",
            "training 39700 loss: 6.054811819922179e-05\n",
            "training 39800 loss: 0.0002921116538345814\n",
            "training 39900 loss: 0.024027207866311073\n",
            "training 40000 loss: 0.0003666684206109494\n",
            "40000 Validation Loss: 0.006\n",
            "training 40100 loss: 0.00019409369269851595\n",
            "training 40200 loss: 0.003102072048932314\n",
            "training 40300 loss: 0.0007755777915008366\n",
            "training 40400 loss: 0.004570664372295141\n",
            "training 40500 loss: 0.01028900034725666\n",
            "training 40600 loss: 0.044548556208610535\n",
            "training 40700 loss: 0.002220280934125185\n",
            "training 40800 loss: 0.001362545881420374\n",
            "training 40900 loss: 0.005840318277478218\n",
            "training 41000 loss: 0.034683190286159515\n",
            "training 41100 loss: 0.00016715191304683685\n",
            "training 41200 loss: 0.0004982796381227672\n",
            "training 41300 loss: 0.0028901686891913414\n",
            "training 41400 loss: 0.0012992473784834146\n",
            "training 41500 loss: 0.01817803457379341\n",
            "training 41600 loss: 0.0030142904724925756\n",
            "training 41700 loss: 0.0006919885636307299\n",
            "training 41800 loss: 0.002045615576207638\n",
            "training 41900 loss: 6.243135430850089e-08\n",
            "training 42000 loss: 0.012890666723251343\n",
            "training 42100 loss: 0.001403258298523724\n",
            "training 42200 loss: 0.003711856435984373\n",
            "training 42300 loss: 0.001122726360335946\n",
            "training 42400 loss: 0.004192834720015526\n",
            "training 42500 loss: 0.005994335748255253\n",
            "training 42600 loss: 0.00011354777961969376\n",
            "training 42700 loss: 0.000913149444386363\n",
            "training 42800 loss: 0.015511303208768368\n",
            "training 42900 loss: 0.008765519596636295\n",
            "training 43000 loss: 0.0008095388766378164\n",
            "training 43100 loss: 0.009271280840039253\n",
            "training 43200 loss: 0.0007527612615376711\n",
            "training 43300 loss: 0.015765205025672913\n",
            "training 43400 loss: 0.010358767583966255\n",
            "training 43500 loss: 0.017925359308719635\n",
            "training 43600 loss: 0.0003503012121655047\n",
            "training 43700 loss: 0.00805575679987669\n",
            "training 43800 loss: 0.04391096159815788\n",
            "training 43900 loss: 0.006919499021023512\n",
            "training 44000 loss: 0.0009698685025796294\n",
            "training 44100 loss: 0.01760624349117279\n",
            "training 44200 loss: 0.01457007322460413\n",
            "training 44300 loss: 0.0033558839932084084\n",
            "training 44400 loss: 0.00487871840596199\n",
            "training 44500 loss: 0.004119671415537596\n",
            "training 44600 loss: 0.004864606540650129\n",
            "training 44700 loss: 1.9019651517737657e-05\n",
            "training 44800 loss: 0.007950137369334698\n",
            "training 44900 loss: 0.0026662498712539673\n",
            "training 45000 loss: 0.007423535920679569\n",
            "45000 Validation Loss: 0.006\n",
            "training 45100 loss: 0.0057042124681174755\n",
            "training 45200 loss: 0.022424418479204178\n",
            "training 45300 loss: 0.0084901237860322\n",
            "training 45400 loss: 0.008026167750358582\n",
            "training 45500 loss: 0.004982311278581619\n",
            "training 45600 loss: 0.007870449684560299\n",
            "training 45700 loss: 0.004622356966137886\n",
            "training 45800 loss: 7.177420775406063e-05\n",
            "training 45900 loss: 0.010681896470487118\n",
            "training 46000 loss: 5.779004641226493e-05\n",
            "training 46100 loss: 0.005001278594136238\n",
            "training 46200 loss: 0.004045999143272638\n",
            "training 46300 loss: 0.0027066089678555727\n",
            "training 46400 loss: 0.000493681407533586\n",
            "training 46500 loss: 0.002149693202227354\n",
            "training 46600 loss: 0.00047742726746946573\n",
            "training 46700 loss: 0.0025328611955046654\n",
            "training 46800 loss: 0.0013620706740766764\n",
            "training 46900 loss: 0.010196343064308167\n",
            "training 47000 loss: 5.281340236251708e-06\n",
            "training 47100 loss: 0.0009263959946110845\n",
            "training 47200 loss: 0.00023488698934670538\n",
            "training 47300 loss: 0.00044014540617354214\n",
            "training 47400 loss: 0.009803969413042068\n",
            "training 47500 loss: 0.013963929377496243\n",
            "[3] average loss per epoch: 0.006\n",
            "Saved model checkpoint to /content/drive/My Drive/Task4/test5/model_epoch3.pt\n",
            "training 100 loss: 3.608925180742517e-05\n",
            "training 200 loss: 0.00392806576564908\n",
            "training 300 loss: 0.07608812302350998\n",
            "training 400 loss: 0.0006968010566197336\n",
            "training 500 loss: 0.0023621181026101112\n",
            "training 600 loss: 1.3184995850679115e-07\n",
            "training 700 loss: 0.0012138662859797478\n",
            "training 800 loss: 0.002831458579748869\n",
            "training 900 loss: 0.006793192587792873\n",
            "training 1000 loss: 0.0015082721365615726\n",
            "training 1100 loss: 0.012938516214489937\n",
            "training 1200 loss: 0.0002852576435543597\n",
            "training 1300 loss: 0.0037358624394983053\n",
            "training 1400 loss: 0.015020141378045082\n",
            "training 1500 loss: 1.8643055454958812e-07\n",
            "training 1600 loss: 7.162077235989273e-05\n",
            "training 1700 loss: 0.0038721414748579264\n",
            "training 1800 loss: 0.055422570556402206\n",
            "training 1900 loss: 0.025569824501872063\n",
            "training 2000 loss: 0.0003607393300626427\n",
            "training 2100 loss: 0.007437428925186396\n",
            "training 2200 loss: 0.000702476769220084\n",
            "training 2300 loss: 0.004221722483634949\n",
            "training 2400 loss: 0.024249756708741188\n",
            "training 2500 loss: 0.0015957654686644673\n",
            "training 2600 loss: 0.0010461872443556786\n",
            "training 2700 loss: 0.001224038191139698\n",
            "training 2800 loss: 0.004643583670258522\n",
            "training 2900 loss: 0.0012026781914755702\n",
            "training 3000 loss: 0.011862652376294136\n",
            "training 3100 loss: 0.00036380699020810425\n",
            "training 3200 loss: 7.200641448434908e-06\n",
            "training 3300 loss: 0.002931201132014394\n",
            "training 3400 loss: 0.0009496159618720412\n",
            "training 3500 loss: 0.025831107050180435\n",
            "training 3600 loss: 0.00877266377210617\n",
            "training 3700 loss: 0.002545956987887621\n",
            "training 3800 loss: 0.0014479246456176043\n",
            "training 3900 loss: 0.004822626709938049\n",
            "training 4000 loss: 0.028198445215821266\n",
            "training 4100 loss: 0.022614113986492157\n",
            "training 4200 loss: 0.0019123981473967433\n",
            "training 4300 loss: 0.0028545951936393976\n",
            "training 4400 loss: 0.011463875882327557\n",
            "training 4500 loss: 0.005482724402099848\n",
            "training 4600 loss: 0.0008586142212152481\n",
            "training 4700 loss: 0.005076624918729067\n",
            "training 4800 loss: 0.00065493059810251\n",
            "training 4900 loss: 0.016651999205350876\n",
            "training 5000 loss: 0.008112478069961071\n",
            "5000 Validation Loss: 0.006\n",
            "training 5100 loss: 0.033430419862270355\n",
            "training 5200 loss: 7.88060569902882e-05\n",
            "training 5300 loss: 0.0016921334899961948\n",
            "training 5400 loss: 0.0003583162324503064\n",
            "training 5500 loss: 0.0027686315588653088\n",
            "training 5600 loss: 0.0058645401149988174\n",
            "training 5700 loss: 0.0015959368320181966\n",
            "training 5800 loss: 3.743064735317603e-05\n",
            "training 5900 loss: 0.0016733944648876786\n",
            "training 6000 loss: 3.1456420401809737e-06\n",
            "training 6100 loss: 0.0032129157334566116\n",
            "training 6200 loss: 0.001814915449358523\n",
            "training 6300 loss: 0.0009250902803614736\n",
            "training 6400 loss: 0.0006824384909123182\n",
            "training 6500 loss: 2.4439400476694573e-06\n",
            "training 6600 loss: 5.41552908543963e-05\n",
            "training 6700 loss: 0.000276166305411607\n",
            "training 6800 loss: 0.0019217932131141424\n",
            "training 6900 loss: 0.049099188297986984\n",
            "training 7000 loss: 0.010053840465843678\n",
            "training 7100 loss: 0.0010058812331408262\n",
            "training 7200 loss: 0.012251880019903183\n",
            "training 7300 loss: 0.005853262264281511\n",
            "training 7400 loss: 0.0009946461068466306\n",
            "training 7500 loss: 0.0011693921405822039\n",
            "training 7600 loss: 0.002222303533926606\n",
            "training 7700 loss: 0.011039077304303646\n",
            "training 7800 loss: 0.002968494314700365\n",
            "training 7900 loss: 0.001840233220718801\n",
            "training 8000 loss: 0.02725914493203163\n",
            "training 8100 loss: 0.008792906999588013\n",
            "training 8200 loss: 0.017510518431663513\n",
            "training 8300 loss: 0.0006572878919541836\n",
            "training 8400 loss: 0.009665182791650295\n",
            "training 8500 loss: 2.598337687231833e-07\n",
            "training 8600 loss: 0.0031553353182971478\n",
            "training 8700 loss: 0.00031555446912534535\n",
            "training 8800 loss: 0.008064104244112968\n",
            "training 8900 loss: 0.004027518443763256\n",
            "training 9000 loss: 0.00251401006244123\n",
            "training 9100 loss: 0.005498128943145275\n",
            "training 9200 loss: 0.018967485055327415\n",
            "training 9300 loss: 0.00013376507558859885\n",
            "training 9400 loss: 0.0027476961258798838\n",
            "training 9500 loss: 0.05220958590507507\n",
            "training 9600 loss: 0.0008022156544029713\n",
            "training 9700 loss: 0.0023824863601475954\n",
            "training 9800 loss: 0.00010636358638294041\n",
            "training 9900 loss: 0.0019254322396591306\n",
            "training 10000 loss: 0.011911469511687756\n",
            "10000 Validation Loss: 0.005\n",
            "training 10100 loss: 3.633432697824901e-06\n",
            "training 10200 loss: 0.00019507812976371497\n",
            "training 10300 loss: 0.002807758981361985\n",
            "training 10400 loss: 0.000738179893232882\n",
            "training 10500 loss: 0.00558552285656333\n",
            "training 10600 loss: 0.002108768094331026\n",
            "training 10700 loss: 0.0013649407774209976\n",
            "training 10800 loss: 0.00039041569107212126\n",
            "training 10900 loss: 0.001016419380903244\n",
            "training 11000 loss: 0.0032624860759824514\n",
            "training 11100 loss: 0.0004454432928469032\n",
            "training 11200 loss: 0.0018346940632909536\n",
            "training 11300 loss: 0.0036603540647774935\n",
            "training 11400 loss: 0.0028627279680222273\n",
            "training 11500 loss: 0.0003060387389268726\n",
            "training 11600 loss: 0.013812989927828312\n",
            "training 11700 loss: 0.002314549870789051\n",
            "training 11800 loss: 0.006065279711037874\n",
            "training 11900 loss: 6.273437611525878e-05\n",
            "training 12000 loss: 0.0004721905570477247\n",
            "training 12100 loss: 0.001406582654453814\n",
            "training 12200 loss: 0.0031166430562734604\n",
            "training 12300 loss: 0.005651718005537987\n",
            "training 12400 loss: 0.021432602778077126\n",
            "training 12500 loss: 0.001162651227787137\n",
            "training 12600 loss: 0.004839794710278511\n",
            "training 12700 loss: 0.0010031156707555056\n",
            "training 12800 loss: 0.0020227329805493355\n",
            "training 12900 loss: 0.007218725513666868\n",
            "training 13000 loss: 0.0016891140257939696\n",
            "training 13100 loss: 0.00396193889901042\n",
            "training 13200 loss: 0.006671437527984381\n",
            "training 13300 loss: 0.03035454824566841\n",
            "training 13400 loss: 0.0001476526667829603\n",
            "training 13500 loss: 0.0007380633032880723\n",
            "training 13600 loss: 0.013535421341657639\n",
            "training 13700 loss: 0.00040700097451917827\n",
            "training 13800 loss: 0.0013935223687440157\n",
            "training 13900 loss: 6.307853618636727e-05\n",
            "training 14000 loss: 0.001139806816354394\n",
            "training 14100 loss: 0.002954066963866353\n",
            "training 14200 loss: 0.010273769497871399\n",
            "training 14300 loss: 0.0009118531015701592\n",
            "training 14400 loss: 2.8332806323305704e-08\n",
            "training 14500 loss: 0.0016278306720778346\n",
            "training 14600 loss: 3.4109121770597994e-05\n",
            "training 14700 loss: 0.0001388476084684953\n",
            "training 14800 loss: 0.0075747487135231495\n",
            "training 14900 loss: 0.00029390736017376184\n",
            "training 15000 loss: 0.003119491972029209\n",
            "15000 Validation Loss: 0.005\n",
            "training 15100 loss: 0.04058464244008064\n",
            "training 15200 loss: 0.0007305946201086044\n",
            "training 15300 loss: 0.005747402552515268\n",
            "training 15400 loss: 0.0005647566867992282\n",
            "training 15500 loss: 0.0017857003258541226\n",
            "training 15600 loss: 0.0011357535840943456\n",
            "training 15700 loss: 0.07546332478523254\n",
            "training 15800 loss: 0.02358885109424591\n",
            "training 15900 loss: 0.01416274905204773\n",
            "training 16000 loss: 0.0029217598494142294\n",
            "training 16100 loss: 0.0026790685951709747\n",
            "training 16200 loss: 0.01993422769010067\n",
            "training 16300 loss: 0.0010347898351028562\n",
            "training 16400 loss: 0.0010626307921484113\n",
            "training 16500 loss: 0.009783348068594933\n",
            "training 16600 loss: 8.873934712028131e-05\n",
            "training 16700 loss: 0.0011177309788763523\n",
            "training 16800 loss: 0.0004435231676325202\n",
            "training 16900 loss: 0.021145958453416824\n",
            "training 17000 loss: 0.02259110100567341\n",
            "training 17100 loss: 0.005092571955174208\n",
            "training 17200 loss: 0.01340398658066988\n",
            "training 17300 loss: 0.0002964141604024917\n",
            "training 17400 loss: 0.003694214392453432\n",
            "training 17500 loss: 4.883948713541031e-05\n",
            "training 17600 loss: 6.633478915318847e-05\n",
            "training 17700 loss: 0.017233047634363174\n",
            "training 17800 loss: 0.0035756013821810484\n",
            "training 17900 loss: 0.006785059813410044\n",
            "training 18000 loss: 0.00134792341850698\n",
            "training 18100 loss: 7.009951514191926e-05\n",
            "training 18200 loss: 0.0003686250129248947\n",
            "training 18300 loss: 0.004028668627142906\n",
            "training 18400 loss: 0.007004323415458202\n",
            "training 18500 loss: 0.011592946015298367\n",
            "training 18600 loss: 0.003599020419642329\n",
            "training 18700 loss: 0.00019081265782006085\n",
            "training 18800 loss: 0.0008195414557121694\n",
            "training 18900 loss: 0.00033866360899992287\n",
            "training 19000 loss: 2.2940181224839762e-05\n",
            "training 19100 loss: 5.997068001306616e-05\n",
            "training 19200 loss: 0.0007715990650467575\n",
            "training 19300 loss: 0.00013627501903101802\n",
            "training 19400 loss: 0.006747797131538391\n",
            "training 19500 loss: 0.015540595166385174\n",
            "training 19600 loss: 0.007797983009368181\n",
            "training 19700 loss: 0.001278243726119399\n",
            "training 19800 loss: 0.002303734654560685\n",
            "training 19900 loss: 0.009226065129041672\n",
            "training 20000 loss: 0.0004912264994345605\n",
            "20000 Validation Loss: 0.005\n",
            "training 20100 loss: 0.02572813257575035\n",
            "training 20200 loss: 0.003912958316504955\n",
            "training 20300 loss: 0.001005095080472529\n",
            "training 20400 loss: 0.0006377530517056584\n",
            "training 20500 loss: 0.002631792798638344\n",
            "training 20600 loss: 0.00048693452845327556\n",
            "training 20700 loss: 0.016400184482336044\n",
            "training 20800 loss: 0.00011523585999384522\n",
            "training 20900 loss: 0.0001043519951053895\n",
            "training 21000 loss: 0.004759454634040594\n",
            "training 21100 loss: 0.0018770097522065043\n",
            "training 21200 loss: 0.022295288741588593\n",
            "training 21300 loss: 0.0022535002790391445\n",
            "training 21400 loss: 0.0022664666175842285\n",
            "training 21500 loss: 0.007522382773458958\n",
            "training 21600 loss: 1.58830721375125e-06\n",
            "training 21700 loss: 0.0019105218816548586\n",
            "training 21800 loss: 0.00027478919946588576\n",
            "training 21900 loss: 8.255346619989723e-05\n",
            "training 22000 loss: 0.0009055141708813608\n",
            "training 22100 loss: 1.1811620424850844e-05\n",
            "training 22200 loss: 0.00035668438067659736\n",
            "training 22300 loss: 0.0032153755892068148\n",
            "training 22400 loss: 0.010415772907435894\n",
            "training 22500 loss: 0.0015765847638249397\n",
            "training 22600 loss: 0.0014094812795519829\n",
            "training 22700 loss: 0.0019373141694813967\n",
            "training 22800 loss: 2.918770860560471e-07\n",
            "training 22900 loss: 0.00041203800356015563\n",
            "training 23000 loss: 0.00034856307320296764\n",
            "training 23100 loss: 0.008595014922320843\n",
            "training 23200 loss: 0.0018323459662497044\n",
            "training 23300 loss: 0.00038699369179084897\n",
            "training 23400 loss: 0.012792160734534264\n",
            "training 23500 loss: 0.0007611574837937951\n",
            "training 23600 loss: 0.0326729379594326\n",
            "training 23700 loss: 0.008150273002684116\n",
            "training 23800 loss: 0.0014410741860046983\n",
            "training 23900 loss: 0.009660636074841022\n",
            "training 24000 loss: 1.9265831951997825e-07\n",
            "training 24100 loss: 0.0009181996574625373\n",
            "training 24200 loss: 0.003177337348461151\n",
            "training 24300 loss: 0.02121618762612343\n",
            "training 24400 loss: 0.0023279665037989616\n",
            "training 24500 loss: 0.0021703920792788267\n",
            "training 24600 loss: 0.009051434695720673\n",
            "training 24700 loss: 0.03317641094326973\n",
            "training 24800 loss: 2.2298401745501906e-05\n",
            "training 24900 loss: 0.0002181115560233593\n",
            "training 25000 loss: 0.001308200298808515\n",
            "25000 Validation Loss: 0.005\n",
            "training 25100 loss: 0.0094317477196455\n",
            "training 25200 loss: 0.003084144787862897\n",
            "training 25300 loss: 0.0032307791989296675\n",
            "training 25400 loss: 5.010936092730844e-06\n",
            "training 25500 loss: 0.0024583751801401377\n",
            "training 25600 loss: 0.018475685268640518\n",
            "training 25700 loss: 9.993306593969464e-05\n",
            "training 25800 loss: 0.005021735094487667\n",
            "training 25900 loss: 0.010338733904063702\n",
            "training 26000 loss: 0.0026248502545058727\n",
            "training 26100 loss: 0.010122663341462612\n",
            "training 26200 loss: 0.0036373112816363573\n",
            "training 26300 loss: 0.007646381389349699\n",
            "training 26400 loss: 2.946141830761917e-05\n",
            "training 26500 loss: 7.280396675923839e-05\n",
            "training 26600 loss: 4.104038453078829e-06\n",
            "training 26700 loss: 0.00048737655743025243\n",
            "training 26800 loss: 0.024055007845163345\n",
            "training 26900 loss: 0.004429444205015898\n",
            "training 27000 loss: 0.00470013078302145\n",
            "training 27100 loss: 0.0016508052358403802\n",
            "training 27200 loss: 0.0003258267242927104\n",
            "training 27300 loss: 0.0016959802014753222\n",
            "training 27400 loss: 0.017113881185650826\n",
            "training 27500 loss: 0.0007515058387070894\n",
            "training 27600 loss: 1.9975488612544723e-05\n",
            "training 27700 loss: 0.0017575817182660103\n",
            "training 27800 loss: 0.004900624975562096\n",
            "training 27900 loss: 0.001886711223050952\n",
            "training 28000 loss: 7.11011525709182e-05\n",
            "training 28100 loss: 0.003981833346188068\n",
            "training 28200 loss: 0.0009849399793893099\n",
            "training 28300 loss: 0.0010701674036681652\n",
            "training 28400 loss: 7.769250078126788e-05\n",
            "training 28500 loss: 0.0033853393979370594\n",
            "training 28600 loss: 0.0015458742855116725\n",
            "training 28700 loss: 0.016969280317425728\n",
            "training 28800 loss: 0.008594263345003128\n",
            "training 28900 loss: 0.000124043581308797\n",
            "training 29000 loss: 0.0029582930728793144\n",
            "training 29100 loss: 0.00041663870797492564\n",
            "training 29200 loss: 0.0055679320357739925\n",
            "training 29300 loss: 9.878756827674806e-05\n",
            "training 29400 loss: 0.019382407888770103\n",
            "training 29500 loss: 0.002629518276080489\n",
            "training 29600 loss: 0.10241597890853882\n",
            "training 29700 loss: 0.016775229945778847\n",
            "training 29800 loss: 0.005498023238033056\n",
            "training 29900 loss: 0.000774940534029156\n",
            "training 30000 loss: 0.002587848110124469\n",
            "30000 Validation Loss: 0.005\n",
            "training 30100 loss: 0.004934363998472691\n",
            "training 30200 loss: 0.0039162402972579\n",
            "training 30300 loss: 0.009549086913466454\n",
            "training 30400 loss: 0.00041360751492902637\n",
            "training 30500 loss: 1.7539852706249803e-05\n",
            "training 30600 loss: 0.001997634069994092\n",
            "training 30700 loss: 0.005666712298989296\n",
            "training 30800 loss: 0.009233119897544384\n",
            "training 30900 loss: 0.010772769339382648\n",
            "training 31000 loss: 0.002709115156903863\n",
            "training 31100 loss: 0.0025071052368730307\n",
            "training 31200 loss: 2.9918701329734176e-05\n",
            "training 31300 loss: 0.017612317577004433\n",
            "training 31400 loss: 0.014841093681752682\n",
            "training 31500 loss: 0.0013972807209938765\n",
            "training 31600 loss: 0.0005763400113210082\n",
            "training 31700 loss: 0.00626154663041234\n",
            "training 31800 loss: 0.01811360940337181\n",
            "training 31900 loss: 0.0009089180384762585\n",
            "training 32000 loss: 0.0014862234238535166\n",
            "training 32100 loss: 0.0016735895769670606\n",
            "training 32200 loss: 0.00023458014766220003\n",
            "training 32300 loss: 9.324870916316286e-06\n",
            "training 32400 loss: 0.01048167236149311\n",
            "training 32500 loss: 0.0006339054671116173\n",
            "training 32600 loss: 0.0053736069239676\n",
            "training 32700 loss: 0.0006240276270546019\n",
            "training 32800 loss: 0.003081259084865451\n",
            "training 32900 loss: 0.010799759067595005\n",
            "training 33000 loss: 0.0016157521167770028\n",
            "training 33100 loss: 0.013863585889339447\n",
            "training 33200 loss: 0.00042201890028081834\n",
            "training 33300 loss: 0.0011987622128799558\n",
            "training 33400 loss: 0.0024605507496744394\n",
            "training 33500 loss: 0.0011569351190701127\n",
            "training 33600 loss: 0.0007410328253172338\n",
            "training 33700 loss: 0.005746245849877596\n",
            "training 33800 loss: 0.056530870497226715\n",
            "training 33900 loss: 0.014695297926664352\n",
            "training 34000 loss: 0.011589250527322292\n",
            "training 34100 loss: 0.026908446103334427\n",
            "training 34200 loss: 0.011368602514266968\n",
            "training 34300 loss: 0.00013696613314095885\n",
            "training 34400 loss: 0.02175675332546234\n",
            "training 34500 loss: 0.004444880876690149\n",
            "training 34600 loss: 0.015457544475793839\n",
            "training 34700 loss: 0.005991234909743071\n",
            "training 34800 loss: 0.0003643073723651469\n",
            "training 34900 loss: 0.000669791828840971\n",
            "training 35000 loss: 0.0001937749475473538\n",
            "35000 Validation Loss: 0.005\n",
            "training 35100 loss: 0.000461578369140625\n",
            "training 35200 loss: 0.0024595102295279503\n",
            "training 35300 loss: 5.713938662665896e-06\n",
            "training 35400 loss: 1.5060137229738757e-05\n",
            "training 35500 loss: 0.00019235708168707788\n",
            "training 35600 loss: 0.006517634727060795\n",
            "training 35700 loss: 9.267194400308654e-05\n",
            "training 35800 loss: 0.0007400466129183769\n",
            "training 35900 loss: 0.0016840617172420025\n",
            "training 36000 loss: 0.0008175087277777493\n",
            "training 36100 loss: 0.00462102796882391\n",
            "training 36200 loss: 0.004563542548567057\n",
            "training 36300 loss: 0.0004365414206404239\n",
            "training 36400 loss: 0.00014738045865669847\n",
            "training 36500 loss: 0.003476726822555065\n",
            "training 36600 loss: 0.022505607455968857\n",
            "training 36700 loss: 0.009705634787678719\n",
            "training 36800 loss: 0.01630932278931141\n",
            "training 36900 loss: 0.007225533481687307\n",
            "training 37000 loss: 0.006143366452306509\n",
            "training 37100 loss: 0.013099124655127525\n",
            "training 37200 loss: 0.002589279320091009\n",
            "training 37300 loss: 0.0137709341943264\n",
            "training 37400 loss: 0.0018246793188154697\n",
            "training 37500 loss: 0.0022160811349749565\n",
            "training 37600 loss: 0.004188111983239651\n",
            "training 37700 loss: 0.00038741593016311526\n",
            "training 37800 loss: 0.002931794850155711\n",
            "training 37900 loss: 0.0023224942851811647\n",
            "training 38000 loss: 0.004515864886343479\n",
            "training 38100 loss: 0.0039128391072154045\n",
            "training 38200 loss: 0.003137067658826709\n",
            "training 38300 loss: 0.0029478503856807947\n",
            "training 38400 loss: 0.0269729383289814\n",
            "training 38500 loss: 0.008269086480140686\n",
            "training 38600 loss: 0.0005579104763455689\n",
            "training 38700 loss: 0.00019095100287813693\n",
            "training 38800 loss: 7.018828910076991e-06\n",
            "training 38900 loss: 0.000438386487076059\n",
            "training 39000 loss: 0.0005661627510562539\n",
            "training 39100 loss: 1.0535415640333667e-05\n",
            "training 39200 loss: 0.014298471622169018\n",
            "training 39300 loss: 1.8417267710901797e-11\n",
            "training 39400 loss: 0.0065350462682545185\n",
            "training 39500 loss: 0.016121091321110725\n",
            "training 39600 loss: 0.0016188202425837517\n",
            "training 39700 loss: 5.3811949328519404e-05\n",
            "training 39800 loss: 0.0005797563935630023\n",
            "training 39900 loss: 0.025299174711108208\n",
            "training 40000 loss: 0.0001366872456856072\n",
            "40000 Validation Loss: 0.005\n",
            "training 40100 loss: 4.395953510538675e-06\n",
            "training 40200 loss: 0.0009534108685329556\n",
            "training 40300 loss: 0.0002951758215203881\n",
            "training 40400 loss: 0.002556940307840705\n",
            "training 40500 loss: 0.01137872226536274\n",
            "training 40600 loss: 0.043449051678180695\n",
            "training 40700 loss: 0.002178018447011709\n",
            "training 40800 loss: 0.002880077576264739\n",
            "training 40900 loss: 0.003984571900218725\n",
            "training 41000 loss: 0.0307993832975626\n",
            "training 41100 loss: 0.0002932537463493645\n",
            "training 41200 loss: 0.0006089562084525824\n",
            "training 41300 loss: 0.0036723939701914787\n",
            "training 41400 loss: 0.0006619906635023654\n",
            "training 41500 loss: 0.020310305058956146\n",
            "training 41600 loss: 0.004042481537908316\n",
            "training 41700 loss: 0.0007028686231933534\n",
            "training 41800 loss: 0.0009673901367932558\n",
            "training 41900 loss: 3.504561027511954e-05\n",
            "training 42000 loss: 0.011307318694889545\n",
            "training 42100 loss: 0.0009288067813031375\n",
            "training 42200 loss: 0.004525290336459875\n",
            "training 42300 loss: 0.0009758027154020965\n",
            "training 42400 loss: 0.006434212904423475\n",
            "training 42500 loss: 0.006292033474892378\n",
            "training 42600 loss: 0.00023014632461126894\n",
            "training 42700 loss: 0.002094471827149391\n",
            "training 42800 loss: 0.018929677084088326\n",
            "training 42900 loss: 0.008456413634121418\n",
            "training 43000 loss: 0.0009691558079794049\n",
            "training 43100 loss: 0.008095736615359783\n",
            "training 43200 loss: 0.0011844583787024021\n",
            "training 43300 loss: 0.013744871132075787\n",
            "training 43400 loss: 0.007488013245165348\n",
            "training 43500 loss: 0.01559896394610405\n",
            "training 43600 loss: 0.0007442815112881362\n",
            "training 43700 loss: 0.007781233172863722\n",
            "training 43800 loss: 0.03679478541016579\n",
            "training 43900 loss: 0.009689622558653355\n",
            "training 44000 loss: 0.0006671658484265208\n",
            "training 44100 loss: 0.01353958249092102\n",
            "training 44200 loss: 0.009903511963784695\n",
            "training 44300 loss: 0.001380771049298346\n",
            "training 44400 loss: 0.004328629467636347\n",
            "training 44500 loss: 0.0026961008552461863\n",
            "training 44600 loss: 0.003637972753494978\n",
            "training 44700 loss: 0.00020407054398674518\n",
            "training 44800 loss: 0.0038205075543373823\n",
            "training 44900 loss: 0.003587301354855299\n",
            "training 45000 loss: 0.0077804760076105595\n",
            "45000 Validation Loss: 0.005\n",
            "training 45100 loss: 0.008636043407022953\n",
            "training 45200 loss: 0.022831546142697334\n",
            "training 45300 loss: 0.010278071276843548\n",
            "training 45400 loss: 0.0077113984152674675\n",
            "training 45500 loss: 0.0038624447770416737\n",
            "training 45600 loss: 0.006914264522492886\n",
            "training 45700 loss: 0.002597559941932559\n",
            "training 45800 loss: 0.0002103647420881316\n",
            "training 45900 loss: 0.009890798479318619\n",
            "training 46000 loss: 7.31379768694751e-05\n",
            "training 46100 loss: 0.005407005548477173\n",
            "training 46200 loss: 0.0023309814278036356\n",
            "training 46300 loss: 0.00145919609349221\n",
            "training 46400 loss: 1.8571065083961003e-05\n",
            "training 46500 loss: 0.001313120243139565\n",
            "training 46600 loss: 0.0001846518716774881\n",
            "training 46700 loss: 0.0017354820156469941\n",
            "training 46800 loss: 0.0006441512377932668\n",
            "training 46900 loss: 0.008101572282612324\n",
            "training 47000 loss: 1.5865048226260114e-06\n",
            "training 47100 loss: 0.0010244314325973392\n",
            "training 47200 loss: 0.0012202708749100566\n",
            "training 47300 loss: 0.00040984374936670065\n",
            "training 47400 loss: 0.010867257602512836\n",
            "training 47500 loss: 0.014070685021579266\n",
            "[4] average loss per epoch: 0.006\n",
            "Saved model checkpoint to /content/drive/My Drive/Task4/test5/model_epoch4.pt\n",
            "training 100 loss: 4.715147952083498e-05\n",
            "training 200 loss: 0.0035025570541620255\n",
            "training 300 loss: 0.06684621423482895\n",
            "training 400 loss: 0.0005507590249180794\n",
            "training 500 loss: 0.0009448170894756913\n",
            "training 600 loss: 2.31087688007392e-06\n",
            "training 700 loss: 0.0014318391913548112\n",
            "training 800 loss: 0.003214348340407014\n",
            "training 900 loss: 0.005601499695330858\n",
            "training 1000 loss: 0.0011684303171932697\n",
            "training 1100 loss: 0.015550225973129272\n",
            "training 1200 loss: 0.00026981631526723504\n",
            "training 1300 loss: 0.003178815823048353\n",
            "training 1400 loss: 0.019578872248530388\n",
            "training 1500 loss: 1.6483581930515356e-06\n",
            "training 1600 loss: 0.00029436530894599855\n",
            "training 1700 loss: 0.004188142716884613\n",
            "training 1800 loss: 0.05241931602358818\n",
            "training 1900 loss: 0.035541120916604996\n",
            "training 2000 loss: 0.0007654785877093673\n",
            "training 2100 loss: 0.004517819732427597\n",
            "training 2200 loss: 0.0014953737845644355\n",
            "training 2300 loss: 0.0048795840702950954\n",
            "training 2400 loss: 0.014099999330937862\n",
            "training 2500 loss: 0.0008101766579784453\n",
            "training 2600 loss: 0.000652236514724791\n",
            "training 2700 loss: 0.0013316565891727805\n",
            "training 2800 loss: 0.004970101173967123\n",
            "training 2900 loss: 0.0007912194705568254\n",
            "training 3000 loss: 0.011405493132770061\n",
            "training 3100 loss: 0.00014174498210195452\n",
            "training 3200 loss: 1.3222534107626416e-06\n",
            "training 3300 loss: 0.0017748158425092697\n",
            "training 3400 loss: 0.0016986714908853173\n",
            "training 3500 loss: 0.01982242427766323\n",
            "training 3600 loss: 0.006568973418325186\n",
            "training 3700 loss: 0.002865892369300127\n",
            "training 3800 loss: 0.001568000647239387\n",
            "training 3900 loss: 0.0029981592670083046\n",
            "training 4000 loss: 0.029108650982379913\n",
            "training 4100 loss: 0.021591361612081528\n",
            "training 4200 loss: 0.0027631146367639303\n",
            "training 4300 loss: 0.0021245195530354977\n",
            "training 4400 loss: 0.008372388780117035\n",
            "training 4500 loss: 0.005922634620219469\n",
            "training 4600 loss: 0.0001703419111436233\n",
            "training 4700 loss: 0.0026764036156237125\n",
            "training 4800 loss: 0.0010151427704840899\n",
            "training 4900 loss: 0.015500140376389027\n",
            "training 5000 loss: 0.008014296181499958\n",
            "5000 Validation Loss: 0.005\n",
            "training 5100 loss: 0.031603239476680756\n",
            "training 5200 loss: 3.069306694669649e-05\n",
            "training 5300 loss: 0.0015084943734109402\n",
            "training 5400 loss: 6.0040860262233764e-05\n",
            "training 5500 loss: 0.0021747706923633814\n",
            "training 5600 loss: 0.004332677461206913\n",
            "training 5700 loss: 0.0007374027627520263\n",
            "training 5800 loss: 0.0003087641962338239\n",
            "training 5900 loss: 0.0020896736532449722\n",
            "training 6000 loss: 0.00011650880333036184\n",
            "training 6100 loss: 0.0025758787523955107\n",
            "training 6200 loss: 0.0017356210155412555\n",
            "training 6300 loss: 0.0009839824633672833\n",
            "training 6400 loss: 0.0005976540851406753\n",
            "training 6500 loss: 2.6485779471840942e-06\n",
            "training 6600 loss: 4.093392635695636e-05\n",
            "training 6700 loss: 0.00031626640702597797\n",
            "training 6800 loss: 0.0015784786082804203\n",
            "training 6900 loss: 0.04320785775780678\n",
            "training 7000 loss: 0.011782807298004627\n",
            "training 7100 loss: 0.00036001516855321825\n",
            "training 7200 loss: 0.011635702103376389\n",
            "training 7300 loss: 0.00441840710118413\n",
            "training 7400 loss: 0.0022606588900089264\n",
            "training 7500 loss: 0.0013922766083851457\n",
            "training 7600 loss: 0.0004950066795572639\n",
            "training 7700 loss: 0.011654483154416084\n",
            "training 7800 loss: 0.002948678797110915\n",
            "training 7900 loss: 0.0022546774707734585\n",
            "training 8000 loss: 0.031852781772613525\n",
            "training 8100 loss: 0.007758452091366053\n",
            "training 8200 loss: 0.016632990911602974\n",
            "training 8300 loss: 0.001117396168410778\n",
            "training 8400 loss: 0.010454985313117504\n",
            "training 8500 loss: 5.759084729106689e-07\n",
            "training 8600 loss: 0.0021091841626912355\n",
            "training 8700 loss: 0.00023765754303894937\n",
            "training 8800 loss: 0.0036801134701818228\n",
            "training 8900 loss: 0.005289592780172825\n",
            "training 9000 loss: 0.003995445091277361\n",
            "training 9100 loss: 0.007066396530717611\n",
            "training 9200 loss: 0.0152510404586792\n",
            "training 9300 loss: 0.00036442570853978395\n",
            "training 9400 loss: 0.001991522265598178\n",
            "training 9500 loss: 0.04711228609085083\n",
            "training 9600 loss: 0.001872859662398696\n",
            "training 9700 loss: 0.002502022311091423\n",
            "training 9800 loss: 0.000322512787533924\n",
            "training 9900 loss: 0.0021319326478987932\n",
            "training 10000 loss: 0.008244691416621208\n",
            "10000 Validation Loss: 0.005\n",
            "training 10100 loss: 9.653513188823126e-07\n",
            "training 10200 loss: 0.0002872261102311313\n",
            "training 10300 loss: 0.0017496743239462376\n",
            "training 10400 loss: 0.0013890402624383569\n",
            "training 10500 loss: 0.004571760538965464\n",
            "training 10600 loss: 0.0009147351374849677\n",
            "training 10700 loss: 0.0016730043571442366\n",
            "training 10800 loss: 0.0013222421985119581\n",
            "training 10900 loss: 0.0016122846864163876\n",
            "training 11000 loss: 0.0035473438911139965\n",
            "training 11100 loss: 0.0003858407726511359\n",
            "training 11200 loss: 0.0014776510652154684\n",
            "training 11300 loss: 0.003164743771776557\n",
            "training 11400 loss: 0.004130880814045668\n",
            "training 11500 loss: 0.0002532011130824685\n",
            "training 11600 loss: 0.013981739990413189\n",
            "training 11700 loss: 0.0018242516089230776\n",
            "training 11800 loss: 0.004833261948078871\n",
            "training 11900 loss: 0.00024480416323058307\n",
            "training 12000 loss: 0.000539481348823756\n",
            "training 12100 loss: 0.001246427884325385\n",
            "training 12200 loss: 0.0024762102402746677\n",
            "training 12300 loss: 0.0048559303395450115\n",
            "training 12400 loss: 0.014805447310209274\n",
            "training 12500 loss: 0.002624385990202427\n",
            "training 12600 loss: 0.004199815448373556\n",
            "training 12700 loss: 0.0003137190651614219\n",
            "training 12800 loss: 0.00183534761890769\n",
            "training 12900 loss: 0.012237158603966236\n",
            "training 13000 loss: 0.0019878637976944447\n",
            "training 13100 loss: 0.003789740614593029\n",
            "training 13200 loss: 0.00586665840819478\n",
            "training 13300 loss: 0.029467055574059486\n",
            "training 13400 loss: 5.2363204304128885e-05\n",
            "training 13500 loss: 0.00031388801289722323\n",
            "training 13600 loss: 0.013843324966728687\n",
            "training 13700 loss: 0.00024409592151641846\n",
            "training 13800 loss: 0.0010277153924107552\n",
            "training 13900 loss: 0.000289266201434657\n",
            "training 14000 loss: 0.0018210350535809994\n",
            "training 14100 loss: 0.0031288471072912216\n",
            "training 14200 loss: 0.008466282859444618\n",
            "training 14300 loss: 0.0011525762965902686\n",
            "training 14400 loss: 7.573794573545456e-05\n",
            "training 14500 loss: 0.0017180846771225333\n",
            "training 14600 loss: 3.809580812230706e-05\n",
            "training 14700 loss: 0.0003253878967370838\n",
            "training 14800 loss: 0.008104362525045872\n",
            "training 14900 loss: 0.001010302104987204\n",
            "training 15000 loss: 0.0017053009942173958\n",
            "15000 Validation Loss: 0.005\n",
            "training 15100 loss: 0.040284719318151474\n",
            "training 15200 loss: 0.0009885798208415508\n",
            "training 15300 loss: 0.006380471400916576\n",
            "training 15400 loss: 0.0005925939185544848\n",
            "training 15500 loss: 0.0017865063855424523\n",
            "training 15600 loss: 0.0009833693038672209\n",
            "training 15700 loss: 0.06585244089365005\n",
            "training 15800 loss: 0.025289390236139297\n",
            "training 15900 loss: 0.014061015099287033\n",
            "training 16000 loss: 0.002877314342185855\n",
            "training 16100 loss: 0.0021955447737127542\n",
            "training 16200 loss: 0.024854423478245735\n",
            "training 16300 loss: 0.00047884529340080917\n",
            "training 16400 loss: 0.0011248203227296472\n",
            "training 16500 loss: 0.005971873644739389\n",
            "training 16600 loss: 2.334159580641426e-05\n",
            "training 16700 loss: 0.0002559094864409417\n",
            "training 16800 loss: 7.864528015488759e-05\n",
            "training 16900 loss: 0.019403457641601562\n",
            "training 17000 loss: 0.022078730165958405\n",
            "training 17100 loss: 0.005760496016591787\n",
            "training 17200 loss: 0.015505245886743069\n",
            "training 17300 loss: 0.0008813136955723166\n",
            "training 17400 loss: 0.004536010324954987\n",
            "training 17500 loss: 0.00016328449419233948\n",
            "training 17600 loss: 1.1205065675312653e-07\n",
            "training 17700 loss: 0.014862303622066975\n",
            "training 17800 loss: 0.002194718224927783\n",
            "training 17900 loss: 0.004882282577455044\n",
            "training 18000 loss: 0.0020246636122465134\n",
            "training 18100 loss: 4.85420514451107e-06\n",
            "training 18200 loss: 9.059511648956686e-05\n",
            "training 18300 loss: 0.0035281148739159107\n",
            "training 18400 loss: 0.004113552626222372\n",
            "training 18500 loss: 0.011636524461209774\n",
            "training 18600 loss: 0.004330920055508614\n",
            "training 18700 loss: 4.800333499588305e-07\n",
            "training 18800 loss: 0.0006767080631107092\n",
            "training 18900 loss: 0.00034152151783928275\n",
            "training 19000 loss: 3.1161116567091085e-06\n",
            "training 19100 loss: 9.943793702404946e-05\n",
            "training 19200 loss: 0.0018695180770009756\n",
            "training 19300 loss: 3.780316092161229e-06\n",
            "training 19400 loss: 0.004212989937514067\n",
            "training 19500 loss: 0.015417909249663353\n",
            "training 19600 loss: 0.006648866925388575\n",
            "training 19700 loss: 0.0008968681795522571\n",
            "training 19800 loss: 0.002553782658651471\n",
            "training 19900 loss: 0.008417957462370396\n",
            "training 20000 loss: 2.4948718419182114e-05\n",
            "20000 Validation Loss: 0.005\n",
            "training 20100 loss: 0.024660401046276093\n",
            "training 20200 loss: 0.0037684885319322348\n",
            "training 20300 loss: 0.0009589254041202366\n",
            "training 20400 loss: 0.000545723654795438\n",
            "training 20500 loss: 0.0018800891702994704\n",
            "training 20600 loss: 0.00015609030378982425\n",
            "training 20700 loss: 0.015978345647454262\n",
            "training 20800 loss: 0.00015793656348250806\n",
            "training 20900 loss: 0.00017640047008171678\n",
            "training 21000 loss: 0.0033420040272176266\n",
            "training 21100 loss: 0.0019641451071947813\n",
            "training 21200 loss: 0.021412573754787445\n",
            "training 21300 loss: 0.0021523027680814266\n",
            "training 21400 loss: 0.0023876328486949205\n",
            "training 21500 loss: 0.007978862151503563\n",
            "training 21600 loss: 0.00015499605797231197\n",
            "training 21700 loss: 0.0011176671832799911\n",
            "training 21800 loss: 0.0001581223914399743\n",
            "training 21900 loss: 1.5546984286629595e-05\n",
            "training 22000 loss: 0.0015666414983570576\n",
            "training 22100 loss: 0.00014443721738643944\n",
            "training 22200 loss: 0.0005991237703710794\n",
            "training 22300 loss: 0.00515825254842639\n",
            "training 22400 loss: 0.009184250608086586\n",
            "training 22500 loss: 0.0016765949549153447\n",
            "training 22600 loss: 0.0010834676213562489\n",
            "training 22700 loss: 0.0036272816359996796\n",
            "training 22800 loss: 0.0001387127849739045\n",
            "training 22900 loss: 0.0009656260372139513\n",
            "training 23000 loss: 0.0002850322052836418\n",
            "training 23100 loss: 0.006769710686057806\n",
            "training 23200 loss: 0.001846088096499443\n",
            "training 23300 loss: 0.0004153160552959889\n",
            "training 23400 loss: 0.015772270038723946\n",
            "training 23500 loss: 0.0007893294095993042\n",
            "training 23600 loss: 0.029157401993870735\n",
            "training 23700 loss: 0.0067118871957063675\n",
            "training 23800 loss: 0.0014933645725250244\n",
            "training 23900 loss: 0.011761854402720928\n",
            "training 24000 loss: 1.9644409121610806e-07\n",
            "training 24100 loss: 0.0006463314639404416\n",
            "training 24200 loss: 0.0017675913404673338\n",
            "training 24300 loss: 0.01927713118493557\n",
            "training 24400 loss: 0.0022096659522503614\n",
            "training 24500 loss: 0.001731987576931715\n",
            "training 24600 loss: 0.009213520213961601\n",
            "training 24700 loss: 0.03139665722846985\n",
            "training 24800 loss: 0.00012733120820485055\n",
            "training 24900 loss: 8.358345803571865e-05\n",
            "training 25000 loss: 0.0011390341678634286\n",
            "25000 Validation Loss: 0.004\n",
            "training 25100 loss: 0.012177941389381886\n",
            "training 25200 loss: 0.002592264674603939\n",
            "training 25300 loss: 0.0019462862983345985\n",
            "training 25400 loss: 7.055137393763289e-05\n",
            "training 25500 loss: 0.003124101087450981\n",
            "training 25600 loss: 0.013545741327106953\n",
            "training 25700 loss: 2.5980945792980492e-05\n",
            "training 25800 loss: 0.005454338621348143\n",
            "training 25900 loss: 0.00917018111795187\n",
            "training 26000 loss: 0.0013898223405703902\n",
            "training 26100 loss: 0.010997183620929718\n",
            "training 26200 loss: 0.00430575804784894\n",
            "training 26300 loss: 0.007468385621905327\n",
            "training 26400 loss: 7.025530067039654e-05\n",
            "training 26500 loss: 6.2538629208575e-06\n",
            "training 26600 loss: 3.140780972898938e-05\n",
            "training 26700 loss: 0.00038482050877064466\n",
            "training 26800 loss: 0.019532328471541405\n",
            "training 26900 loss: 0.0037414603866636753\n",
            "training 27000 loss: 0.00522938696667552\n",
            "training 27100 loss: 0.0018941126763820648\n",
            "training 27200 loss: 0.00011316194286337122\n",
            "training 27300 loss: 0.0018542512552812696\n",
            "training 27400 loss: 0.014206819236278534\n",
            "training 27500 loss: 0.0008660355815663934\n",
            "training 27600 loss: 0.00017378199845552444\n",
            "training 27700 loss: 0.001840028678998351\n",
            "training 27800 loss: 0.004132688976824284\n",
            "training 27900 loss: 0.0014572476502507925\n",
            "training 28000 loss: 1.697698280622717e-06\n",
            "training 28100 loss: 0.004847792908549309\n",
            "training 28200 loss: 0.00038739715819247067\n",
            "training 28300 loss: 0.001286251819692552\n",
            "training 28400 loss: 0.0005137033294886351\n",
            "training 28500 loss: 0.002506890334188938\n",
            "training 28600 loss: 0.0007510876166634262\n",
            "training 28700 loss: 0.01582273468375206\n",
            "training 28800 loss: 0.007614517118781805\n",
            "training 28900 loss: 0.00014738625031895936\n",
            "training 29000 loss: 0.0026664715260267258\n",
            "training 29100 loss: 0.00027220265474170446\n",
            "training 29200 loss: 0.00550704263150692\n",
            "training 29300 loss: 2.7362352739146445e-07\n",
            "training 29400 loss: 0.01759808324277401\n",
            "training 29500 loss: 0.0028646928258240223\n",
            "training 29600 loss: 0.09135962277650833\n",
            "training 29700 loss: 0.015709873288869858\n",
            "training 29800 loss: 0.004468182101845741\n",
            "training 29900 loss: 0.0002036143996519968\n",
            "training 30000 loss: 0.00210732314735651\n",
            "30000 Validation Loss: 0.005\n",
            "training 30100 loss: 0.003692852333188057\n",
            "training 30200 loss: 0.0028833283577114344\n",
            "training 30300 loss: 0.010547485202550888\n",
            "training 30400 loss: 0.0006291840691119432\n",
            "training 30500 loss: 3.4989177947863936e-05\n",
            "training 30600 loss: 0.0019498642068356276\n",
            "training 30700 loss: 0.005253586918115616\n",
            "training 30800 loss: 0.007035685703158379\n",
            "training 30900 loss: 0.009449724107980728\n",
            "training 31000 loss: 0.003981502261012793\n",
            "training 31100 loss: 0.0017472418257966638\n",
            "training 31200 loss: 0.0003605039091780782\n",
            "training 31300 loss: 0.019040778279304504\n",
            "training 31400 loss: 0.011949386447668076\n",
            "training 31500 loss: 0.0015221559442579746\n",
            "training 31600 loss: 0.0004680962592829019\n",
            "training 31700 loss: 0.004850249737501144\n",
            "training 31800 loss: 0.018014272674918175\n",
            "training 31900 loss: 0.0011123968288302422\n",
            "training 32000 loss: 0.0012048454955220222\n",
            "training 32100 loss: 0.0008487781160511076\n",
            "training 32200 loss: 4.225627344567329e-05\n",
            "training 32300 loss: 1.5441482901223935e-06\n",
            "training 32400 loss: 0.010700829327106476\n",
            "training 32500 loss: 0.0004956858465448022\n",
            "training 32600 loss: 0.006966900546103716\n",
            "training 32700 loss: 3.212266892660409e-05\n",
            "training 32800 loss: 0.004109913948923349\n",
            "training 32900 loss: 0.007381648290902376\n",
            "training 33000 loss: 0.0014590867795050144\n",
            "training 33100 loss: 0.015286333858966827\n",
            "training 33200 loss: 0.0005010082386434078\n",
            "training 33300 loss: 0.001554642105475068\n",
            "training 33400 loss: 0.003178573679178953\n",
            "training 33500 loss: 0.0018808955792337656\n",
            "training 33600 loss: 0.0007708310149610043\n",
            "training 33700 loss: 0.006966820918023586\n",
            "training 33800 loss: 0.054950863122940063\n",
            "training 33900 loss: 0.011267601512372494\n",
            "training 34000 loss: 0.00965075008571148\n",
            "training 34100 loss: 0.024967839941382408\n",
            "training 34200 loss: 0.009501107037067413\n",
            "training 34300 loss: 0.000149098937981762\n",
            "training 34400 loss: 0.02189934253692627\n",
            "training 34500 loss: 0.005061619449406862\n",
            "training 34600 loss: 0.01659187488257885\n",
            "training 34700 loss: 0.0046308222226798534\n",
            "training 34800 loss: 0.00022095879830885679\n",
            "training 34900 loss: 9.673670137999579e-05\n",
            "training 35000 loss: 3.619811650423799e-08\n",
            "35000 Validation Loss: 0.005\n",
            "training 35100 loss: 0.0003914150584023446\n",
            "training 35200 loss: 0.002971534850075841\n",
            "training 35300 loss: 8.15643034002278e-07\n",
            "training 35400 loss: 5.059448449173942e-05\n",
            "training 35500 loss: 8.666735084261745e-05\n",
            "training 35600 loss: 0.006455573718994856\n",
            "training 35700 loss: 4.313803219702095e-05\n",
            "training 35800 loss: 7.345231279032305e-05\n",
            "training 35900 loss: 0.0015386459417641163\n",
            "training 36000 loss: 0.0007728976779617369\n",
            "training 36100 loss: 0.004994974471628666\n",
            "training 36200 loss: 0.0035357095766812563\n",
            "training 36300 loss: 0.0001593178603798151\n",
            "training 36400 loss: 0.0002051140763796866\n",
            "training 36500 loss: 0.002694640075787902\n",
            "training 36600 loss: 0.017888158559799194\n",
            "training 36700 loss: 0.00886167585849762\n",
            "training 36800 loss: 0.013525381684303284\n",
            "training 36900 loss: 0.004351624753326178\n",
            "training 37000 loss: 0.006301530636847019\n",
            "training 37100 loss: 0.011201493442058563\n",
            "training 37200 loss: 0.0015642831567674875\n",
            "training 37300 loss: 0.011138246394693851\n",
            "training 37400 loss: 0.0013485187664628029\n",
            "training 37500 loss: 0.0005806178087368608\n",
            "training 37600 loss: 0.0043037873692810535\n",
            "training 37700 loss: 7.654514047317207e-05\n",
            "training 37800 loss: 0.002352348528802395\n",
            "training 37900 loss: 0.0026300319004803896\n",
            "training 38000 loss: 0.0037010572850704193\n",
            "training 38100 loss: 0.003514730604365468\n",
            "training 38200 loss: 0.003716331673786044\n",
            "training 38300 loss: 0.0015513536054641008\n",
            "training 38400 loss: 0.030383799225091934\n",
            "training 38500 loss: 0.007889497093856335\n",
            "training 38600 loss: 0.0007436311570927501\n",
            "training 38700 loss: 0.00047236672253347933\n",
            "training 38800 loss: 5.364767275750637e-05\n",
            "training 38900 loss: 3.9900216506794095e-05\n",
            "training 39000 loss: 0.0005914221401326358\n",
            "training 39100 loss: 3.726745489984751e-05\n",
            "training 39200 loss: 0.014086526818573475\n",
            "training 39300 loss: 0.0002725095837377012\n",
            "training 39400 loss: 0.005115805193781853\n",
            "training 39500 loss: 0.011378366500139236\n",
            "training 39600 loss: 0.002918667858466506\n",
            "training 39700 loss: 2.403783582849428e-05\n",
            "training 39800 loss: 0.0007053485605865717\n",
            "training 39900 loss: 0.029838936403393745\n",
            "training 40000 loss: 0.0002431135217193514\n",
            "40000 Validation Loss: 0.005\n",
            "training 40100 loss: 9.358391253044829e-06\n",
            "training 40200 loss: 0.00017120807024184614\n",
            "training 40300 loss: 0.0006590617122128606\n",
            "training 40400 loss: 0.002447771141305566\n",
            "training 40500 loss: 0.010408425703644753\n",
            "training 40600 loss: 0.044987939298152924\n",
            "training 40700 loss: 0.002338008489459753\n",
            "training 40800 loss: 0.0039644306525588036\n",
            "training 40900 loss: 0.003811376402154565\n",
            "training 41000 loss: 0.027912992984056473\n",
            "training 41100 loss: 0.00019200672977603972\n",
            "training 41200 loss: 0.0005633071414195001\n",
            "training 41300 loss: 0.003665231168270111\n",
            "training 41400 loss: 0.000308429152937606\n",
            "training 41500 loss: 0.018783332780003548\n",
            "training 41600 loss: 0.005703096278011799\n",
            "training 41700 loss: 0.0006873802631162107\n",
            "training 41800 loss: 0.0006142391939647496\n",
            "training 41900 loss: 6.31732473266311e-05\n",
            "training 42000 loss: 0.015402163378894329\n",
            "training 42100 loss: 0.0013332579983398318\n",
            "training 42200 loss: 0.0044166324660182\n",
            "training 42300 loss: 0.001056747860275209\n",
            "training 42400 loss: 0.007063230033963919\n",
            "training 42500 loss: 0.006531038321554661\n",
            "training 42600 loss: 2.163264980481472e-06\n",
            "training 42700 loss: 0.0033448715694248676\n",
            "training 42800 loss: 0.018376652151346207\n",
            "training 42900 loss: 0.009912103414535522\n",
            "training 43000 loss: 0.000531690486241132\n",
            "training 43100 loss: 0.007550614885985851\n",
            "training 43200 loss: 0.0011159302666783333\n",
            "training 43300 loss: 0.01240653544664383\n",
            "training 43400 loss: 0.007692316081374884\n",
            "training 43500 loss: 0.015698758885264397\n",
            "training 43600 loss: 0.0013414537534117699\n",
            "training 43700 loss: 0.006467839237302542\n",
            "training 43800 loss: 0.030237440019845963\n",
            "training 43900 loss: 0.010137445293366909\n",
            "training 44000 loss: 0.00039324734825640917\n",
            "training 44100 loss: 0.011386149562895298\n",
            "training 44200 loss: 0.007785860449075699\n",
            "training 44300 loss: 0.0011403542011976242\n",
            "training 44400 loss: 0.003799933474510908\n",
            "training 44500 loss: 0.0028952723369002342\n",
            "training 44600 loss: 0.004238811321556568\n",
            "training 44700 loss: 0.0005006880965083838\n",
            "training 44800 loss: 0.0034407475031912327\n",
            "training 44900 loss: 0.003318147035315633\n",
            "training 45000 loss: 0.00544156227260828\n",
            "45000 Validation Loss: 0.005\n",
            "training 45100 loss: 0.008324289694428444\n",
            "training 45200 loss: 0.020423881709575653\n",
            "training 45300 loss: 0.012332342565059662\n",
            "training 45400 loss: 0.004844373557716608\n",
            "training 45500 loss: 0.004325618501752615\n",
            "training 45600 loss: 0.0086570605635643\n",
            "training 45700 loss: 0.003954678773880005\n",
            "training 45800 loss: 0.00023078334925230592\n",
            "training 45900 loss: 0.009863123297691345\n",
            "training 46000 loss: 0.0002845172130037099\n",
            "training 46100 loss: 0.004999120719730854\n",
            "training 46200 loss: 0.0009639820200391114\n",
            "training 46300 loss: 0.001986184623092413\n",
            "training 46400 loss: 3.574632137315348e-05\n",
            "training 46500 loss: 0.00033540709409862757\n",
            "training 46600 loss: 0.00015737974899820983\n",
            "training 46700 loss: 0.0021288515999913216\n",
            "training 46800 loss: 0.00112363719381392\n",
            "training 46900 loss: 0.004442465491592884\n",
            "training 47000 loss: 4.91064420202747e-05\n",
            "training 47100 loss: 0.000989599502645433\n",
            "training 47200 loss: 0.0007881241617724299\n",
            "training 47300 loss: 0.0004990888992324471\n",
            "training 47400 loss: 0.011889622546732426\n",
            "training 47500 loss: 0.013616816140711308\n",
            "[5] average loss per epoch: 0.005\n",
            "Saved model checkpoint to /content/drive/My Drive/Task4/test5/model_epoch5.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_gap = pd.read_csv('train_features.csv')  # features\n",
        "label_gap = pd.read_csv('train_labels.csv')  # labels\n",
        "\n",
        "\n",
        "print(feature_gap.shape)\n",
        "print(label_gap.shape)\n",
        "\n",
        "# Merge two dataframes\n",
        "train_gap_data = pd.merge(label_gap, feature_gap, on='Id')\n",
        "print(train_gap_data.shape)\n",
        "print(train_gap_data.head(10))\n",
        "\n",
        "train_gap_data_noidsmiles = train_gap_data.drop(['Id', 'smiles'], axis=1)\n",
        "print(train_gap_data_noidsmiles.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo3C10RO796F",
        "outputId": "d68c7406-ac2e-4fd6-9a31-eb6351bd368d"
      },
      "id": "Lo3C10RO796F",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1002)\n",
            "(100, 2)\n",
            "(100, 1003)\n",
            "      Id  homo_lumo_gap                                             smiles  \\\n",
            "0  50000       2.052872    C1C=c2c3ccoc3c3c4ccccc4c(cc3c2=C1)-c1scc2ccsc12   \n",
            "1  50001       1.325530  c1cc([se]c1-c1sc(-c2cccc3nsnc23)c2nccnc12)-c1c...   \n",
            "2  50002       1.837294  [SiH2]1C=CC=C1c1cc2cnc3c(sc4ccc5c[nH]cc5c34)c2...   \n",
            "3  50003       1.388601  C1C=c2ccc3c4cocc4c4c([se]c5cc(-c6cccs6)c6nsnc6...   \n",
            "4  50004       0.991851  C1c(ccc1-c1sc(-c2nccc3nsnc23)c2ccoc12)-c1scc2c...   \n",
            "5  50005       1.181848  c1c[nH]c(c1)-c1sc(-c2ccc(-c3scc4[se]ccc34)c3ns...   \n",
            "6  50006       1.469864  C1C(=Cc2c1c1cnc3ccc4=C[SiH2]C=c4c3c1c1c[nH]cc2...   \n",
            "7  50007       1.780535  c1c[nH]c(c1)-c1ccc([nH]1)-c1sc(-c2scc3ccoc23)c...   \n",
            "8  50008       2.959695        c1cc2oc3c(ccc4cc(cnc34)-c3cccc4ccccc34)c2s1   \n",
            "9  50009       2.224978  c1cc2csc(-c3cc4cc5c6c[nH]cc6c6cc[se]c6c5cc4[nH...   \n",
            "\n",
            "   feature_0000  feature_0001  feature_0002  feature_0003  feature_0004  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           0.0           0.0           1.0           0.0   \n",
            "2           0.0           0.0           0.0           1.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           0.0           1.0           1.0   \n",
            "5           0.0           0.0           0.0           1.0           1.0   \n",
            "6           0.0           0.0           0.0           0.0           0.0   \n",
            "7           0.0           0.0           0.0           1.0           0.0   \n",
            "8           0.0           1.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0005  feature_0006  ...  feature_0990  feature_0991  feature_0992  \\\n",
            "0           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "1           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "2           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "3           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "4           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "5           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "6           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "7           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "8           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "9           0.0           0.0  ...           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0993  feature_0994  feature_0995  feature_0996  feature_0997  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           0.0           0.0           0.0           0.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           0.0           0.0           0.0   \n",
            "5           0.0           0.0           0.0           0.0           0.0   \n",
            "6           0.0           0.0           0.0           0.0           0.0   \n",
            "7           0.0           0.0           0.0           0.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0998  feature_0999  \n",
            "0           0.0           0.0  \n",
            "1           0.0           0.0  \n",
            "2           0.0           0.0  \n",
            "3           0.0           0.0  \n",
            "4           0.0           0.0  \n",
            "5           0.0           0.0  \n",
            "6           0.0           0.0  \n",
            "7           0.0           0.0  \n",
            "8           0.0           0.0  \n",
            "9           0.0           0.0  \n",
            "\n",
            "[10 rows x 1003 columns]\n",
            "   homo_lumo_gap  feature_0000  feature_0001  feature_0002  feature_0003  \\\n",
            "0       2.052872           0.0           0.0           0.0           0.0   \n",
            "1       1.325530           0.0           0.0           0.0           1.0   \n",
            "2       1.837294           0.0           0.0           0.0           1.0   \n",
            "3       1.388601           0.0           0.0           0.0           0.0   \n",
            "4       0.991851           0.0           0.0           0.0           1.0   \n",
            "5       1.181848           0.0           0.0           0.0           1.0   \n",
            "6       1.469864           0.0           0.0           0.0           0.0   \n",
            "7       1.780535           0.0           0.0           0.0           1.0   \n",
            "8       2.959695           0.0           1.0           0.0           0.0   \n",
            "9       2.224978           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0004  feature_0005  feature_0006  feature_0007  feature_0008  ...  \\\n",
            "0           0.0           0.0           0.0           0.0           1.0  ...   \n",
            "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "4           1.0           0.0           0.0           0.0           1.0  ...   \n",
            "5           1.0           0.0           0.0           0.0           0.0  ...   \n",
            "6           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "7           0.0           0.0           0.0           0.0           1.0  ...   \n",
            "8           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "9           0.0           0.0           0.0           1.0           0.0  ...   \n",
            "\n",
            "   feature_0990  feature_0991  feature_0992  feature_0993  feature_0994  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           0.0           0.0           0.0           0.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           0.0           0.0           0.0   \n",
            "5           0.0           0.0           0.0           0.0           0.0   \n",
            "6           0.0           0.0           0.0           0.0           0.0   \n",
            "7           0.0           0.0           0.0           0.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0995  feature_0996  feature_0997  feature_0998  feature_0999  \n",
            "0           0.0           0.0           0.0           0.0           0.0  \n",
            "1           0.0           0.0           0.0           0.0           0.0  \n",
            "2           0.0           0.0           0.0           0.0           0.0  \n",
            "3           0.0           0.0           0.0           0.0           0.0  \n",
            "4           0.0           0.0           0.0           0.0           0.0  \n",
            "5           0.0           0.0           0.0           0.0           0.0  \n",
            "6           0.0           0.0           0.0           0.0           0.0  \n",
            "7           0.0           0.0           0.0           0.0           0.0  \n",
            "8           0.0           0.0           0.0           0.0           0.0  \n",
            "9           0.0           0.0           0.0           0.0           0.0  \n",
            "\n",
            "[10 rows x 1001 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate gradient except for the first layer\n",
        "for name, param in model.named_parameters():\n",
        "  print(name)\n",
        "  if ('encoder_hidden_layer_1' in name):\n",
        "    param.requires_grad = False\n",
        "  else:\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_6BNjoydTdi",
        "outputId": "9b33b715-50ea-4ee5-f83a-8a142b9ae3f1"
      },
      "id": "n_6BNjoydTdi",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_hidden_layer_1.weight\n",
            "encoder_hidden_layer_1.bias\n",
            "encoder_hidden_layer_2.weight\n",
            "encoder_hidden_layer_2.bias\n",
            "encoder_hidden_layer_3.weight\n",
            "encoder_hidden_layer_3.bias\n",
            "encoder_hidden_layer_4.weight\n",
            "encoder_hidden_layer_4.bias\n",
            "prediction_layer.weight\n",
            "prediction_layer.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD2Gohc7UfgN",
        "outputId": "0f0d7e7b-7842-4e43-9361-66813f6e24c4"
      },
      "id": "tD2Gohc7UfgN",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeatureExtract(\n",
              "  (encoder_hidden_layer_1): Linear(in_features=1000, out_features=512, bias=True)\n",
              "  (encoder_hidden_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (encoder_hidden_layer_3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (encoder_hidden_layer_4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (prediction_layer): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_gap = nn.MSELoss()\n",
        "\n",
        "# Update the parameters for the layers with gradient calculated\n",
        "optimizer_gap = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                            lr=0.00005,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=2e-3,#The value used in the paper is 1e-3\n",
        "                            nesterov=True)\n",
        "\n",
        "def train_gap(model, criterion, optimizer, training_set_gap):\n",
        "  # Empty the cache of CUDA  \n",
        "  torch.cuda.empty_cache()\n",
        "   \n",
        "  print('================== START TRANSFER LEARNING ==================')\n",
        "\n",
        "  # Change to train mode\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0\n",
        "  for i in range(len(training_set_gap)):\n",
        "    # Get one input from the training set\n",
        "    input = torch.tensor(training_set_gap.iloc[i][1:], dtype=torch.float).to(device)\n",
        "\n",
        "    # Calculate its corresponding output\n",
        "    result = model(input)\n",
        "    # print(result)\n",
        "\n",
        "    # Calculate the MSE loss\n",
        "    loss = criterion(result, torch.tensor(training_set_gap.iloc[i][0:1], dtype=torch.float).to(device))\n",
        "\n",
        "    # print(f'training {i+1} loss: {loss}')\n",
        "\n",
        "    # Zero the gradient\n",
        "    optimizer.zero_grad()\n",
        "              \n",
        "    # Back prop and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  print(f'average loss per epoch: {running_loss / len(training_set_gap):.3f}')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hfcDHCGb7jus"
      },
      "id": "hfcDHCGb7jus",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the weight before training\n",
        "print(\"model.layer1.weight\", model.encoder_hidden_layer_1.weight)\n",
        "print(\"model.layer2.weight\", model.encoder_hidden_layer_2.weight)\n",
        "print(\"model.layer3.weight\", model.encoder_hidden_layer_3.weight)\n",
        "print(\"model.layer4.weight\", model.encoder_hidden_layer_4.weight)\n",
        "print(\"model.output.weight\", model.prediction_layer.weight)\n",
        "\n",
        "for i in range(4000):\n",
        "  train_gap(model, criterion_gap, optimizer_gap, train_gap_data_noidsmiles)\n",
        "\n",
        "# Print the weight after training\n",
        "print(\"model.layer1.weight\", model.encoder_hidden_layer_1.weight)\n",
        "print(\"model.layer2.weight\", model.encoder_hidden_layer_2.weight)\n",
        "print(\"model.layer3.weight\", model.encoder_hidden_layer_3.weight)\n",
        "print(\"model.layer4.weight\", model.encoder_hidden_layer_4.weight)\n",
        "print(\"model.output.weight\", model.prediction_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux66DHUtmFJY",
        "outputId": "575d8a52-246e-45bf-e357-2dc9e37f4f1b"
      },
      "id": "Ux66DHUtmFJY",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.017\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.016\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.015\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.014\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.013\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.012\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.011\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.010\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.009\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.008\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.007\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.006\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "================== START TRANSFER LEARNING ==================\n",
            "average loss per epoch: 0.005\n",
            "model.layer1.weight Parameter containing:\n",
            "tensor([[-0.0041,  0.0033,  0.0027,  ..., -0.0011, -0.0023,  0.0012],\n",
            "        [ 0.0025, -0.0027,  0.0013,  ...,  0.0013,  0.0036,  0.0023],\n",
            "        [-0.0076,  0.0048, -0.0019,  ..., -0.0031, -0.0022, -0.0025],\n",
            "        ...,\n",
            "        [ 0.0012,  0.0018,  0.0024,  ...,  0.0024, -0.0025,  0.0018],\n",
            "        [-0.0022,  0.0038, -0.0034,  ...,  0.0022, -0.0011, -0.0009],\n",
            "        [ 0.0040, -0.0041, -0.0042,  ...,  0.0017,  0.0025,  0.0034]],\n",
            "       device='cuda:0')\n",
            "model.layer2.weight Parameter containing:\n",
            "tensor([[ 1.1317e-03, -1.4788e-04,  1.2066e-03,  ..., -3.1741e-04,\n",
            "         -1.1650e-03, -2.3426e-03],\n",
            "        [ 1.1238e-03,  5.2366e-04,  5.0376e-04,  ...,  2.1917e-03,\n",
            "         -6.8162e-04, -7.4254e-05],\n",
            "        [-5.2288e-03, -5.4266e-04, -1.1727e-02,  ...,  6.5101e-04,\n",
            "         -5.1971e-03, -9.3264e-04],\n",
            "        ...,\n",
            "        [ 3.8605e-03,  3.7784e-03,  5.1975e-03,  ..., -1.9146e-03,\n",
            "          4.6234e-03,  2.1511e-03],\n",
            "        [ 1.4085e-02,  3.6537e-03,  2.1143e-02,  ...,  2.1320e-03,\n",
            "          1.8235e-02,  1.2095e-03],\n",
            "        [ 2.3046e-03,  1.6724e-03,  4.5733e-03,  ...,  2.7701e-03,\n",
            "          5.1495e-03, -1.5724e-03]], device='cuda:0', requires_grad=True)\n",
            "model.layer3.weight Parameter containing:\n",
            "tensor([[ 0.0004, -0.0010, -0.0004,  ...,  0.0007,  0.0058,  0.0044],\n",
            "        [-0.0013, -0.0037, -0.0012,  ..., -0.0021, -0.0016,  0.0024],\n",
            "        [-0.0020,  0.0012, -0.0001,  ...,  0.0039, -0.0033,  0.0011],\n",
            "        ...,\n",
            "        [-0.0023,  0.0009, -0.0035,  ...,  0.0026, -0.0010, -0.0020],\n",
            "        [-0.0028, -0.0007,  0.0099,  ..., -0.0043, -0.0209, -0.0042],\n",
            "        [ 0.0012,  0.0026, -0.0041,  ...,  0.0040,  0.0004, -0.0034]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "model.layer4.weight Parameter containing:\n",
            "tensor([[-4.8663e-03, -3.2639e-03,  2.3786e-04,  ..., -3.7801e-03,\n",
            "         -2.8136e-03, -4.5949e-03],\n",
            "        [-2.6766e-03, -4.1808e-03,  1.3577e-03,  ...,  4.9880e-03,\n",
            "         -5.7739e-04, -1.0891e-03],\n",
            "        [-2.1140e-03,  5.8433e-05, -5.1228e-03,  ...,  5.1342e-03,\n",
            "         -1.4505e-03,  3.4206e-03],\n",
            "        ...,\n",
            "        [-1.5130e-02, -3.6842e-03, -4.2137e-03,  ...,  4.2713e-03,\n",
            "          6.9892e-02,  2.4463e-03],\n",
            "        [-5.5215e-03, -4.2401e-03,  1.7316e-03,  ...,  5.0463e-03,\n",
            "          5.4042e-02,  1.9703e-03],\n",
            "        [-9.5304e-04, -5.2129e-04, -4.1114e-03,  ...,  2.5814e-03,\n",
            "          1.7947e-03, -1.1683e-03]], device='cuda:0', requires_grad=True)\n",
            "model.output.weight Parameter containing:\n",
            "tensor([[-1.2205e-02,  7.0546e-04, -4.3361e-03,  1.9835e-03,  6.1055e-03,\n",
            "         -3.2724e-01,  3.5211e-03,  6.5187e-03,  3.5812e-03, -7.1859e-03,\n",
            "         -7.5616e-02,  9.0855e-04,  8.7913e-03,  7.5126e-02,  7.6867e-04,\n",
            "          2.4466e-03,  1.2191e-04,  1.6969e-01,  8.4730e-03, -2.7852e-02,\n",
            "          3.8129e-04, -1.5643e-01, -3.8545e-01,  4.9770e-03, -7.2441e-03,\n",
            "         -3.6743e-03, -7.4971e-03,  8.9529e-02, -5.6166e-03, -1.2066e-03,\n",
            "         -4.4026e-03, -1.6848e-01,  1.5945e-03, -1.7457e-01,  7.5030e-03,\n",
            "         -4.9637e-01, -1.9301e-01,  3.3731e-03, -1.1931e-01, -6.7829e-03,\n",
            "         -4.5285e-01, -6.8062e-03,  1.0071e-02,  8.6369e-03,  3.2562e-02,\n",
            "         -1.0651e-02, -3.4249e-01,  5.1592e-03,  8.8347e-04,  7.4396e-03,\n",
            "         -5.6014e-01, -9.2516e-02,  6.4339e-03, -4.5751e-01, -4.2847e-03,\n",
            "          4.0868e-03,  6.0466e-01, -3.2803e-01, -1.0744e-01,  7.0891e-03,\n",
            "         -1.0158e-01, -5.5417e-01, -4.7025e-01,  2.1877e-02]], device='cuda:0',\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(train_gap_data_noidsmiles.iloc[95][1:], dtype=torch.float).to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQc7311qeqOy",
        "outputId": "de0d0655-4c76-427f-fb81-f2f0c14bf782"
      },
      "id": "KQc7311qeqOy",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.9445], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Prediction\n",
        "test_feature = pd.read_csv('test_features.csv')\n",
        "test_id = test_feature['Id']\n",
        "print(test_id.head(10))\n",
        "test_feature_noidsmiles = test_feature.drop(['Id', 'smiles'], axis=1)\n",
        "print(test_feature_noidsmiles.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl04ftEW-BwI",
        "outputId": "56694d04-e8c5-4328-db89-c716224a3b1e"
      },
      "id": "hl04ftEW-BwI",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    50100\n",
            "1    50101\n",
            "2    50102\n",
            "3    50103\n",
            "4    50104\n",
            "5    50105\n",
            "6    50106\n",
            "7    50107\n",
            "8    50108\n",
            "9    50109\n",
            "Name: Id, dtype: int64\n",
            "   feature_0000  feature_0001  feature_0002  feature_0003  feature_0004  \\\n",
            "0           0.0           0.0           0.0           1.0           1.0   \n",
            "1           0.0           0.0           0.0           0.0           0.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           1.0   \n",
            "4           0.0           0.0           0.0           1.0           0.0   \n",
            "5           0.0           0.0           0.0           1.0           0.0   \n",
            "6           0.0           0.0           0.0           0.0           1.0   \n",
            "7           0.0           0.0           0.0           1.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0005  feature_0006  feature_0007  feature_0008  feature_0009  ...  \\\n",
            "0           0.0           0.0           0.0           1.0           0.0  ...   \n",
            "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "5           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "6           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "7           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "8           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "9           0.0           0.0           0.0           0.0           0.0  ...   \n",
            "\n",
            "   feature_0990  feature_0991  feature_0992  feature_0993  feature_0994  \\\n",
            "0           0.0           0.0           0.0           0.0           0.0   \n",
            "1           0.0           0.0           0.0           0.0           0.0   \n",
            "2           0.0           0.0           0.0           0.0           0.0   \n",
            "3           0.0           0.0           0.0           0.0           0.0   \n",
            "4           0.0           0.0           0.0           0.0           0.0   \n",
            "5           0.0           0.0           0.0           0.0           0.0   \n",
            "6           0.0           0.0           0.0           0.0           1.0   \n",
            "7           0.0           0.0           0.0           0.0           0.0   \n",
            "8           0.0           0.0           0.0           0.0           0.0   \n",
            "9           0.0           0.0           0.0           0.0           0.0   \n",
            "\n",
            "   feature_0995  feature_0996  feature_0997  feature_0998  feature_0999  \n",
            "0           0.0           0.0           0.0           0.0           0.0  \n",
            "1           0.0           0.0           0.0           0.0           0.0  \n",
            "2           0.0           0.0           0.0           0.0           0.0  \n",
            "3           0.0           0.0           0.0           0.0           0.0  \n",
            "4           0.0           0.0           0.0           0.0           0.0  \n",
            "5           0.0           0.0           0.0           0.0           0.0  \n",
            "6           0.0           0.0           0.0           0.0           0.0  \n",
            "7           0.0           0.0           0.0           0.0           0.0  \n",
            "8           0.0           0.0           0.0           0.0           0.0  \n",
            "9           0.0           0.0           0.0           0.0           0.0  \n",
            "\n",
            "[10 rows x 1000 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_features, test_id, epoch_run):\n",
        "  \n",
        "  print('================== START PREDICTION ==================')\n",
        "  model.eval()\n",
        "\n",
        "  y = []\n",
        "  for i in range(len(test_feature)):\n",
        "    input = torch.tensor(test_features.iloc[i], dtype=torch.float).to(device)\n",
        "\n",
        "    # Calculate its corresponding output\n",
        "    with torch.no_grad():\n",
        "      result = model(input)\n",
        "      result = result.item()\n",
        "      y.append(result)\n",
        "\n",
        "  output_df = pd.DataFrame(y, columns=['y'])\n",
        "\n",
        "  prediction = pd.concat([test_id, output_df], join='outer', axis=1)\n",
        "\n",
        "  prediction.to_csv('/content/drive/MyDrive/Task4/test5/submission_epoch{0}.csv'.format(epoch_run), index=False, header=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OlQ5zEQY9P2e"
      },
      "id": "OlQ5zEQY9P2e",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, test_feature_noidsmiles, test_id, 5)"
      ],
      "metadata": {
        "id": "_nTFSvJLDYMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc6440b-b5aa-49ec-8bc1-4115c5c5b017"
      },
      "id": "_nTFSvJLDYMV",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START PREDICTION ==================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Task_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}