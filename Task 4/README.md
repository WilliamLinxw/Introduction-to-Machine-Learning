In this project, we are required to predict the HOMO-LUMO gap of a given molecule. It's a transfer learning task, namely using part of a pretrained network with the features it learns to solve a similar but different task. Because the data set for HOMO-KUMO-gap-labeled samples is very limited, i.e. 100, while the dataset for E-LUMO-labeled data is sufficient, i.e. 50000. We would like to use the E-LUMO dataset to pretrain a neural network. With the features learned, we train a new network with the backbone of the pretrained network to predict the HOMO-LUMO gap of a molecule. Unlike task 3 where I used a pretrained ResNet18, I constructed the backbone neural network constructed myself.

Each of the sample in the pretrain dataset has 1000 features. I first constructed the backbone network, a fully connected NN with 5 layers, mapping from 1000 dimensions to 512, 512 to 256, 256 to 128, 128 to 64, and the last 64 to 1 output. I then fed all the samples, one by one, to the network to train it for five epochs. Feeding the samples one by one increases the randomness but leads to a slower convergence. With the pretrained network, I froze the first layer and use the training set of 100 samples to update the parameters of the last four layers for 4000 epochs until convergence. It doesn't overfit after so many epochs, I consider, because with the first layer fixed, the function class this NN can represent is not complex enough to fit the noise. Therefore, running the training until convergence can give a good result.

I used Colab for the task. To reproduce the result, make sure the datasets are in the corresponding folder and run the code until the last cell.
