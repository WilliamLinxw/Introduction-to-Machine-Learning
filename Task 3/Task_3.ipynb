{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVAaWUZmJuEf",
        "outputId": "a85bc0e5-0d81-4ea1-8593-2cdd303e86e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BVAaWUZmJuEf"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9cafface"
      },
      "outputs": [],
      "source": [
        "# Solve the imshow dead kernel problem\n",
        "import os    \n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "id": "9cafface"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc461e5c",
        "outputId": "55313919-ad05-420f-a805-7e6b4dfcc5d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START LOADING DATA ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start loading the data\n",
        "'''\n",
        "print('================== START LOADING DATA ==================')"
      ],
      "id": "bc461e5c"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aa85839b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "aa85839b"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e8218f58"
      },
      "outputs": [],
      "source": [
        "# Split the training set into a 80% training set and 20% validation set\n",
        "import random\n",
        "\n",
        "def split_huge_file(file,out1,out2,percentage=0.75,seed=2022):\n",
        "    \"\"\"Splits a file in 2 given the approximate `percentage` to go in the large file.\"\"\"\n",
        "    random.seed(seed)\n",
        "    with open(file, 'r',encoding=\"utf-8\") as fin, \\\n",
        "         open(out1, 'w') as foutBig, \\\n",
        "         open(out2, 'w') as foutSmall:\n",
        "\n",
        "        for line in fin:\n",
        "            r = random.random() \n",
        "            if r < percentage:\n",
        "                foutBig.write(line)\n",
        "            else:\n",
        "                foutSmall.write(line)"
      ],
      "id": "e8218f58"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7a883b1c"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/My Drive/'\n",
        "split_huge_file(os.path.join(path, f'train_triplets.txt'), 'train_triplets_splits.txt', 'val_triplets_splits.txt', percentage=0.99, seed=2022)"
      ],
      "id": "7a883b1c"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RWybssqKbO2N"
      },
      "id": "RWybssqKbO2N",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhLDDexX2vpg",
        "outputId": "0f003768-3143-4f1b-b720-9c58ecaffdd6"
      },
      "id": "WhLDDexX2vpg",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May  7 08:03:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8aea5632"
      },
      "outputs": [],
      "source": [
        "# Image loader helper function\n",
        "def default_image_loader(path):\n",
        "    return Image.open(path).convert('RGB')"
      ],
      "id": "8aea5632"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0fd6fd88"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "im = Image.open(r\"/content/drive/My Drive/food/00003.jpg\")"
      ],
      "id": "0fd6fd88"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cbb865c1"
      },
      "outputs": [],
      "source": [
        "# display(im)"
      ],
      "id": "cbb865c1"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9407678a",
        "outputId": "67986e1c-4de0-4ac2-b931-a7c42c03d844"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(329, 468, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data = np.asarray(im)\n",
        "data.shape"
      ],
      "id": "9407678a"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7960455c"
      },
      "outputs": [],
      "source": [
        "# im.resize((354,242))"
      ],
      "id": "7960455c"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "33fe9cfd"
      },
      "outputs": [],
      "source": [
        "class TripletImageLoader(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_path, triplets_file_name, transform=None, loader=default_image_loader):\n",
        "        \"\"\" base_path: The path contains the text file of the training triplets\n",
        "            triplets_file_name: The text file with each line containing three integers, \n",
        "            where integer i refers to the i-th image in the filenames file.  \n",
        "            Each line contains three integers (a triplet).\n",
        "            For example, the triplet \"00723 00478 02630\" denotes that the dish in image \"00723.jpg\" is more similar in taste \n",
        "            to the dish in image \"00478.jpg\" than to the dish in image \"02630.jpg\" according to a human annotator.\n",
        "         \"\"\"\n",
        "        self.base_path = base_path  \n",
        "        triplets = []\n",
        "        for line in open(triplets_file_name):\n",
        "            triplets.append((line.split()[0], line.split()[1], line.split()[2])) # anchor, positive, negative\n",
        "        self.triplets = triplets\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path1, path2, path3 = self.triplets[index]\n",
        "        img1 = self.loader(os.path.join(self.base_path, f'{path1}.jpg'))\n",
        "        img2 = self.loader(os.path.join(self.base_path, f'{path2}.jpg'))\n",
        "        img3 = self.loader(os.path.join(self.base_path, f'{path3}.jpg'))\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "\n",
        "        return img1, img2, img3\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)"
      ],
      "id": "33fe9cfd"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "38209c7b"
      },
      "outputs": [],
      "source": [
        "# Initialization: importing the packages that we will use\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Google colab offers time limited use of GPU for free\n",
        "\n",
        "################# Configuration  ######################\n",
        "IMAGE_SIZE = (242, 354) # bigger image size improves performance but makes training slower.\n",
        "\n",
        "# Training parameters \n",
        "BATCH_SIZE = 64"
      ],
      "id": "38209c7b"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9974abbb",
        "outputId": "c5f89eb7-00ea-41b2-895b-58a7c466c682"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ],
      "id": "9974abbb"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zS2MM05sNKp-",
        "outputId": "aa9ea4ce-83a0-491e-d67c-5a886646774c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "device"
      ],
      "id": "zS2MM05sNKp-"
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/test_triplets.txt /content"
      ],
      "metadata": {
        "id": "Ay-2AUvx-Hyu"
      },
      "id": "Ay-2AUvx-Hyu",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3b63d3d1"
      },
      "outputs": [],
      "source": [
        "# Dataset and Trasformations\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "############# Datasets and Dataloaders ################\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(), # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "\n",
        "    # we want our network to be robust over geometrical transformations that leave the image semantically invariant\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation(45),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #  We transform them to Tensors of normalized range [-1, 1].\n",
        "    # (mean, mean, mean) , (std, std, std): output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "path = '/content/drive/MyDrive/food'\n",
        "train_dataset = TripletImageLoader(path.rstrip('\\n'), 'train_triplets_splits.txt', transform=transform_train)\n",
        "val_dataset = TripletImageLoader(path.rstrip('\\n'), 'val_triplets_splits.txt', transform=transform_val)\n",
        "test_dataset = TripletImageLoader(path.rstrip('\\n'), 'test_triplets.txt', transform=transform_test)"
      ],
      "id": "3b63d3d1"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ff181f",
        "outputId": "ff574580-fb73-4b6b-a5ad-511ca79f030a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58933, 582, 59544)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "id": "a7ff181f"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66669e53",
        "outputId": "d481c0f6-379b-4e72-c9e7-56503c5251ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.6848,  0.6188,  0.6295,  ...,  0.6845,  0.6174,  0.5918],\n",
              "          [ 0.7634,  0.6081,  0.6502,  ...,  0.7868,  0.7373,  0.6763],\n",
              "          [ 0.7259,  0.7257,  0.6671,  ...,  0.7282,  0.7121,  0.6763],\n",
              "          ...,\n",
              "          [ 0.1368,  0.1779,  0.1858,  ...,  0.5491,  0.5174,  0.5192],\n",
              "          [ 0.1719,  0.1433,  0.1679,  ...,  0.5418,  0.5576,  0.5111],\n",
              "          [ 0.2298,  0.0134,  0.0636,  ...,  0.6480,  0.7295,  0.7176]],\n",
              " \n",
              "         [[ 0.3083,  0.2423,  0.2557,  ...,  0.2424,  0.3027,  0.3446],\n",
              "          [ 0.3778,  0.2235,  0.2622,  ...,  0.3537,  0.3888,  0.3711],\n",
              "          [ 0.3208,  0.3215,  0.2648,  ...,  0.3097,  0.3167,  0.2928],\n",
              "          ...,\n",
              "          [-0.2550, -0.2139, -0.2107,  ...,  0.0953,  0.1322,  0.1737],\n",
              "          [-0.2046, -0.2332, -0.2138,  ...,  0.0879,  0.1528,  0.1360],\n",
              "          [-0.1141, -0.3449, -0.3147,  ...,  0.1256,  0.0728, -0.0351]],\n",
              " \n",
              "         [[ 0.1512,  0.0762,  0.0626,  ..., -0.0076, -0.0635, -0.0872],\n",
              "          [ 0.2222,  0.0503,  0.0729,  ...,  0.0857,  0.1187,  0.0999],\n",
              "          [ 0.1609,  0.1532,  0.0806,  ...,  0.0070,  0.1151,  0.1430],\n",
              "          ...,\n",
              "          [-0.6279, -0.5769, -0.5614,  ..., -0.1400, -0.3230, -0.3943],\n",
              "          [-0.5889, -0.6165, -0.5872,  ..., -0.1474, -0.1789, -0.2451],\n",
              "          [-0.4484, -0.6471, -0.5584,  ..., -0.1159, -0.0343, -0.0555]]]),\n",
              " tensor([[[0.9716, 0.9817, 0.9216,  ..., 0.9213, 0.8747, 0.8964],\n",
              "          [0.8742, 0.9024, 0.9108,  ..., 0.9460, 0.9252, 0.9207],\n",
              "          [0.9315, 0.9169, 0.9928,  ..., 0.9602, 0.9514, 0.9227],\n",
              "          ...,\n",
              "          [0.9928, 0.8873, 0.8463,  ..., 0.9299, 0.9639, 0.9754],\n",
              "          [0.9918, 0.8900, 0.8946,  ..., 0.9904, 0.8834, 0.8569],\n",
              "          [0.9392, 0.9270, 0.9678,  ..., 1.0000, 0.9470, 0.9445]],\n",
              " \n",
              "         [[0.9498, 0.9751, 0.8980,  ..., 0.8986, 0.8418, 0.8492],\n",
              "          [0.8506, 0.8799, 0.8897,  ..., 0.9100, 0.8667, 0.8481],\n",
              "          [0.9057, 0.8961, 0.9808,  ..., 0.9032, 0.8778, 0.8328],\n",
              "          ...,\n",
              "          [0.9762, 0.8023, 0.7349,  ..., 0.9023, 0.9496, 0.9684],\n",
              "          [0.9093, 0.7772, 0.7547,  ..., 0.9391, 0.8346, 0.8207],\n",
              "          [0.8007, 0.7855, 0.8103,  ..., 0.9511, 0.8843, 0.8824]],\n",
              " \n",
              "         [[0.8159, 0.8326, 0.7496,  ..., 0.8866, 0.8447, 0.8637],\n",
              "          [0.7200, 0.7490, 0.7521,  ..., 0.8754, 0.8447, 0.8475],\n",
              "          [0.7949, 0.7840, 0.8595,  ..., 0.8347, 0.8253, 0.7896],\n",
              "          ...,\n",
              "          [0.4972, 0.3007, 0.2132,  ..., 0.4547, 0.4431, 0.3899],\n",
              "          [0.4153, 0.2669, 0.2286,  ..., 0.4663, 0.2925, 0.1991],\n",
              "          [0.2980, 0.2647, 0.2715,  ..., 0.4342, 0.3089, 0.2374]]]),\n",
              " tensor([[[ 0.0331,  0.0900,  0.1960,  ..., -0.1534, -0.2033, -0.2207],\n",
              "          [ 0.1129,  0.1367,  0.2146,  ..., -0.1964, -0.1716, -0.1799],\n",
              "          [ 0.2135,  0.2242,  0.2509,  ..., -0.1864, -0.2052, -0.1794],\n",
              "          ...,\n",
              "          [-0.8423, -0.7983, -0.8123,  ..., -0.9410, -0.9332, -0.9569],\n",
              "          [-0.8787, -0.7546, -0.7393,  ..., -0.9468, -0.9598, -0.9783],\n",
              "          [-0.8815, -0.8117, -0.6378,  ..., -0.9609, -0.9906, -0.9991]],\n",
              " \n",
              "         [[-0.6539, -0.6050, -0.5067,  ..., -0.8570, -0.9107, -0.9321],\n",
              "          [-0.6197, -0.6015, -0.5323,  ..., -0.8866, -0.8696, -0.8779],\n",
              "          [-0.5927, -0.5852, -0.5585,  ..., -0.8533, -0.8764, -0.8541],\n",
              "          ...,\n",
              "          [-0.8133, -0.7810, -0.8200,  ..., -0.9331, -0.9019, -0.9193],\n",
              "          [-0.8412, -0.7344, -0.7470,  ..., -0.9389, -0.9328, -0.9415],\n",
              "          [-0.8298, -0.7848, -0.6522,  ..., -0.9530, -0.9725, -0.9763]],\n",
              " \n",
              "         [[-0.9876, -0.9549, -0.8518,  ..., -0.9287, -0.9644, -0.9599],\n",
              "          [-0.9759, -0.9650, -0.8879,  ..., -0.9650, -0.9260, -0.9124],\n",
              "          [-0.9720, -0.9597, -0.9330,  ..., -0.9375, -0.9421, -0.8945],\n",
              "          ...,\n",
              "          [-0.7238, -0.6822, -0.7102,  ..., -0.9018, -0.8783, -0.8957],\n",
              "          [-0.7376, -0.6251, -0.6372,  ..., -0.9075, -0.9006, -0.9128],\n",
              "          [-0.7064, -0.6645, -0.5424,  ..., -0.9149, -0.9333, -0.9434]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "test_dataset[0]"
      ],
      "id": "66669e53"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9804534d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
      ],
      "id": "9804534d"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a237b3",
        "outputId": "b79b779e-49a1-40cd-923d-c9d0c6224183"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(921, 10, 931)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ],
      "id": "f7a237b3"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cdce85ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "ab88255c-7216-42a9-f798-582b66e22fe9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cdf5b05605db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# get some random training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mimages_anchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_positive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_negative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "# Visualization of Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    plt.figure()\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images_anchor, images_positive, images_negative = dataiter.next()\n",
        "\n",
        "# show images\n",
        "# imshow(torchvision.utils.make_grid(images_anchor))\n",
        "# imshow(torchvision.utils.make_grid(images_positive))\n",
        "# imshow(torchvision.utils.make_grid(images_negative))"
      ],
      "id": "cdce85ca"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40002326",
        "outputId": "33f3ceff-089b-4aa9-b4e9-a2f4caa329d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== DATA LOADED ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Data loaded\n",
        "'''\n",
        "print('================== DATA LOADED ==================')"
      ],
      "id": "40002326"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87028c17",
        "outputId": "2988d8cb-dfaf-48a6-c68e-719ec571b0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START CONSTRUCTING NETWORK ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start constructing the network\n",
        "'''\n",
        "print('================== START CONSTRUCTING NETWORK ==================')"
      ],
      "id": "87028c17"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7605800f"
      },
      "outputs": [],
      "source": [
        "# Construct a triplet net\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "#########################NET##############################\n",
        "\n",
        "#The backbone for the CNNS with shared weights\n",
        "def FeatureExtractNET(**kwargs):\n",
        "    \"\"\"\n",
        "    Construct a ResNet-101 model.\n",
        "    Returns: The CNN for feature extraction with a fully connected layer\n",
        "    \"\"\"\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    return EmbeddingNet(model)\n",
        "\n",
        "#The CNN used by Triplet Net with 'model' as its backbone and a final fully connected Layer\n",
        "class EmbeddingNet(nn.Module):\n",
        "    \"\"\"EmbeddingNet using the specified model in backbone().\"\"\"\n",
        "\n",
        "    def __init__(self, resnet):\n",
        "        \"\"\"Initialize EmbeddingNet model.\"\"\"\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        # Everything excluding the last linear layer\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        num_ftrs =  resnet.fc.in_features\n",
        "        self.fc1 = nn.Linear(num_ftrs, 1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of EmbeddingNet.\"\"\"\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "    \n",
        "#The overall network consisting of three embedding nets with shared weights\n",
        "class TripletNet(nn.Module):\n",
        "    \"\"\"Triplet Network.\"\"\"\n",
        "\n",
        "    def __init__(self, embeddingnet):\n",
        "        \"\"\"Triplet Network Builder.\"\"\"\n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embeddingnet = embeddingnet\n",
        "\n",
        "    def forward(self, a, p, n):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # anchor\n",
        "        embedded_a = self.embeddingnet(a)\n",
        "\n",
        "        # positive examples\n",
        "        embedded_p = self.embeddingnet(p)\n",
        "\n",
        "        # negative examples\n",
        "        embedded_n = self.embeddingnet(n)\n",
        "\n",
        "        return embedded_a, embedded_p, embedded_n"
      ],
      "id": "7605800f"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7843631c",
        "outputId": "10d0ae08-8ce3-4048-b6bb-c2e75026b9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Initialize CUDA support for TripletNet model ...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): TripletNet(\n",
              "    (embeddingnet): EmbeddingNet(\n",
              "      (features): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "        (4): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      )\n",
              "      (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "net = TripletNet(FeatureExtractNET())\n",
        "\n",
        "#Move the net to GPU for training\n",
        "print(\"==> Initialize CUDA support for TripletNet model ...\")\n",
        "net = torch.nn.DataParallel(net).cuda()\n",
        "cudnn.benchmark = True\n",
        "net"
      ],
      "id": "7843631c"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wO2TH6TjURu8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "wO2TH6TjURu8"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3626cfdd"
      },
      "outputs": [],
      "source": [
        "# batch = next(iter(train_loader))\n",
        "# batch[0], batch[1], batch[2] = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()\n",
        "# net(batch[0],batch[1],batch[2])[0].size()"
      ],
      "id": "3626cfdd"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "b71be7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97296dee-b1ae-43f3-eea2-7f9b3cf0b8d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Used to visualize the network structure'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "''' Used to visualize the network structure'''\n",
        "# import hiddenlayer as hl\n",
        "\n",
        "# transforms = [hl.transforms.Prune('Constant')] # Removes Constant nodes from graph.\n",
        "\n",
        "# graph = hl.build_graph(net, (batch[0], batch[1], batch[2]), transforms=transforms)\n",
        "# graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "# graph.save('rnn_hiddenlayer_1', format='png')"
      ],
      "id": "b71be7bc"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5fa4121e"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.TripletMarginLoss(margin=5.0, p=2)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(),\n",
        "                            lr=0.001,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=2e-3,#The value used in the paper is 1e-3\n",
        "                            nesterov=True)"
      ],
      "id": "5fa4121e"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vQXSC0QUb2y9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "vQXSC0QUb2y9"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "239e50ac"
      },
      "outputs": [],
      "source": [
        "# from torch.autograd import Variable\n",
        "# for epoch in range(1):\n",
        "\n",
        "#         running_loss = 0.0\n",
        "#         loss_train = 0.0\n",
        "#         for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
        "\n",
        "# #             if is_gpu:\n",
        "# #                 data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "#             # wrap in torch.autograd.Variable\n",
        "#             data1, data2, data3 = Variable(\n",
        "#                 data1), Variable(data2), Variable(data3)\n",
        "#             print('anchor', data1.size())\n",
        "#             print('positive', data2.size())\n",
        "#             print('negative', data3.size())\n",
        "\n",
        "#             # compute output and loss\n",
        "#             embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
        "#             loss = criterion(embedded_a, embedded_p, embedded_n)\n",
        "#             print(loss)\n",
        "\n",
        "#             # compute gradient and do optimizer step\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # print the loss\n",
        "#             running_loss += loss.data\n",
        "\n",
        "# #             loss_train_cls = torch.sum(\n",
        "# #                 1 * (criterion_val(embedded_a, embedded_p,\n",
        "# #                                    embedded_n) > 0)) / train_batch_size  # CHANGED, MAY NEED TO REVERT BACK\n",
        "\n",
        "# #             loss_train += loss_train_cls.data\n",
        "\n",
        "#             if batch_idx % 30 == 0:\n",
        "#                 print(\"mini Batch Loss: {}\".format(loss.data))"
      ],
      "id": "239e50ac"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3dca38",
        "outputId": "9d644484-5456-4bf3-b3c6-956747a008ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== NETWORK CONSTRUCTED ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Network constructed\n",
        "'''\n",
        "print('================== NETWORK CONSTRUCTED ==================')"
      ],
      "id": "1c3dca38"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "5vdmS_syHlgh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5182db74-2b8e-4b57-9bb1-e2449a9abcb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Used to test the accuracy function'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "'''Used to test the accuracy function'''\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# pdist = nn.PairwiseDistance(p=2)\n",
        "# input1 = torch.randn(64, 1024)\n",
        "# input2 = torch.randn(64, 1024)\n",
        "# input3 = torch.randn(64, 1024)\n",
        "# print(input1.size())\n",
        "# print(input2.size())\n",
        "# print(input3.size())\n",
        "# dist1 = pdist(input1, input2)\n",
        "# dist2 = pdist(input1, input3)\n",
        "# print(dist1.size())\n",
        "# print(dist2.size())\n",
        "# pred = dist1 - dist2\n",
        "# print(pred.size())\n",
        "# sum = 0\n",
        "# for i in range(pred.size()[0]):\n",
        "#   if pred[i] < 0:\n",
        "#     sum+=1\n",
        "# print(sum/pred.size()[0])\n",
        "# print((pred < 0).sum()*1.0/pred.size()[0])"
      ],
      "id": "5vdmS_syHlgh"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "49lQY_gyLSqM"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# for i in range(10):\n",
        "#   a = random.randint(0,10)\n",
        "#   print(a)"
      ],
      "id": "49lQY_gyLSqM"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "M5uSm08ngNj7"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def accuracy(dista, distb):\n",
        "    margin = 0\n",
        "    pred = (dista - distb).cpu().data\n",
        "    return (pred < 0).sum()*1.0/dista.size()[0]\n",
        "\n",
        "def val_accuracy(trainednet, valloader, valloader_iter):\n",
        "  # sum_accuracy = 0\n",
        "  # num_batch_evaluated = 0\n",
        "  \n",
        "  # Pick one batches for evaluation\n",
        "  # selected_batch = random.randint(0, len(valloader))\n",
        "\n",
        "  # print('Batch selected for evaluation: ', selected_batch)\n",
        "  try:\n",
        "    data1, data2, data3 = next(valloader_iter)\n",
        "  except StopIteration:\n",
        "    valloader_iterator = iter(valloader)\n",
        "    data1, data2, data3 = next(valloader_iterator)\n",
        "\n",
        "  data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "  # wrap in torch.autograd.Variable\n",
        "  data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # compute output and loss\n",
        "    embedded_x, embedded_y, embedded_z = trainednet(data1, data2, data3)\n",
        "    dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "    dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "    print('dist a: {0}, dist b: {1}'.format(dist_a, dist_b))\n",
        "    batch_accuracy = accuracy(dist_a, dist_b)\n",
        "    print('random batch accuracy: {0} '.format(batch_accuracy))\n",
        "\n",
        "  # mean_accuracy = sum_accuracy / num_batch_evaluated\n",
        "  return batch_accuracy"
      ],
      "id": "M5uSm08ngNj7"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6bw80HHgiuxo"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# batch = next(iter(train_loader))\n",
        "# batch[0], batch[1], batch[2] = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()\n",
        "# embedded_x, embedded_y, embedded_z = net(batch[0],batch[1],batch[2])\n",
        "# dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "# dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "# accuracy = accuracy(dist_a, dist_b)\n",
        "# print(accuracy)"
      ],
      "id": "6bw80HHgiuxo"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "d1e25fe5"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, epochs, trainloader, valloader, testloader):\n",
        "\n",
        "  # Create an iterator object for valloader, for selecting a random batch from val set for validation\n",
        "  valloader_iterator = iter(valloader)\n",
        "\n",
        "  # Empty the cache of CUDA  \n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  print('================== START TRAINING ==================')\n",
        "  # Change to train mode\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "      running_loss = 0\n",
        "      for batch_idx, (data0, data1, data2) in enumerate(trainloader):\n",
        "          anchor, positive, negative = data0, data1, data2\n",
        "          anchor = Variable(anchor)\n",
        "          positive = Variable(positive)\n",
        "          negative = Variable(negative)\n",
        "          # print('anchor', anchor.size())\n",
        "          # print('positive', positive.size())\n",
        "          # print('negative', negative.size())\n",
        "          \n",
        "          # Calculate the output of three networks\n",
        "          embedded_a, embedded_p, embedded_n = model(anchor, positive, negative)\n",
        "          \n",
        "          # Calculate the loss\n",
        "          loss = criterion(embedded_a, embedded_p, embedded_n)\n",
        "          print(\"mini Batch {0} Loss: {1}\".format(batch_idx+1, loss.data))\n",
        "          \n",
        "          # Zero the gradient\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          # Back prop and update\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if batch_idx % 200 == 0 and batch_idx != 0:\n",
        "            print(\"Training Batch: {0} | Training Loss: {1}\".format(batch_idx+1, loss.data))\n",
        "\n",
        "            ''' For Validation'''\n",
        "            # Change to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            mean_accuracy = val_accuracy(model, valloader, valloader_iterator)\n",
        "            print(mean_accuracy)\n",
        "\n",
        "            # ''' For Prediction '''\n",
        "            # print('================== START PREDICTION ==================')\n",
        "\n",
        "            # redicted_labels = np.zeros(59544)\n",
        "            # pred_test=[]\n",
        "\n",
        "            # #Predict labels 1 or 0 for each test triplet\n",
        "            # for batch_idx_predict_in_epoch, (data1, data2, data3) in enumerate(testloader):\n",
        "\n",
        "            #     data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "            #     # wrap in torch.autograd.Variable\n",
        "            #     data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "            #     with torch.no_grad():\n",
        "            #         # compute output and loss\n",
        "            #         embedded_x, embedded_y, embedded_z = model(data1, data2, data3)\n",
        "\n",
        "            #     dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "            #     dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "            #     #print(np.squeeze(embedded_a.cpu().detach().numpy()).shape)\n",
        "                \n",
        "\n",
        "            #     pred_test.append(1*(dist_a <= dist_b))\n",
        "\n",
        "            #     print('batch id predict in epoch: ', batch_idx_predict_in_epoch)\n",
        "\n",
        "            # pred_test_np = []\n",
        "            # for i in range(len(pred_test)):\n",
        "            #   pred_test_cpu = pred_test[i].cpu().detach().numpy()\n",
        "            #   pred_test_np += list(pred_test_cpu)\n",
        "            # print(len(pred_test_np))\n",
        "            # predicted_labels = np.hstack(pred_test_np)\n",
        "            # print(predicted_labels)\n",
        "\n",
        "            # # Write submisison file, should be saved to the current training batch ID instead of the prediction ID \n",
        "            # df = pd.DataFrame(predicted_labels)\n",
        "            # df.to_csv('/content/drive/MyDrive/test6/submission_epoch{0}_batch{1}.txt'.format(epoch+1, batch_idx+1), index=False, header=None) #write CSV\n",
        "\n",
        "            save_path = f'/content/drive/My Drive/test6/model_epoch_{epoch+1}_batch_{batch_idx+1}.pt'\n",
        "            torch.save({'Batch id in epoch': batch_idx+1, 'model_state_dict': model.state_dict()}, save_path)\n",
        "            print(\"Training Batch: {0} | Model saved to: {1}\".format(batch_idx+1, save_path))\n",
        "\n",
        "            # Change back to train mode\n",
        "            model.train()\n",
        "\n",
        "            # Empty the cache of CUDA  \n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "          \n",
        "      print(f'[{epoch + 1}] average loss per epoch: {running_loss / len(train_loader):.3f}')\n",
        "      # # save checkpoint of model\n",
        "      # if epoch % 5 == 0 and epoch > 0:\n",
        "\n",
        "      save_path = f'/content/drive/My Drive/test6/model_epoch{epoch+1}.pt'\n",
        "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict()}, save_path)\n",
        "      print(f'Saved model checkpoint to {save_path}')\n",
        "\n",
        "      ''' For Validation'''\n",
        "      # Change to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      mean_accuracy = val_accuracy(model, valloader, valloader_iterator)\n",
        "      print(mean_accuracy)\n",
        "\n",
        "      ''' For Prediction '''\n",
        "      print('================== START PREDICTION ==================')\n",
        "\n",
        "      redicted_labels = np.zeros(59544)\n",
        "      pred_test=[]\n",
        "\n",
        "      #Predict labels 1 or 0 for each test triplet\n",
        "      for batch_idx_predict_after_epoch, (data1, data2, data3) in enumerate(testloader):\n",
        "\n",
        "          data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "          # wrap in torch.autograd.Variable\n",
        "          data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              # compute output and loss\n",
        "              embedded_x, embedded_y, embedded_z = model(data1, data2, data3)\n",
        "\n",
        "          dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "          dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "          #print(np.squeeze(embedded_a.cpu().detach().numpy()).shape)\n",
        "          \n",
        "\n",
        "          pred_test.append(1*(dist_a <= dist_b))\n",
        "\n",
        "          print('batch id predict after epoch: ', batch_idx_predict_after_epoch+1)\n",
        "\n",
        "      pred_test_np = []\n",
        "      for i in range(len(pred_test)):\n",
        "        pred_test_cpu = pred_test[i].cpu().detach().numpy()\n",
        "        pred_test_np += list(pred_test_cpu)\n",
        "      len(pred_test_np)\n",
        "      predicted_labels = np.hstack(pred_test_np)\n",
        "      print(predicted_labels)\n",
        "\n",
        "      #Write submisison file\n",
        "      df = pd.DataFrame(predicted_labels)\n",
        "      df.to_csv('/content/drive/MyDrive/test6/submission_epoch{0}.txt'.format(epoch+1), index=False, header=None) #write CSV\n",
        "\n",
        "      # Change back to train mode\n",
        "      model.train()\n",
        "\n",
        "      # Empty the cache of CUDA  \n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  \n",
        "  print('Finished Training')\n",
        "  return model"
      ],
      "id": "d1e25fe5"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7f8cdf",
        "outputId": "3c16a668-b5c2-4557-953d-57ee61349f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "mini Batch 800 Loss: 3.646195650100708\n",
            "mini Batch 801 Loss: 3.163114070892334\n",
            "Training Batch: 801 | Training Loss: 3.163114070892334\n",
            "dist a: tensor([ 9.7996,  9.8889, 15.0378, 11.0743, 12.4819,  7.8157, 10.4168, 11.6447,\n",
            "         9.1114, 15.1727, 10.2421, 13.9735, 14.8031, 13.5268, 16.2529, 14.7657,\n",
            "        20.0343, 14.2923, 11.8492, 17.1070, 18.7477, 12.1506,  9.0547,  9.7904,\n",
            "        17.5547,  6.4881, 11.2657, 10.5141, 14.1530, 10.4314, 14.6249, 12.0653,\n",
            "        16.0641, 11.5427, 14.2233, 10.7387, 14.1161, 13.4734, 16.9712,  8.5366,\n",
            "        11.6482, 14.1830,  9.7802, 13.5853, 16.1300,  8.9199, 13.6755, 12.4957,\n",
            "         9.5469, 16.9877, 17.2061, 19.0218, 13.4048,  6.7925,  9.7478, 11.8144,\n",
            "        10.5264,  9.2235, 11.3929,  9.5206, 11.3003, 14.8563, 11.5138, 17.3337],\n",
            "       device='cuda:0'), dist b: tensor([13.0556, 14.8479, 17.7426, 16.4674, 21.0731, 15.3461, 16.6558,  9.7069,\n",
            "        14.8913, 11.4613, 17.0815, 12.1827, 13.7135, 16.1093, 12.6347, 10.2526,\n",
            "        14.2834, 15.4460, 13.6728, 14.9033, 20.6991, 10.8444, 11.7336, 13.2936,\n",
            "        23.0912, 15.3438, 21.3243, 15.4686, 11.0958, 14.4052, 18.0460, 15.6685,\n",
            "        22.6489, 14.6687, 12.5957, 12.0630, 10.4728, 13.3621, 15.0206, 15.1729,\n",
            "        14.9806, 16.0041, 14.7485, 13.0429, 15.0494, 11.9499, 13.4337, 14.0714,\n",
            "        18.7752, 16.6215, 12.6523, 11.4645, 14.3178, 12.8506,  7.6152, 13.4237,\n",
            "        16.0538,  8.9055, 12.1861, 13.0115, 20.6989, 12.4954, 19.1715, 16.0272],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.625 \n",
            "tensor(0.6250)\n",
            "Training Batch: 801 | Model saved to: /content/drive/My Drive/test6/model_epoch_1_batch_801.pt\n",
            "mini Batch 802 Loss: 3.0728399753570557\n",
            "mini Batch 803 Loss: 2.8754477500915527\n",
            "mini Batch 804 Loss: 3.6196300983428955\n",
            "mini Batch 805 Loss: 3.4472789764404297\n",
            "mini Batch 806 Loss: 3.1236844062805176\n",
            "mini Batch 807 Loss: 2.9993114471435547\n",
            "mini Batch 808 Loss: 3.3448123931884766\n",
            "mini Batch 809 Loss: 3.148911476135254\n",
            "mini Batch 810 Loss: 3.3575096130371094\n",
            "mini Batch 811 Loss: 2.4652810096740723\n",
            "mini Batch 812 Loss: 2.2706542015075684\n",
            "mini Batch 813 Loss: 2.9459362030029297\n",
            "mini Batch 814 Loss: 2.395205020904541\n",
            "mini Batch 815 Loss: 2.873481273651123\n",
            "mini Batch 816 Loss: 2.4107718467712402\n",
            "mini Batch 817 Loss: 2.745420217514038\n",
            "mini Batch 818 Loss: 1.908172845840454\n",
            "mini Batch 819 Loss: 2.8782734870910645\n",
            "mini Batch 820 Loss: 2.4749412536621094\n",
            "mini Batch 821 Loss: 3.744710683822632\n",
            "mini Batch 822 Loss: 2.78279185295105\n",
            "mini Batch 823 Loss: 1.9969267845153809\n",
            "mini Batch 824 Loss: 3.0016419887542725\n",
            "mini Batch 825 Loss: 2.2666497230529785\n",
            "mini Batch 826 Loss: 1.7092070579528809\n",
            "mini Batch 827 Loss: 2.8313183784484863\n",
            "mini Batch 828 Loss: 1.8612465858459473\n",
            "mini Batch 829 Loss: 1.4299509525299072\n",
            "mini Batch 830 Loss: 3.2047762870788574\n",
            "mini Batch 831 Loss: 2.5185160636901855\n",
            "mini Batch 832 Loss: 1.7205440998077393\n",
            "mini Batch 833 Loss: 1.4350777864456177\n",
            "mini Batch 834 Loss: 1.3810282945632935\n",
            "mini Batch 835 Loss: 4.074252128601074\n",
            "mini Batch 836 Loss: 3.178859233856201\n",
            "mini Batch 837 Loss: 2.3181843757629395\n",
            "mini Batch 838 Loss: 3.792767286300659\n",
            "mini Batch 839 Loss: 3.1614511013031006\n",
            "mini Batch 840 Loss: 3.533461093902588\n",
            "mini Batch 841 Loss: 3.090363025665283\n",
            "mini Batch 842 Loss: 1.9071681499481201\n",
            "mini Batch 843 Loss: 2.9169039726257324\n",
            "mini Batch 844 Loss: 2.323045015335083\n",
            "mini Batch 845 Loss: 3.7920007705688477\n",
            "mini Batch 846 Loss: 3.7104978561401367\n",
            "mini Batch 847 Loss: 3.545437812805176\n",
            "mini Batch 848 Loss: 3.772369146347046\n",
            "mini Batch 849 Loss: 2.7084953784942627\n",
            "mini Batch 850 Loss: 2.135862112045288\n",
            "mini Batch 851 Loss: 3.0199179649353027\n",
            "mini Batch 852 Loss: 4.528891563415527\n",
            "mini Batch 853 Loss: 2.985630512237549\n",
            "mini Batch 854 Loss: 3.2527709007263184\n",
            "mini Batch 855 Loss: 3.8214898109436035\n",
            "mini Batch 856 Loss: 2.6695635318756104\n",
            "mini Batch 857 Loss: 3.788583517074585\n",
            "mini Batch 858 Loss: 2.5780601501464844\n",
            "mini Batch 859 Loss: 3.392798662185669\n",
            "mini Batch 860 Loss: 2.6407241821289062\n",
            "mini Batch 861 Loss: 3.7819626331329346\n",
            "mini Batch 862 Loss: 3.705471992492676\n",
            "mini Batch 863 Loss: 2.217456340789795\n",
            "mini Batch 864 Loss: 2.2433881759643555\n",
            "mini Batch 865 Loss: 3.833667516708374\n",
            "mini Batch 866 Loss: 2.7391459941864014\n",
            "mini Batch 867 Loss: 3.2661983966827393\n",
            "mini Batch 868 Loss: 3.781505584716797\n",
            "mini Batch 869 Loss: 1.2655359506607056\n",
            "mini Batch 870 Loss: 3.146192789077759\n",
            "mini Batch 871 Loss: 3.1552789211273193\n",
            "mini Batch 872 Loss: 1.7705708742141724\n",
            "mini Batch 873 Loss: 2.537052869796753\n",
            "mini Batch 874 Loss: 2.833014965057373\n",
            "mini Batch 875 Loss: 2.145338535308838\n",
            "mini Batch 876 Loss: 2.3039050102233887\n",
            "mini Batch 877 Loss: 2.4684438705444336\n",
            "mini Batch 878 Loss: 3.1348483562469482\n",
            "mini Batch 879 Loss: 2.9840879440307617\n",
            "mini Batch 880 Loss: 2.8107314109802246\n",
            "mini Batch 881 Loss: 1.7360620498657227\n",
            "mini Batch 882 Loss: 2.573622703552246\n",
            "mini Batch 883 Loss: 2.4555299282073975\n",
            "mini Batch 884 Loss: 3.330350875854492\n",
            "mini Batch 885 Loss: 3.4550223350524902\n",
            "mini Batch 886 Loss: 1.7412631511688232\n",
            "mini Batch 887 Loss: 4.383630275726318\n",
            "mini Batch 888 Loss: 3.393921136856079\n",
            "mini Batch 889 Loss: 2.1601924896240234\n",
            "mini Batch 890 Loss: 2.7179436683654785\n",
            "mini Batch 891 Loss: 2.49922513961792\n",
            "mini Batch 892 Loss: 2.6315698623657227\n",
            "mini Batch 893 Loss: 3.9449589252471924\n",
            "mini Batch 894 Loss: 2.270413875579834\n",
            "mini Batch 895 Loss: 2.7529959678649902\n",
            "mini Batch 896 Loss: 3.15695858001709\n",
            "mini Batch 897 Loss: 3.256542205810547\n",
            "mini Batch 898 Loss: 4.827108860015869\n",
            "mini Batch 899 Loss: 3.0521702766418457\n",
            "mini Batch 900 Loss: 2.944730758666992\n",
            "mini Batch 901 Loss: 3.9699134826660156\n",
            "mini Batch 902 Loss: 2.4744012355804443\n",
            "mini Batch 903 Loss: 2.0359995365142822\n",
            "mini Batch 904 Loss: 2.0669960975646973\n",
            "mini Batch 905 Loss: 2.2845914363861084\n",
            "mini Batch 906 Loss: 3.0812184810638428\n",
            "mini Batch 907 Loss: 2.264885902404785\n",
            "mini Batch 908 Loss: 2.801731586456299\n",
            "mini Batch 909 Loss: 1.6250762939453125\n",
            "mini Batch 910 Loss: 2.1387767791748047\n",
            "mini Batch 911 Loss: 2.676509380340576\n",
            "mini Batch 912 Loss: 2.5880684852600098\n",
            "mini Batch 913 Loss: 3.276193857192993\n",
            "mini Batch 914 Loss: 4.627401828765869\n",
            "mini Batch 915 Loss: 2.8568859100341797\n",
            "mini Batch 916 Loss: 3.348137617111206\n",
            "mini Batch 917 Loss: 2.702451467514038\n",
            "mini Batch 918 Loss: 2.7316150665283203\n",
            "mini Batch 919 Loss: 3.049928665161133\n",
            "mini Batch 920 Loss: 2.8921425342559814\n",
            "mini Batch 921 Loss: 3.4388680458068848\n",
            "[1] average loss per epoch: 3.234\n",
            "Saved model checkpoint to /content/drive/My Drive/test6/model_epoch1.pt\n",
            "dist a: tensor([17.8983, 11.8586, 21.0348, 16.3215, 14.8423, 13.5614, 17.0931, 23.0327,\n",
            "        20.9313, 13.0014, 18.5536, 18.0321, 13.7442, 15.4569, 16.4324, 24.2211,\n",
            "        12.9771, 12.2253, 17.7645, 20.1563, 22.4242, 20.4803, 16.6704, 13.3188,\n",
            "        15.9143, 15.2850,  9.1758, 15.4178, 15.9471, 21.3437, 20.7330, 17.3990,\n",
            "        17.7327, 18.4470, 20.7716,  7.5203, 20.7652, 15.1611, 20.0527, 11.1864,\n",
            "        22.7343,  7.9101, 12.8316, 16.0983, 18.9366,  9.5953, 14.6487, 11.8795,\n",
            "        20.6465, 10.8754, 10.7189, 16.9049, 16.8420, 16.6709, 14.3360, 24.3534,\n",
            "        15.6523, 19.3252, 19.3252, 12.4582, 12.7056, 14.6541, 23.9648, 11.5934],\n",
            "       device='cuda:0'), dist b: tensor([28.5348, 18.0582, 19.2688, 17.8040, 18.6415, 23.6625, 18.0840, 21.7187,\n",
            "        23.7068, 20.0528, 24.2944, 15.6674, 21.8611, 11.5709, 26.9988, 22.6165,\n",
            "        25.1244, 12.4716, 14.2293, 20.0163, 28.3759, 16.3472, 21.9294, 22.9328,\n",
            "        20.9140, 25.8273, 18.1795, 25.1416, 18.4442, 18.7450, 19.4087, 22.0711,\n",
            "        19.6121, 18.1471, 19.1284, 25.3530, 24.5040, 12.6670, 23.3169, 24.0067,\n",
            "        15.1182, 25.8165, 28.0942, 19.0349, 20.0578, 23.3229, 23.4304, 15.4445,\n",
            "        21.0725, 21.8789, 20.1315, 22.2061, 22.1184, 23.6408, 22.0001, 22.2969,\n",
            "        24.8903, 20.9548, 19.5606, 17.5811, 17.2180, 11.9788, 23.2407, 22.3885],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "================== START PREDICTION ==================\n",
            "batch id predict after epoch:  1\n",
            "batch id predict after epoch:  2\n",
            "batch id predict after epoch:  3\n",
            "batch id predict after epoch:  4\n",
            "batch id predict after epoch:  5\n",
            "batch id predict after epoch:  6\n",
            "batch id predict after epoch:  7\n",
            "batch id predict after epoch:  8\n",
            "batch id predict after epoch:  9\n",
            "batch id predict after epoch:  10\n",
            "batch id predict after epoch:  11\n",
            "batch id predict after epoch:  12\n",
            "batch id predict after epoch:  13\n",
            "batch id predict after epoch:  14\n",
            "batch id predict after epoch:  15\n",
            "batch id predict after epoch:  16\n",
            "batch id predict after epoch:  17\n",
            "batch id predict after epoch:  18\n",
            "batch id predict after epoch:  19\n",
            "batch id predict after epoch:  20\n",
            "batch id predict after epoch:  21\n",
            "batch id predict after epoch:  22\n",
            "batch id predict after epoch:  23\n",
            "batch id predict after epoch:  24\n",
            "batch id predict after epoch:  25\n",
            "batch id predict after epoch:  26\n",
            "batch id predict after epoch:  27\n",
            "batch id predict after epoch:  28\n",
            "batch id predict after epoch:  29\n",
            "batch id predict after epoch:  30\n",
            "batch id predict after epoch:  31\n",
            "batch id predict after epoch:  32\n",
            "batch id predict after epoch:  33\n",
            "batch id predict after epoch:  34\n",
            "batch id predict after epoch:  35\n",
            "batch id predict after epoch:  36\n",
            "batch id predict after epoch:  37\n",
            "batch id predict after epoch:  38\n",
            "batch id predict after epoch:  39\n",
            "batch id predict after epoch:  40\n",
            "batch id predict after epoch:  41\n",
            "batch id predict after epoch:  42\n",
            "batch id predict after epoch:  43\n",
            "batch id predict after epoch:  44\n",
            "batch id predict after epoch:  45\n",
            "batch id predict after epoch:  46\n",
            "batch id predict after epoch:  47\n",
            "batch id predict after epoch:  48\n",
            "batch id predict after epoch:  49\n",
            "batch id predict after epoch:  50\n",
            "batch id predict after epoch:  51\n",
            "batch id predict after epoch:  52\n",
            "batch id predict after epoch:  53\n",
            "batch id predict after epoch:  54\n",
            "batch id predict after epoch:  55\n",
            "batch id predict after epoch:  56\n",
            "batch id predict after epoch:  57\n",
            "batch id predict after epoch:  58\n",
            "batch id predict after epoch:  59\n",
            "batch id predict after epoch:  60\n",
            "batch id predict after epoch:  61\n",
            "batch id predict after epoch:  62\n",
            "batch id predict after epoch:  63\n",
            "batch id predict after epoch:  64\n",
            "batch id predict after epoch:  65\n",
            "batch id predict after epoch:  66\n",
            "batch id predict after epoch:  67\n",
            "batch id predict after epoch:  68\n",
            "batch id predict after epoch:  69\n",
            "batch id predict after epoch:  70\n",
            "batch id predict after epoch:  71\n",
            "batch id predict after epoch:  72\n",
            "batch id predict after epoch:  73\n",
            "batch id predict after epoch:  74\n",
            "batch id predict after epoch:  75\n",
            "batch id predict after epoch:  76\n",
            "batch id predict after epoch:  77\n",
            "batch id predict after epoch:  78\n",
            "batch id predict after epoch:  79\n",
            "batch id predict after epoch:  80\n",
            "batch id predict after epoch:  81\n",
            "batch id predict after epoch:  82\n",
            "batch id predict after epoch:  83\n",
            "batch id predict after epoch:  84\n",
            "batch id predict after epoch:  85\n",
            "batch id predict after epoch:  86\n",
            "batch id predict after epoch:  87\n",
            "batch id predict after epoch:  88\n",
            "batch id predict after epoch:  89\n",
            "batch id predict after epoch:  90\n",
            "batch id predict after epoch:  91\n",
            "batch id predict after epoch:  92\n",
            "batch id predict after epoch:  93\n",
            "batch id predict after epoch:  94\n",
            "batch id predict after epoch:  95\n",
            "batch id predict after epoch:  96\n",
            "batch id predict after epoch:  97\n",
            "batch id predict after epoch:  98\n",
            "batch id predict after epoch:  99\n",
            "batch id predict after epoch:  100\n",
            "batch id predict after epoch:  101\n",
            "batch id predict after epoch:  102\n",
            "batch id predict after epoch:  103\n",
            "batch id predict after epoch:  104\n",
            "batch id predict after epoch:  105\n",
            "batch id predict after epoch:  106\n",
            "batch id predict after epoch:  107\n",
            "batch id predict after epoch:  108\n",
            "batch id predict after epoch:  109\n",
            "batch id predict after epoch:  110\n",
            "batch id predict after epoch:  111\n",
            "batch id predict after epoch:  112\n",
            "batch id predict after epoch:  113\n",
            "batch id predict after epoch:  114\n",
            "batch id predict after epoch:  115\n",
            "batch id predict after epoch:  116\n",
            "batch id predict after epoch:  117\n",
            "batch id predict after epoch:  118\n",
            "batch id predict after epoch:  119\n",
            "batch id predict after epoch:  120\n",
            "batch id predict after epoch:  121\n",
            "batch id predict after epoch:  122\n",
            "batch id predict after epoch:  123\n",
            "batch id predict after epoch:  124\n",
            "batch id predict after epoch:  125\n",
            "batch id predict after epoch:  126\n",
            "batch id predict after epoch:  127\n",
            "batch id predict after epoch:  128\n",
            "batch id predict after epoch:  129\n",
            "batch id predict after epoch:  130\n",
            "batch id predict after epoch:  131\n",
            "batch id predict after epoch:  132\n",
            "batch id predict after epoch:  133\n",
            "batch id predict after epoch:  134\n",
            "batch id predict after epoch:  135\n",
            "batch id predict after epoch:  136\n",
            "batch id predict after epoch:  137\n",
            "batch id predict after epoch:  138\n",
            "batch id predict after epoch:  139\n",
            "batch id predict after epoch:  140\n",
            "batch id predict after epoch:  141\n",
            "batch id predict after epoch:  142\n",
            "batch id predict after epoch:  143\n",
            "batch id predict after epoch:  144\n",
            "batch id predict after epoch:  145\n",
            "batch id predict after epoch:  146\n",
            "batch id predict after epoch:  147\n",
            "batch id predict after epoch:  148\n",
            "batch id predict after epoch:  149\n",
            "batch id predict after epoch:  150\n",
            "batch id predict after epoch:  151\n",
            "batch id predict after epoch:  152\n",
            "batch id predict after epoch:  153\n",
            "batch id predict after epoch:  154\n",
            "batch id predict after epoch:  155\n",
            "batch id predict after epoch:  156\n",
            "batch id predict after epoch:  157\n",
            "batch id predict after epoch:  158\n",
            "batch id predict after epoch:  159\n",
            "batch id predict after epoch:  160\n",
            "batch id predict after epoch:  161\n",
            "batch id predict after epoch:  162\n",
            "batch id predict after epoch:  163\n",
            "batch id predict after epoch:  164\n",
            "batch id predict after epoch:  165\n",
            "batch id predict after epoch:  166\n",
            "batch id predict after epoch:  167\n",
            "batch id predict after epoch:  168\n",
            "batch id predict after epoch:  169\n",
            "batch id predict after epoch:  170\n",
            "batch id predict after epoch:  171\n",
            "batch id predict after epoch:  172\n",
            "batch id predict after epoch:  173\n",
            "batch id predict after epoch:  174\n",
            "batch id predict after epoch:  175\n",
            "batch id predict after epoch:  176\n",
            "batch id predict after epoch:  177\n",
            "batch id predict after epoch:  178\n",
            "batch id predict after epoch:  179\n",
            "batch id predict after epoch:  180\n",
            "batch id predict after epoch:  181\n",
            "batch id predict after epoch:  182\n",
            "batch id predict after epoch:  183\n",
            "batch id predict after epoch:  184\n",
            "batch id predict after epoch:  185\n",
            "batch id predict after epoch:  186\n",
            "batch id predict after epoch:  187\n",
            "batch id predict after epoch:  188\n",
            "batch id predict after epoch:  189\n",
            "batch id predict after epoch:  190\n",
            "batch id predict after epoch:  191\n",
            "batch id predict after epoch:  192\n",
            "batch id predict after epoch:  193\n",
            "batch id predict after epoch:  194\n",
            "batch id predict after epoch:  195\n",
            "batch id predict after epoch:  196\n",
            "batch id predict after epoch:  197\n",
            "batch id predict after epoch:  198\n",
            "batch id predict after epoch:  199\n",
            "batch id predict after epoch:  200\n",
            "batch id predict after epoch:  201\n",
            "batch id predict after epoch:  202\n",
            "batch id predict after epoch:  203\n",
            "batch id predict after epoch:  204\n",
            "batch id predict after epoch:  205\n",
            "batch id predict after epoch:  206\n",
            "batch id predict after epoch:  207\n",
            "batch id predict after epoch:  208\n",
            "batch id predict after epoch:  209\n",
            "batch id predict after epoch:  210\n",
            "batch id predict after epoch:  211\n",
            "batch id predict after epoch:  212\n",
            "batch id predict after epoch:  213\n",
            "batch id predict after epoch:  214\n",
            "batch id predict after epoch:  215\n",
            "batch id predict after epoch:  216\n",
            "batch id predict after epoch:  217\n",
            "batch id predict after epoch:  218\n",
            "batch id predict after epoch:  219\n",
            "batch id predict after epoch:  220\n",
            "batch id predict after epoch:  221\n",
            "batch id predict after epoch:  222\n",
            "batch id predict after epoch:  223\n",
            "batch id predict after epoch:  224\n",
            "batch id predict after epoch:  225\n",
            "batch id predict after epoch:  226\n",
            "batch id predict after epoch:  227\n",
            "batch id predict after epoch:  228\n",
            "batch id predict after epoch:  229\n",
            "batch id predict after epoch:  230\n",
            "batch id predict after epoch:  231\n",
            "batch id predict after epoch:  232\n",
            "batch id predict after epoch:  233\n",
            "batch id predict after epoch:  234\n",
            "batch id predict after epoch:  235\n",
            "batch id predict after epoch:  236\n",
            "batch id predict after epoch:  237\n",
            "batch id predict after epoch:  238\n",
            "batch id predict after epoch:  239\n",
            "batch id predict after epoch:  240\n",
            "batch id predict after epoch:  241\n",
            "batch id predict after epoch:  242\n",
            "batch id predict after epoch:  243\n",
            "batch id predict after epoch:  244\n",
            "batch id predict after epoch:  245\n",
            "batch id predict after epoch:  246\n",
            "batch id predict after epoch:  247\n",
            "batch id predict after epoch:  248\n",
            "batch id predict after epoch:  249\n",
            "batch id predict after epoch:  250\n",
            "batch id predict after epoch:  251\n",
            "batch id predict after epoch:  252\n",
            "batch id predict after epoch:  253\n",
            "batch id predict after epoch:  254\n",
            "batch id predict after epoch:  255\n",
            "batch id predict after epoch:  256\n",
            "batch id predict after epoch:  257\n",
            "batch id predict after epoch:  258\n",
            "batch id predict after epoch:  259\n",
            "batch id predict after epoch:  260\n",
            "batch id predict after epoch:  261\n",
            "batch id predict after epoch:  262\n",
            "batch id predict after epoch:  263\n",
            "batch id predict after epoch:  264\n",
            "batch id predict after epoch:  265\n",
            "batch id predict after epoch:  266\n",
            "batch id predict after epoch:  267\n",
            "batch id predict after epoch:  268\n",
            "batch id predict after epoch:  269\n",
            "batch id predict after epoch:  270\n",
            "batch id predict after epoch:  271\n",
            "batch id predict after epoch:  272\n",
            "batch id predict after epoch:  273\n",
            "batch id predict after epoch:  274\n",
            "batch id predict after epoch:  275\n",
            "batch id predict after epoch:  276\n",
            "batch id predict after epoch:  277\n",
            "batch id predict after epoch:  278\n",
            "batch id predict after epoch:  279\n",
            "batch id predict after epoch:  280\n",
            "batch id predict after epoch:  281\n",
            "batch id predict after epoch:  282\n",
            "batch id predict after epoch:  283\n",
            "batch id predict after epoch:  284\n",
            "batch id predict after epoch:  285\n",
            "batch id predict after epoch:  286\n",
            "batch id predict after epoch:  287\n",
            "batch id predict after epoch:  288\n",
            "batch id predict after epoch:  289\n",
            "batch id predict after epoch:  290\n",
            "batch id predict after epoch:  291\n",
            "batch id predict after epoch:  292\n",
            "batch id predict after epoch:  293\n",
            "batch id predict after epoch:  294\n",
            "batch id predict after epoch:  295\n",
            "batch id predict after epoch:  296\n",
            "batch id predict after epoch:  297\n",
            "batch id predict after epoch:  298\n",
            "batch id predict after epoch:  299\n",
            "batch id predict after epoch:  300\n",
            "batch id predict after epoch:  301\n",
            "batch id predict after epoch:  302\n",
            "batch id predict after epoch:  303\n",
            "batch id predict after epoch:  304\n",
            "batch id predict after epoch:  305\n",
            "batch id predict after epoch:  306\n",
            "batch id predict after epoch:  307\n",
            "batch id predict after epoch:  308\n",
            "batch id predict after epoch:  309\n",
            "batch id predict after epoch:  310\n",
            "batch id predict after epoch:  311\n",
            "batch id predict after epoch:  312\n",
            "batch id predict after epoch:  313\n",
            "batch id predict after epoch:  314\n",
            "batch id predict after epoch:  315\n",
            "batch id predict after epoch:  316\n",
            "batch id predict after epoch:  317\n",
            "batch id predict after epoch:  318\n",
            "batch id predict after epoch:  319\n",
            "batch id predict after epoch:  320\n",
            "batch id predict after epoch:  321\n",
            "batch id predict after epoch:  322\n",
            "batch id predict after epoch:  323\n",
            "batch id predict after epoch:  324\n",
            "batch id predict after epoch:  325\n",
            "batch id predict after epoch:  326\n",
            "batch id predict after epoch:  327\n",
            "batch id predict after epoch:  328\n",
            "batch id predict after epoch:  329\n",
            "batch id predict after epoch:  330\n",
            "batch id predict after epoch:  331\n",
            "batch id predict after epoch:  332\n",
            "batch id predict after epoch:  333\n",
            "batch id predict after epoch:  334\n",
            "batch id predict after epoch:  335\n",
            "batch id predict after epoch:  336\n",
            "batch id predict after epoch:  337\n",
            "batch id predict after epoch:  338\n",
            "batch id predict after epoch:  339\n",
            "batch id predict after epoch:  340\n",
            "batch id predict after epoch:  341\n",
            "batch id predict after epoch:  342\n",
            "batch id predict after epoch:  343\n",
            "batch id predict after epoch:  344\n",
            "batch id predict after epoch:  345\n",
            "batch id predict after epoch:  346\n",
            "batch id predict after epoch:  347\n",
            "batch id predict after epoch:  348\n",
            "batch id predict after epoch:  349\n",
            "batch id predict after epoch:  350\n",
            "batch id predict after epoch:  351\n",
            "batch id predict after epoch:  352\n",
            "batch id predict after epoch:  353\n",
            "batch id predict after epoch:  354\n",
            "batch id predict after epoch:  355\n",
            "batch id predict after epoch:  356\n",
            "batch id predict after epoch:  357\n",
            "batch id predict after epoch:  358\n",
            "batch id predict after epoch:  359\n",
            "batch id predict after epoch:  360\n",
            "batch id predict after epoch:  361\n",
            "batch id predict after epoch:  362\n",
            "batch id predict after epoch:  363\n",
            "batch id predict after epoch:  364\n",
            "batch id predict after epoch:  365\n",
            "batch id predict after epoch:  366\n",
            "batch id predict after epoch:  367\n",
            "batch id predict after epoch:  368\n",
            "batch id predict after epoch:  369\n",
            "batch id predict after epoch:  370\n",
            "batch id predict after epoch:  371\n",
            "batch id predict after epoch:  372\n",
            "batch id predict after epoch:  373\n",
            "batch id predict after epoch:  374\n",
            "batch id predict after epoch:  375\n",
            "batch id predict after epoch:  376\n",
            "batch id predict after epoch:  377\n",
            "batch id predict after epoch:  378\n",
            "batch id predict after epoch:  379\n",
            "batch id predict after epoch:  380\n",
            "batch id predict after epoch:  381\n",
            "batch id predict after epoch:  382\n",
            "batch id predict after epoch:  383\n",
            "batch id predict after epoch:  384\n",
            "batch id predict after epoch:  385\n",
            "batch id predict after epoch:  386\n",
            "batch id predict after epoch:  387\n",
            "batch id predict after epoch:  388\n",
            "batch id predict after epoch:  389\n",
            "batch id predict after epoch:  390\n",
            "batch id predict after epoch:  391\n",
            "batch id predict after epoch:  392\n",
            "batch id predict after epoch:  393\n",
            "batch id predict after epoch:  394\n",
            "batch id predict after epoch:  395\n",
            "batch id predict after epoch:  396\n",
            "batch id predict after epoch:  397\n",
            "batch id predict after epoch:  398\n",
            "batch id predict after epoch:  399\n",
            "batch id predict after epoch:  400\n",
            "batch id predict after epoch:  401\n",
            "batch id predict after epoch:  402\n",
            "batch id predict after epoch:  403\n",
            "batch id predict after epoch:  404\n",
            "batch id predict after epoch:  405\n",
            "batch id predict after epoch:  406\n",
            "batch id predict after epoch:  407\n",
            "batch id predict after epoch:  408\n",
            "batch id predict after epoch:  409\n",
            "batch id predict after epoch:  410\n",
            "batch id predict after epoch:  411\n",
            "batch id predict after epoch:  412\n",
            "batch id predict after epoch:  413\n",
            "batch id predict after epoch:  414\n",
            "batch id predict after epoch:  415\n",
            "batch id predict after epoch:  416\n",
            "batch id predict after epoch:  417\n",
            "batch id predict after epoch:  418\n",
            "batch id predict after epoch:  419\n",
            "batch id predict after epoch:  420\n",
            "batch id predict after epoch:  421\n",
            "batch id predict after epoch:  422\n",
            "batch id predict after epoch:  423\n",
            "batch id predict after epoch:  424\n",
            "batch id predict after epoch:  425\n",
            "batch id predict after epoch:  426\n",
            "batch id predict after epoch:  427\n",
            "batch id predict after epoch:  428\n",
            "batch id predict after epoch:  429\n",
            "batch id predict after epoch:  430\n",
            "batch id predict after epoch:  431\n",
            "batch id predict after epoch:  432\n",
            "batch id predict after epoch:  433\n",
            "batch id predict after epoch:  434\n",
            "batch id predict after epoch:  435\n",
            "batch id predict after epoch:  436\n",
            "batch id predict after epoch:  437\n",
            "batch id predict after epoch:  438\n",
            "batch id predict after epoch:  439\n",
            "batch id predict after epoch:  440\n",
            "batch id predict after epoch:  441\n",
            "batch id predict after epoch:  442\n",
            "batch id predict after epoch:  443\n",
            "batch id predict after epoch:  444\n",
            "batch id predict after epoch:  445\n",
            "batch id predict after epoch:  446\n",
            "batch id predict after epoch:  447\n",
            "batch id predict after epoch:  448\n",
            "batch id predict after epoch:  449\n",
            "batch id predict after epoch:  450\n",
            "batch id predict after epoch:  451\n",
            "batch id predict after epoch:  452\n",
            "batch id predict after epoch:  453\n",
            "batch id predict after epoch:  454\n",
            "batch id predict after epoch:  455\n",
            "batch id predict after epoch:  456\n",
            "batch id predict after epoch:  457\n",
            "batch id predict after epoch:  458\n",
            "batch id predict after epoch:  459\n",
            "batch id predict after epoch:  460\n",
            "batch id predict after epoch:  461\n",
            "batch id predict after epoch:  462\n",
            "batch id predict after epoch:  463\n",
            "batch id predict after epoch:  464\n",
            "batch id predict after epoch:  465\n",
            "batch id predict after epoch:  466\n",
            "batch id predict after epoch:  467\n",
            "batch id predict after epoch:  468\n",
            "batch id predict after epoch:  469\n",
            "batch id predict after epoch:  470\n",
            "batch id predict after epoch:  471\n",
            "batch id predict after epoch:  472\n",
            "batch id predict after epoch:  473\n",
            "batch id predict after epoch:  474\n",
            "batch id predict after epoch:  475\n",
            "batch id predict after epoch:  476\n",
            "batch id predict after epoch:  477\n",
            "batch id predict after epoch:  478\n",
            "batch id predict after epoch:  479\n",
            "batch id predict after epoch:  480\n",
            "batch id predict after epoch:  481\n",
            "batch id predict after epoch:  482\n",
            "batch id predict after epoch:  483\n",
            "batch id predict after epoch:  484\n",
            "batch id predict after epoch:  485\n",
            "batch id predict after epoch:  486\n",
            "batch id predict after epoch:  487\n",
            "batch id predict after epoch:  488\n",
            "batch id predict after epoch:  489\n",
            "batch id predict after epoch:  490\n",
            "batch id predict after epoch:  491\n",
            "batch id predict after epoch:  492\n",
            "batch id predict after epoch:  493\n",
            "batch id predict after epoch:  494\n",
            "batch id predict after epoch:  495\n",
            "batch id predict after epoch:  496\n",
            "batch id predict after epoch:  497\n",
            "batch id predict after epoch:  498\n",
            "batch id predict after epoch:  499\n",
            "batch id predict after epoch:  500\n",
            "batch id predict after epoch:  501\n",
            "batch id predict after epoch:  502\n",
            "batch id predict after epoch:  503\n",
            "batch id predict after epoch:  504\n",
            "batch id predict after epoch:  505\n",
            "batch id predict after epoch:  506\n",
            "batch id predict after epoch:  507\n",
            "batch id predict after epoch:  508\n",
            "batch id predict after epoch:  509\n",
            "batch id predict after epoch:  510\n",
            "batch id predict after epoch:  511\n",
            "batch id predict after epoch:  512\n",
            "batch id predict after epoch:  513\n",
            "batch id predict after epoch:  514\n",
            "batch id predict after epoch:  515\n",
            "batch id predict after epoch:  516\n",
            "batch id predict after epoch:  517\n",
            "batch id predict after epoch:  518\n",
            "batch id predict after epoch:  519\n",
            "batch id predict after epoch:  520\n",
            "batch id predict after epoch:  521\n",
            "batch id predict after epoch:  522\n",
            "batch id predict after epoch:  523\n",
            "batch id predict after epoch:  524\n",
            "batch id predict after epoch:  525\n",
            "batch id predict after epoch:  526\n",
            "batch id predict after epoch:  527\n",
            "batch id predict after epoch:  528\n",
            "batch id predict after epoch:  529\n",
            "batch id predict after epoch:  530\n",
            "batch id predict after epoch:  531\n",
            "batch id predict after epoch:  532\n",
            "batch id predict after epoch:  533\n",
            "batch id predict after epoch:  534\n",
            "batch id predict after epoch:  535\n",
            "batch id predict after epoch:  536\n",
            "batch id predict after epoch:  537\n",
            "batch id predict after epoch:  538\n",
            "batch id predict after epoch:  539\n",
            "batch id predict after epoch:  540\n",
            "batch id predict after epoch:  541\n",
            "batch id predict after epoch:  542\n",
            "batch id predict after epoch:  543\n",
            "batch id predict after epoch:  544\n",
            "batch id predict after epoch:  545\n",
            "batch id predict after epoch:  546\n",
            "batch id predict after epoch:  547\n",
            "batch id predict after epoch:  548\n",
            "batch id predict after epoch:  549\n",
            "batch id predict after epoch:  550\n",
            "batch id predict after epoch:  551\n",
            "batch id predict after epoch:  552\n",
            "batch id predict after epoch:  553\n",
            "batch id predict after epoch:  554\n",
            "batch id predict after epoch:  555\n",
            "batch id predict after epoch:  556\n",
            "batch id predict after epoch:  557\n",
            "batch id predict after epoch:  558\n",
            "batch id predict after epoch:  559\n",
            "batch id predict after epoch:  560\n",
            "batch id predict after epoch:  561\n",
            "batch id predict after epoch:  562\n",
            "batch id predict after epoch:  563\n",
            "batch id predict after epoch:  564\n",
            "batch id predict after epoch:  565\n",
            "batch id predict after epoch:  566\n",
            "batch id predict after epoch:  567\n",
            "batch id predict after epoch:  568\n",
            "batch id predict after epoch:  569\n",
            "batch id predict after epoch:  570\n",
            "batch id predict after epoch:  571\n",
            "batch id predict after epoch:  572\n",
            "batch id predict after epoch:  573\n",
            "batch id predict after epoch:  574\n",
            "batch id predict after epoch:  575\n",
            "batch id predict after epoch:  576\n",
            "batch id predict after epoch:  577\n",
            "batch id predict after epoch:  578\n",
            "batch id predict after epoch:  579\n",
            "batch id predict after epoch:  580\n",
            "batch id predict after epoch:  581\n",
            "batch id predict after epoch:  582\n",
            "batch id predict after epoch:  583\n",
            "batch id predict after epoch:  584\n",
            "batch id predict after epoch:  585\n",
            "batch id predict after epoch:  586\n",
            "batch id predict after epoch:  587\n",
            "batch id predict after epoch:  588\n",
            "batch id predict after epoch:  589\n",
            "batch id predict after epoch:  590\n",
            "batch id predict after epoch:  591\n",
            "batch id predict after epoch:  592\n",
            "batch id predict after epoch:  593\n",
            "batch id predict after epoch:  594\n",
            "batch id predict after epoch:  595\n",
            "batch id predict after epoch:  596\n",
            "batch id predict after epoch:  597\n",
            "batch id predict after epoch:  598\n",
            "batch id predict after epoch:  599\n",
            "batch id predict after epoch:  600\n",
            "batch id predict after epoch:  601\n",
            "batch id predict after epoch:  602\n",
            "batch id predict after epoch:  603\n",
            "batch id predict after epoch:  604\n",
            "batch id predict after epoch:  605\n",
            "batch id predict after epoch:  606\n",
            "batch id predict after epoch:  607\n",
            "batch id predict after epoch:  608\n",
            "batch id predict after epoch:  609\n",
            "batch id predict after epoch:  610\n",
            "batch id predict after epoch:  611\n",
            "batch id predict after epoch:  612\n",
            "batch id predict after epoch:  613\n",
            "batch id predict after epoch:  614\n",
            "batch id predict after epoch:  615\n",
            "batch id predict after epoch:  616\n",
            "batch id predict after epoch:  617\n",
            "batch id predict after epoch:  618\n",
            "batch id predict after epoch:  619\n",
            "batch id predict after epoch:  620\n",
            "batch id predict after epoch:  621\n",
            "batch id predict after epoch:  622\n",
            "batch id predict after epoch:  623\n",
            "batch id predict after epoch:  624\n",
            "batch id predict after epoch:  625\n",
            "batch id predict after epoch:  626\n",
            "batch id predict after epoch:  627\n",
            "batch id predict after epoch:  628\n",
            "batch id predict after epoch:  629\n",
            "batch id predict after epoch:  630\n",
            "batch id predict after epoch:  631\n",
            "batch id predict after epoch:  632\n",
            "batch id predict after epoch:  633\n",
            "batch id predict after epoch:  634\n",
            "batch id predict after epoch:  635\n",
            "batch id predict after epoch:  636\n",
            "batch id predict after epoch:  637\n",
            "batch id predict after epoch:  638\n",
            "batch id predict after epoch:  639\n",
            "batch id predict after epoch:  640\n",
            "batch id predict after epoch:  641\n",
            "batch id predict after epoch:  642\n",
            "batch id predict after epoch:  643\n",
            "batch id predict after epoch:  644\n",
            "batch id predict after epoch:  645\n",
            "batch id predict after epoch:  646\n",
            "batch id predict after epoch:  647\n",
            "batch id predict after epoch:  648\n",
            "batch id predict after epoch:  649\n",
            "batch id predict after epoch:  650\n",
            "batch id predict after epoch:  651\n",
            "batch id predict after epoch:  652\n",
            "batch id predict after epoch:  653\n",
            "batch id predict after epoch:  654\n",
            "batch id predict after epoch:  655\n",
            "batch id predict after epoch:  656\n",
            "batch id predict after epoch:  657\n",
            "batch id predict after epoch:  658\n",
            "batch id predict after epoch:  659\n",
            "batch id predict after epoch:  660\n",
            "batch id predict after epoch:  661\n",
            "batch id predict after epoch:  662\n",
            "batch id predict after epoch:  663\n",
            "batch id predict after epoch:  664\n",
            "batch id predict after epoch:  665\n",
            "batch id predict after epoch:  666\n",
            "batch id predict after epoch:  667\n",
            "batch id predict after epoch:  668\n",
            "batch id predict after epoch:  669\n",
            "batch id predict after epoch:  670\n",
            "batch id predict after epoch:  671\n",
            "batch id predict after epoch:  672\n",
            "batch id predict after epoch:  673\n",
            "batch id predict after epoch:  674\n",
            "batch id predict after epoch:  675\n",
            "batch id predict after epoch:  676\n",
            "batch id predict after epoch:  677\n",
            "batch id predict after epoch:  678\n",
            "batch id predict after epoch:  679\n",
            "batch id predict after epoch:  680\n",
            "batch id predict after epoch:  681\n",
            "batch id predict after epoch:  682\n",
            "batch id predict after epoch:  683\n",
            "batch id predict after epoch:  684\n",
            "batch id predict after epoch:  685\n",
            "batch id predict after epoch:  686\n",
            "batch id predict after epoch:  687\n",
            "batch id predict after epoch:  688\n",
            "batch id predict after epoch:  689\n",
            "batch id predict after epoch:  690\n",
            "batch id predict after epoch:  691\n",
            "batch id predict after epoch:  692\n",
            "batch id predict after epoch:  693\n",
            "batch id predict after epoch:  694\n",
            "batch id predict after epoch:  695\n",
            "batch id predict after epoch:  696\n",
            "batch id predict after epoch:  697\n",
            "batch id predict after epoch:  698\n",
            "batch id predict after epoch:  699\n",
            "batch id predict after epoch:  700\n",
            "batch id predict after epoch:  701\n",
            "batch id predict after epoch:  702\n",
            "batch id predict after epoch:  703\n",
            "batch id predict after epoch:  704\n",
            "batch id predict after epoch:  705\n",
            "batch id predict after epoch:  706\n",
            "batch id predict after epoch:  707\n",
            "batch id predict after epoch:  708\n",
            "batch id predict after epoch:  709\n",
            "batch id predict after epoch:  710\n",
            "batch id predict after epoch:  711\n",
            "batch id predict after epoch:  712\n",
            "batch id predict after epoch:  713\n",
            "batch id predict after epoch:  714\n",
            "batch id predict after epoch:  715\n",
            "batch id predict after epoch:  716\n",
            "batch id predict after epoch:  717\n",
            "batch id predict after epoch:  718\n",
            "batch id predict after epoch:  719\n",
            "batch id predict after epoch:  720\n",
            "batch id predict after epoch:  721\n",
            "batch id predict after epoch:  722\n",
            "batch id predict after epoch:  723\n",
            "batch id predict after epoch:  724\n",
            "batch id predict after epoch:  725\n",
            "batch id predict after epoch:  726\n",
            "batch id predict after epoch:  727\n",
            "batch id predict after epoch:  728\n",
            "batch id predict after epoch:  729\n",
            "batch id predict after epoch:  730\n",
            "batch id predict after epoch:  731\n",
            "batch id predict after epoch:  732\n",
            "batch id predict after epoch:  733\n",
            "batch id predict after epoch:  734\n",
            "batch id predict after epoch:  735\n",
            "batch id predict after epoch:  736\n",
            "batch id predict after epoch:  737\n",
            "batch id predict after epoch:  738\n",
            "batch id predict after epoch:  739\n",
            "batch id predict after epoch:  740\n",
            "batch id predict after epoch:  741\n",
            "batch id predict after epoch:  742\n",
            "batch id predict after epoch:  743\n",
            "batch id predict after epoch:  744\n",
            "batch id predict after epoch:  745\n",
            "batch id predict after epoch:  746\n",
            "batch id predict after epoch:  747\n",
            "batch id predict after epoch:  748\n",
            "batch id predict after epoch:  749\n",
            "batch id predict after epoch:  750\n",
            "batch id predict after epoch:  751\n",
            "batch id predict after epoch:  752\n",
            "batch id predict after epoch:  753\n",
            "batch id predict after epoch:  754\n",
            "batch id predict after epoch:  755\n",
            "batch id predict after epoch:  756\n",
            "batch id predict after epoch:  757\n",
            "batch id predict after epoch:  758\n",
            "batch id predict after epoch:  759\n",
            "batch id predict after epoch:  760\n",
            "batch id predict after epoch:  761\n",
            "batch id predict after epoch:  762\n",
            "batch id predict after epoch:  763\n",
            "batch id predict after epoch:  764\n",
            "batch id predict after epoch:  765\n",
            "batch id predict after epoch:  766\n",
            "batch id predict after epoch:  767\n",
            "batch id predict after epoch:  768\n",
            "batch id predict after epoch:  769\n",
            "batch id predict after epoch:  770\n",
            "batch id predict after epoch:  771\n",
            "batch id predict after epoch:  772\n",
            "batch id predict after epoch:  773\n",
            "batch id predict after epoch:  774\n",
            "batch id predict after epoch:  775\n",
            "batch id predict after epoch:  776\n",
            "batch id predict after epoch:  777\n",
            "batch id predict after epoch:  778\n",
            "batch id predict after epoch:  779\n",
            "batch id predict after epoch:  780\n",
            "batch id predict after epoch:  781\n",
            "batch id predict after epoch:  782\n",
            "batch id predict after epoch:  783\n",
            "batch id predict after epoch:  784\n",
            "batch id predict after epoch:  785\n",
            "batch id predict after epoch:  786\n",
            "batch id predict after epoch:  787\n",
            "batch id predict after epoch:  788\n",
            "batch id predict after epoch:  789\n",
            "batch id predict after epoch:  790\n",
            "batch id predict after epoch:  791\n",
            "batch id predict after epoch:  792\n",
            "batch id predict after epoch:  793\n",
            "batch id predict after epoch:  794\n",
            "batch id predict after epoch:  795\n",
            "batch id predict after epoch:  796\n",
            "batch id predict after epoch:  797\n",
            "batch id predict after epoch:  798\n",
            "batch id predict after epoch:  799\n",
            "batch id predict after epoch:  800\n",
            "batch id predict after epoch:  801\n",
            "batch id predict after epoch:  802\n",
            "batch id predict after epoch:  803\n",
            "batch id predict after epoch:  804\n",
            "batch id predict after epoch:  805\n",
            "batch id predict after epoch:  806\n",
            "batch id predict after epoch:  807\n",
            "batch id predict after epoch:  808\n",
            "batch id predict after epoch:  809\n",
            "batch id predict after epoch:  810\n",
            "batch id predict after epoch:  811\n",
            "batch id predict after epoch:  812\n",
            "batch id predict after epoch:  813\n",
            "batch id predict after epoch:  814\n",
            "batch id predict after epoch:  815\n",
            "batch id predict after epoch:  816\n",
            "batch id predict after epoch:  817\n",
            "batch id predict after epoch:  818\n",
            "batch id predict after epoch:  819\n",
            "batch id predict after epoch:  820\n",
            "batch id predict after epoch:  821\n",
            "batch id predict after epoch:  822\n",
            "batch id predict after epoch:  823\n",
            "batch id predict after epoch:  824\n",
            "batch id predict after epoch:  825\n",
            "batch id predict after epoch:  826\n",
            "batch id predict after epoch:  827\n",
            "batch id predict after epoch:  828\n",
            "batch id predict after epoch:  829\n",
            "batch id predict after epoch:  830\n",
            "batch id predict after epoch:  831\n",
            "batch id predict after epoch:  832\n",
            "batch id predict after epoch:  833\n",
            "batch id predict after epoch:  834\n",
            "batch id predict after epoch:  835\n",
            "batch id predict after epoch:  836\n",
            "batch id predict after epoch:  837\n",
            "batch id predict after epoch:  838\n",
            "batch id predict after epoch:  839\n",
            "batch id predict after epoch:  840\n",
            "batch id predict after epoch:  841\n",
            "batch id predict after epoch:  842\n",
            "batch id predict after epoch:  843\n",
            "batch id predict after epoch:  844\n",
            "batch id predict after epoch:  845\n",
            "batch id predict after epoch:  846\n",
            "batch id predict after epoch:  847\n",
            "batch id predict after epoch:  848\n",
            "batch id predict after epoch:  849\n",
            "batch id predict after epoch:  850\n",
            "batch id predict after epoch:  851\n",
            "batch id predict after epoch:  852\n",
            "batch id predict after epoch:  853\n",
            "batch id predict after epoch:  854\n",
            "batch id predict after epoch:  855\n",
            "batch id predict after epoch:  856\n",
            "batch id predict after epoch:  857\n",
            "batch id predict after epoch:  858\n",
            "batch id predict after epoch:  859\n",
            "batch id predict after epoch:  860\n",
            "batch id predict after epoch:  861\n",
            "batch id predict after epoch:  862\n",
            "batch id predict after epoch:  863\n",
            "batch id predict after epoch:  864\n",
            "batch id predict after epoch:  865\n",
            "batch id predict after epoch:  866\n",
            "batch id predict after epoch:  867\n",
            "batch id predict after epoch:  868\n",
            "batch id predict after epoch:  869\n",
            "batch id predict after epoch:  870\n",
            "batch id predict after epoch:  871\n",
            "batch id predict after epoch:  872\n",
            "batch id predict after epoch:  873\n",
            "batch id predict after epoch:  874\n",
            "batch id predict after epoch:  875\n",
            "batch id predict after epoch:  876\n",
            "batch id predict after epoch:  877\n",
            "batch id predict after epoch:  878\n",
            "batch id predict after epoch:  879\n",
            "batch id predict after epoch:  880\n",
            "batch id predict after epoch:  881\n",
            "batch id predict after epoch:  882\n",
            "batch id predict after epoch:  883\n",
            "batch id predict after epoch:  884\n",
            "batch id predict after epoch:  885\n",
            "batch id predict after epoch:  886\n",
            "batch id predict after epoch:  887\n",
            "batch id predict after epoch:  888\n",
            "batch id predict after epoch:  889\n",
            "batch id predict after epoch:  890\n",
            "batch id predict after epoch:  891\n",
            "batch id predict after epoch:  892\n",
            "batch id predict after epoch:  893\n",
            "batch id predict after epoch:  894\n",
            "batch id predict after epoch:  895\n",
            "batch id predict after epoch:  896\n",
            "batch id predict after epoch:  897\n",
            "batch id predict after epoch:  898\n",
            "batch id predict after epoch:  899\n",
            "batch id predict after epoch:  900\n",
            "batch id predict after epoch:  901\n",
            "batch id predict after epoch:  902\n",
            "batch id predict after epoch:  903\n",
            "batch id predict after epoch:  904\n",
            "batch id predict after epoch:  905\n",
            "batch id predict after epoch:  906\n",
            "batch id predict after epoch:  907\n",
            "batch id predict after epoch:  908\n",
            "batch id predict after epoch:  909\n",
            "batch id predict after epoch:  910\n",
            "batch id predict after epoch:  911\n",
            "batch id predict after epoch:  912\n",
            "batch id predict after epoch:  913\n",
            "batch id predict after epoch:  914\n",
            "batch id predict after epoch:  915\n",
            "batch id predict after epoch:  916\n",
            "batch id predict after epoch:  917\n",
            "batch id predict after epoch:  918\n",
            "batch id predict after epoch:  919\n",
            "batch id predict after epoch:  920\n",
            "batch id predict after epoch:  921\n",
            "batch id predict after epoch:  922\n",
            "batch id predict after epoch:  923\n",
            "batch id predict after epoch:  924\n",
            "batch id predict after epoch:  925\n",
            "batch id predict after epoch:  926\n",
            "batch id predict after epoch:  927\n",
            "batch id predict after epoch:  928\n",
            "batch id predict after epoch:  929\n",
            "batch id predict after epoch:  930\n",
            "batch id predict after epoch:  931\n",
            "[1 0 0 ... 1 1 1]\n",
            "mini Batch 1 Loss: 2.761552572250366\n",
            "mini Batch 2 Loss: 3.3874473571777344\n",
            "mini Batch 3 Loss: 2.572946548461914\n",
            "mini Batch 4 Loss: 3.145235300064087\n",
            "mini Batch 5 Loss: 3.6130177974700928\n",
            "mini Batch 6 Loss: 2.2754664421081543\n",
            "mini Batch 7 Loss: 2.546923875808716\n",
            "mini Batch 8 Loss: 2.627516746520996\n",
            "mini Batch 9 Loss: 1.9699053764343262\n",
            "mini Batch 10 Loss: 3.4970626831054688\n",
            "mini Batch 11 Loss: 2.4214866161346436\n",
            "mini Batch 12 Loss: 3.119349956512451\n",
            "mini Batch 13 Loss: 2.3045220375061035\n",
            "mini Batch 14 Loss: 2.082629680633545\n",
            "mini Batch 15 Loss: 2.8407950401306152\n",
            "mini Batch 16 Loss: 1.8069944381713867\n",
            "mini Batch 17 Loss: 2.641490936279297\n",
            "mini Batch 18 Loss: 2.4977142810821533\n",
            "mini Batch 19 Loss: 1.9943912029266357\n",
            "mini Batch 20 Loss: 2.3024373054504395\n",
            "mini Batch 21 Loss: 2.793260097503662\n",
            "mini Batch 22 Loss: 2.439451217651367\n",
            "mini Batch 23 Loss: 2.4129576683044434\n",
            "mini Batch 24 Loss: 2.0763015747070312\n",
            "mini Batch 25 Loss: 3.4226551055908203\n",
            "mini Batch 26 Loss: 2.452005386352539\n",
            "mini Batch 27 Loss: 2.2086586952209473\n",
            "mini Batch 28 Loss: 2.424001455307007\n",
            "mini Batch 29 Loss: 2.281731367111206\n",
            "mini Batch 30 Loss: 2.3560729026794434\n",
            "mini Batch 31 Loss: 1.5936352014541626\n",
            "mini Batch 32 Loss: 2.8951568603515625\n",
            "mini Batch 33 Loss: 3.4809625148773193\n",
            "mini Batch 34 Loss: 3.3033981323242188\n",
            "mini Batch 35 Loss: 3.4388487339019775\n",
            "mini Batch 36 Loss: 2.287879467010498\n",
            "mini Batch 37 Loss: 2.752460241317749\n",
            "mini Batch 38 Loss: 2.395678758621216\n",
            "mini Batch 39 Loss: 2.6410441398620605\n",
            "mini Batch 40 Loss: 2.1963279247283936\n",
            "mini Batch 41 Loss: 2.314737319946289\n",
            "mini Batch 42 Loss: 3.2751212120056152\n",
            "mini Batch 43 Loss: 3.164853572845459\n",
            "mini Batch 44 Loss: 2.7238264083862305\n",
            "mini Batch 45 Loss: 2.493227481842041\n",
            "mini Batch 46 Loss: 3.1596086025238037\n",
            "mini Batch 47 Loss: 3.5673000812530518\n",
            "mini Batch 48 Loss: 2.9976463317871094\n",
            "mini Batch 49 Loss: 1.8209247589111328\n",
            "mini Batch 50 Loss: 2.7376885414123535\n",
            "mini Batch 51 Loss: 2.923995018005371\n",
            "mini Batch 52 Loss: 2.72584867477417\n",
            "mini Batch 53 Loss: 0.637682318687439\n",
            "mini Batch 54 Loss: 2.9986584186553955\n",
            "mini Batch 55 Loss: 3.2722926139831543\n",
            "mini Batch 56 Loss: 3.192837953567505\n",
            "mini Batch 57 Loss: 4.632131576538086\n",
            "mini Batch 58 Loss: 3.9548022747039795\n",
            "mini Batch 59 Loss: 3.245709180831909\n",
            "mini Batch 60 Loss: 3.872150182723999\n",
            "mini Batch 61 Loss: 2.546602725982666\n",
            "mini Batch 62 Loss: 2.567735433578491\n",
            "mini Batch 63 Loss: 3.557253360748291\n",
            "mini Batch 64 Loss: 3.470004081726074\n",
            "mini Batch 65 Loss: 2.5825233459472656\n",
            "mini Batch 66 Loss: 3.2897539138793945\n",
            "mini Batch 67 Loss: 3.4468934535980225\n",
            "mini Batch 68 Loss: 3.1729655265808105\n",
            "mini Batch 69 Loss: 4.518071174621582\n",
            "mini Batch 70 Loss: 3.602461338043213\n",
            "mini Batch 71 Loss: 3.46553373336792\n",
            "mini Batch 72 Loss: 2.958256244659424\n",
            "mini Batch 73 Loss: 2.691215753555298\n",
            "mini Batch 74 Loss: 2.4596500396728516\n",
            "mini Batch 75 Loss: 3.492032527923584\n",
            "mini Batch 76 Loss: 3.4977900981903076\n",
            "mini Batch 77 Loss: 3.3711891174316406\n",
            "mini Batch 78 Loss: 2.8546905517578125\n",
            "mini Batch 79 Loss: 2.2741711139678955\n",
            "mini Batch 80 Loss: 3.5986523628234863\n",
            "mini Batch 81 Loss: 3.3858463764190674\n",
            "mini Batch 82 Loss: 2.4591140747070312\n",
            "mini Batch 83 Loss: 2.839561939239502\n",
            "mini Batch 84 Loss: 2.796220302581787\n",
            "mini Batch 85 Loss: 3.522840976715088\n",
            "mini Batch 86 Loss: 4.152806758880615\n",
            "mini Batch 87 Loss: 3.5003726482391357\n",
            "mini Batch 88 Loss: 2.538193941116333\n",
            "mini Batch 89 Loss: 3.3828909397125244\n",
            "mini Batch 90 Loss: 2.567868709564209\n",
            "mini Batch 91 Loss: 3.4194655418395996\n",
            "mini Batch 92 Loss: 3.491724967956543\n",
            "mini Batch 93 Loss: 2.925351142883301\n",
            "mini Batch 94 Loss: 2.867619037628174\n",
            "mini Batch 95 Loss: 2.953263998031616\n",
            "mini Batch 96 Loss: 2.7518887519836426\n",
            "mini Batch 97 Loss: 2.1673474311828613\n",
            "mini Batch 98 Loss: 3.7102389335632324\n",
            "mini Batch 99 Loss: 2.720735549926758\n",
            "mini Batch 100 Loss: 2.6806931495666504\n",
            "mini Batch 101 Loss: 3.7777750492095947\n",
            "mini Batch 102 Loss: 3.132538080215454\n",
            "mini Batch 103 Loss: 3.792771100997925\n",
            "mini Batch 104 Loss: 2.5437734127044678\n",
            "mini Batch 105 Loss: 3.280869960784912\n",
            "mini Batch 106 Loss: 2.9164021015167236\n",
            "mini Batch 107 Loss: 2.1731956005096436\n",
            "mini Batch 108 Loss: 3.4065117835998535\n",
            "mini Batch 109 Loss: 2.7978196144104004\n",
            "mini Batch 110 Loss: 4.160910606384277\n",
            "mini Batch 111 Loss: 2.0080904960632324\n",
            "mini Batch 112 Loss: 4.090805530548096\n",
            "mini Batch 113 Loss: 3.1223769187927246\n",
            "mini Batch 114 Loss: 2.6491777896881104\n",
            "mini Batch 115 Loss: 3.2935893535614014\n",
            "mini Batch 116 Loss: 2.983677625656128\n",
            "mini Batch 117 Loss: 2.698367118835449\n",
            "mini Batch 118 Loss: 2.889773368835449\n",
            "mini Batch 119 Loss: 2.4900429248809814\n",
            "mini Batch 120 Loss: 3.405956745147705\n",
            "mini Batch 121 Loss: 3.606355905532837\n",
            "mini Batch 122 Loss: 2.612938404083252\n",
            "mini Batch 123 Loss: 2.6749298572540283\n",
            "mini Batch 124 Loss: 3.358705520629883\n",
            "mini Batch 125 Loss: 2.835397243499756\n",
            "mini Batch 126 Loss: 1.9659167528152466\n",
            "mini Batch 127 Loss: 2.4908857345581055\n",
            "mini Batch 128 Loss: 2.7779769897460938\n",
            "mini Batch 129 Loss: 3.153902292251587\n",
            "mini Batch 130 Loss: 2.3852927684783936\n",
            "mini Batch 131 Loss: 2.7340636253356934\n",
            "mini Batch 132 Loss: 2.459303855895996\n",
            "mini Batch 133 Loss: 2.4684882164001465\n",
            "mini Batch 134 Loss: 2.154602289199829\n",
            "mini Batch 135 Loss: 3.028380870819092\n",
            "mini Batch 136 Loss: 3.2826366424560547\n",
            "mini Batch 137 Loss: 3.2128398418426514\n",
            "mini Batch 138 Loss: 3.365668296813965\n",
            "mini Batch 139 Loss: 3.503145933151245\n",
            "mini Batch 140 Loss: 3.126286506652832\n",
            "mini Batch 141 Loss: 2.829418659210205\n",
            "mini Batch 142 Loss: 3.4236323833465576\n",
            "mini Batch 143 Loss: 3.09759783744812\n",
            "mini Batch 144 Loss: 3.3425545692443848\n",
            "mini Batch 145 Loss: 2.1725709438323975\n",
            "mini Batch 146 Loss: 2.7912416458129883\n",
            "mini Batch 147 Loss: 2.5824413299560547\n",
            "mini Batch 148 Loss: 2.6857104301452637\n",
            "mini Batch 149 Loss: 3.172457218170166\n",
            "mini Batch 150 Loss: 1.7584383487701416\n",
            "mini Batch 151 Loss: 2.806297779083252\n",
            "mini Batch 152 Loss: 2.5594773292541504\n",
            "mini Batch 153 Loss: 3.4867019653320312\n",
            "mini Batch 154 Loss: 3.263383388519287\n",
            "mini Batch 155 Loss: 2.7127277851104736\n",
            "mini Batch 156 Loss: 3.4408369064331055\n",
            "mini Batch 157 Loss: 3.203672409057617\n",
            "mini Batch 158 Loss: 2.967184543609619\n",
            "mini Batch 159 Loss: 3.279271364212036\n",
            "mini Batch 160 Loss: 3.340095043182373\n",
            "mini Batch 161 Loss: 2.534407138824463\n",
            "mini Batch 162 Loss: 3.743407726287842\n",
            "mini Batch 163 Loss: 3.200836658477783\n",
            "mini Batch 164 Loss: 2.000955104827881\n",
            "mini Batch 165 Loss: 2.8860762119293213\n",
            "mini Batch 166 Loss: 4.386078357696533\n",
            "mini Batch 167 Loss: 2.9735164642333984\n",
            "mini Batch 168 Loss: 2.9494199752807617\n",
            "mini Batch 169 Loss: 2.90803599357605\n",
            "mini Batch 170 Loss: 1.7824722528457642\n",
            "mini Batch 171 Loss: 2.544980764389038\n",
            "mini Batch 172 Loss: 2.107901096343994\n",
            "mini Batch 173 Loss: 3.0058517456054688\n",
            "mini Batch 174 Loss: 2.6863293647766113\n",
            "mini Batch 175 Loss: 2.9600775241851807\n",
            "mini Batch 176 Loss: 2.3040223121643066\n",
            "mini Batch 177 Loss: 3.5007877349853516\n",
            "mini Batch 178 Loss: 2.168083667755127\n",
            "mini Batch 179 Loss: 2.6843130588531494\n",
            "mini Batch 180 Loss: 2.3775410652160645\n",
            "mini Batch 181 Loss: 3.75386905670166\n",
            "mini Batch 182 Loss: 3.4357564449310303\n",
            "mini Batch 183 Loss: 2.316030979156494\n",
            "mini Batch 184 Loss: 2.5501065254211426\n",
            "mini Batch 185 Loss: 3.113023042678833\n",
            "mini Batch 186 Loss: 2.135791301727295\n",
            "mini Batch 187 Loss: 3.8982837200164795\n",
            "mini Batch 188 Loss: 1.811976432800293\n",
            "mini Batch 189 Loss: 1.9209511280059814\n",
            "mini Batch 190 Loss: 2.2674479484558105\n",
            "mini Batch 191 Loss: 2.287323474884033\n",
            "mini Batch 192 Loss: 3.3605146408081055\n",
            "mini Batch 193 Loss: 3.2744903564453125\n",
            "mini Batch 194 Loss: 1.9611327648162842\n",
            "mini Batch 195 Loss: 3.0602588653564453\n",
            "mini Batch 196 Loss: 2.3245842456817627\n",
            "mini Batch 197 Loss: 1.9566477537155151\n",
            "mini Batch 198 Loss: 1.939393401145935\n",
            "mini Batch 199 Loss: 2.946584701538086\n",
            "mini Batch 200 Loss: 2.276310443878174\n",
            "mini Batch 201 Loss: 2.5583040714263916\n",
            "Training Batch: 201 | Training Loss: 2.5583040714263916\n",
            "dist a: tensor([17.4788, 13.4159, 12.9763, 18.5129, 17.9336, 13.0988, 17.6694, 14.5043,\n",
            "        13.5488, 10.4100, 14.1362, 14.8135, 20.6126, 10.2399, 11.9477, 13.7801,\n",
            "        17.3095, 17.5704, 17.4878, 13.7635, 12.5018, 14.5137, 16.1622, 13.4402,\n",
            "        11.8746, 15.3633, 17.2406, 18.4986, 20.5260, 20.2203, 14.6116, 14.6149,\n",
            "         5.7018, 12.4467, 13.1318, 16.8182, 13.6690, 21.0104, 18.8052, 17.1883,\n",
            "        12.2148, 16.0020, 12.5045,  8.6406, 14.3237, 14.7420, 17.1549, 11.4173,\n",
            "        11.9417, 11.0211, 10.8198, 10.2531, 14.0725, 17.6392, 11.5527, 12.5509,\n",
            "        13.0157, 17.4182, 15.0646,  9.9556, 10.1112, 15.3045, 13.8919, 11.1987],\n",
            "       device='cuda:0'), dist b: tensor([21.9021, 17.1405, 20.8642, 12.4918, 13.7318, 12.1086, 24.3060, 18.5359,\n",
            "        21.5337, 12.4360, 15.0247, 22.3884, 15.1761, 16.3733, 15.0981, 11.3319,\n",
            "        13.3560, 11.7645, 13.8108, 11.1330,  9.9517, 10.0146, 28.6326, 11.4264,\n",
            "        21.6653, 22.8086, 18.6395, 19.8997, 10.9101, 11.4689, 16.9581, 20.6324,\n",
            "        22.7966, 17.6643, 15.1829, 15.8486, 15.3343, 19.4464, 15.5285, 26.1366,\n",
            "        13.8825, 20.7111, 25.3388, 15.1607, 25.2499, 11.8524,  9.7715, 18.1560,\n",
            "        18.3012, 17.7039, 17.8382,  9.8453, 13.9995, 11.3920, 13.9804, 13.0441,\n",
            "        15.2428,  9.9186, 16.3441, 11.9810, 15.9001, 14.9131, 14.7801, 17.1713],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.625 \n",
            "tensor(0.6250)\n",
            "Training Batch: 201 | Model saved to: /content/drive/My Drive/test6/model_epoch_2_batch_201.pt\n",
            "mini Batch 202 Loss: 1.7910436391830444\n",
            "mini Batch 203 Loss: 2.635576009750366\n",
            "mini Batch 204 Loss: 1.9583040475845337\n",
            "mini Batch 205 Loss: 2.354694366455078\n",
            "mini Batch 206 Loss: 2.484283924102783\n",
            "mini Batch 207 Loss: 3.6715755462646484\n",
            "mini Batch 208 Loss: 2.2723560333251953\n",
            "mini Batch 209 Loss: 3.680598735809326\n",
            "mini Batch 210 Loss: 2.949721336364746\n",
            "mini Batch 211 Loss: 2.535457134246826\n",
            "mini Batch 212 Loss: 3.891361951828003\n",
            "mini Batch 213 Loss: 3.611316680908203\n",
            "mini Batch 214 Loss: 2.8259315490722656\n",
            "mini Batch 215 Loss: 2.265221357345581\n",
            "mini Batch 216 Loss: 2.5916037559509277\n",
            "mini Batch 217 Loss: 1.4805936813354492\n",
            "mini Batch 218 Loss: 2.234943151473999\n",
            "mini Batch 219 Loss: 3.475630283355713\n",
            "mini Batch 220 Loss: 2.8802430629730225\n",
            "mini Batch 221 Loss: 1.2762324810028076\n",
            "mini Batch 222 Loss: 2.655291795730591\n",
            "mini Batch 223 Loss: 2.410360813140869\n",
            "mini Batch 224 Loss: 2.4797563552856445\n",
            "mini Batch 225 Loss: 2.755765914916992\n",
            "mini Batch 226 Loss: 3.059765100479126\n",
            "mini Batch 227 Loss: 1.956366777420044\n",
            "mini Batch 228 Loss: 2.989595413208008\n",
            "mini Batch 229 Loss: 2.9341695308685303\n",
            "mini Batch 230 Loss: 1.5366473197937012\n",
            "mini Batch 231 Loss: 1.8026982545852661\n",
            "mini Batch 232 Loss: 2.6921746730804443\n",
            "mini Batch 233 Loss: 2.862107276916504\n",
            "mini Batch 234 Loss: 2.3605875968933105\n",
            "mini Batch 235 Loss: 1.717764139175415\n",
            "mini Batch 236 Loss: 2.886176586151123\n",
            "mini Batch 237 Loss: 2.448007822036743\n",
            "mini Batch 238 Loss: 2.2057409286499023\n",
            "mini Batch 239 Loss: 2.7157742977142334\n",
            "mini Batch 240 Loss: 1.7119834423065186\n",
            "mini Batch 241 Loss: 2.2005057334899902\n",
            "mini Batch 242 Loss: 3.15981388092041\n",
            "mini Batch 243 Loss: 2.4396188259124756\n",
            "mini Batch 244 Loss: 1.8702905178070068\n",
            "mini Batch 245 Loss: 1.906086802482605\n",
            "mini Batch 246 Loss: 3.033397674560547\n",
            "mini Batch 247 Loss: 2.8975939750671387\n",
            "mini Batch 248 Loss: 2.287517786026001\n",
            "mini Batch 249 Loss: 2.482114553451538\n",
            "mini Batch 250 Loss: 2.827183723449707\n",
            "mini Batch 251 Loss: 2.1231250762939453\n",
            "mini Batch 252 Loss: 1.0879820585250854\n",
            "mini Batch 253 Loss: 4.112565040588379\n",
            "mini Batch 254 Loss: 2.5628037452697754\n",
            "mini Batch 255 Loss: 1.500974178314209\n",
            "mini Batch 256 Loss: 2.1125388145446777\n",
            "mini Batch 257 Loss: 1.856584072113037\n",
            "mini Batch 258 Loss: 2.0703487396240234\n",
            "mini Batch 259 Loss: 1.4634814262390137\n",
            "mini Batch 260 Loss: 1.4026459455490112\n",
            "mini Batch 261 Loss: 1.4195390939712524\n",
            "mini Batch 262 Loss: 3.1796844005584717\n",
            "mini Batch 263 Loss: 1.9675952196121216\n",
            "mini Batch 264 Loss: 2.2576255798339844\n",
            "mini Batch 265 Loss: 2.5825634002685547\n",
            "mini Batch 266 Loss: 2.4639880657196045\n",
            "mini Batch 267 Loss: 3.3328752517700195\n",
            "mini Batch 268 Loss: 2.4364709854125977\n",
            "mini Batch 269 Loss: 3.0494065284729004\n",
            "mini Batch 270 Loss: 1.389852523803711\n",
            "mini Batch 271 Loss: 4.405661582946777\n",
            "mini Batch 272 Loss: 3.217552661895752\n",
            "mini Batch 273 Loss: 3.060086727142334\n",
            "mini Batch 274 Loss: 1.8917875289916992\n",
            "mini Batch 275 Loss: 3.6596240997314453\n",
            "mini Batch 276 Loss: 1.5792958736419678\n",
            "mini Batch 277 Loss: 1.8050715923309326\n",
            "mini Batch 278 Loss: 2.155884027481079\n",
            "mini Batch 279 Loss: 2.557225465774536\n",
            "mini Batch 280 Loss: 4.3738861083984375\n",
            "mini Batch 281 Loss: 1.7609362602233887\n",
            "mini Batch 282 Loss: 4.356287002563477\n",
            "mini Batch 283 Loss: 3.7349324226379395\n",
            "mini Batch 284 Loss: 3.4616527557373047\n",
            "mini Batch 285 Loss: 3.248227119445801\n",
            "mini Batch 286 Loss: 2.6826589107513428\n",
            "mini Batch 287 Loss: 1.8336163759231567\n",
            "mini Batch 288 Loss: 1.8929526805877686\n",
            "mini Batch 289 Loss: 3.363057851791382\n",
            "mini Batch 290 Loss: 1.782538652420044\n",
            "mini Batch 291 Loss: 2.796967029571533\n",
            "mini Batch 292 Loss: 2.5185294151306152\n",
            "mini Batch 293 Loss: 2.150214672088623\n",
            "mini Batch 294 Loss: 2.230635166168213\n",
            "mini Batch 295 Loss: 2.591205596923828\n",
            "mini Batch 296 Loss: 3.1567869186401367\n",
            "mini Batch 297 Loss: 2.8336734771728516\n",
            "mini Batch 298 Loss: 3.4648571014404297\n",
            "mini Batch 299 Loss: 1.7741376161575317\n",
            "mini Batch 300 Loss: 2.384019136428833\n",
            "mini Batch 301 Loss: 2.5010995864868164\n",
            "mini Batch 302 Loss: 3.285141944885254\n",
            "mini Batch 303 Loss: 2.2316243648529053\n",
            "mini Batch 304 Loss: 3.649158477783203\n",
            "mini Batch 305 Loss: 3.3041462898254395\n",
            "mini Batch 306 Loss: 3.055464506149292\n",
            "mini Batch 307 Loss: 2.425959587097168\n",
            "mini Batch 308 Loss: 2.8740601539611816\n",
            "mini Batch 309 Loss: 2.844796657562256\n",
            "mini Batch 310 Loss: 2.065598487854004\n",
            "mini Batch 311 Loss: 2.9768829345703125\n",
            "mini Batch 312 Loss: 3.207500457763672\n",
            "mini Batch 313 Loss: 3.5124640464782715\n",
            "mini Batch 314 Loss: 2.3702473640441895\n",
            "mini Batch 315 Loss: 3.872641086578369\n",
            "mini Batch 316 Loss: 2.8240628242492676\n",
            "mini Batch 317 Loss: 3.3393986225128174\n",
            "mini Batch 318 Loss: 2.0428247451782227\n",
            "mini Batch 319 Loss: 0.9842532873153687\n",
            "mini Batch 320 Loss: 1.1101864576339722\n",
            "mini Batch 321 Loss: 2.5248074531555176\n",
            "mini Batch 322 Loss: 2.6392135620117188\n",
            "mini Batch 323 Loss: 2.8323166370391846\n",
            "mini Batch 324 Loss: 1.8986711502075195\n",
            "mini Batch 325 Loss: 2.39907169342041\n",
            "mini Batch 326 Loss: 2.055983543395996\n",
            "mini Batch 327 Loss: 1.9945788383483887\n",
            "mini Batch 328 Loss: 1.2467360496520996\n",
            "mini Batch 329 Loss: 2.226362705230713\n",
            "mini Batch 330 Loss: 1.9340324401855469\n",
            "mini Batch 331 Loss: 1.6339704990386963\n",
            "mini Batch 332 Loss: 3.06882643699646\n",
            "mini Batch 333 Loss: 2.40316104888916\n",
            "mini Batch 334 Loss: 2.2896769046783447\n",
            "mini Batch 335 Loss: 2.640170097351074\n",
            "mini Batch 336 Loss: 2.640737533569336\n",
            "mini Batch 337 Loss: 3.775437593460083\n",
            "mini Batch 338 Loss: 2.607731819152832\n",
            "mini Batch 339 Loss: 1.803626298904419\n",
            "mini Batch 340 Loss: 2.432539701461792\n",
            "mini Batch 341 Loss: 3.515207052230835\n",
            "mini Batch 342 Loss: 2.1118245124816895\n",
            "mini Batch 343 Loss: 1.9843158721923828\n",
            "mini Batch 344 Loss: 3.719883918762207\n",
            "mini Batch 345 Loss: 2.1799190044403076\n",
            "mini Batch 346 Loss: 1.3589301109313965\n",
            "mini Batch 347 Loss: 3.6003971099853516\n",
            "mini Batch 348 Loss: 2.6000752449035645\n",
            "mini Batch 349 Loss: 2.8929905891418457\n",
            "mini Batch 350 Loss: 1.0955965518951416\n",
            "mini Batch 351 Loss: 1.616286277770996\n",
            "mini Batch 352 Loss: 1.4419716596603394\n",
            "mini Batch 353 Loss: 2.1988399028778076\n",
            "mini Batch 354 Loss: 1.6951252222061157\n",
            "mini Batch 355 Loss: 2.5275638103485107\n",
            "mini Batch 356 Loss: 3.3137733936309814\n",
            "mini Batch 357 Loss: 1.5358937978744507\n",
            "mini Batch 358 Loss: 1.551275610923767\n",
            "mini Batch 359 Loss: 3.964869499206543\n",
            "mini Batch 360 Loss: 4.132063865661621\n",
            "mini Batch 361 Loss: 5.343090534210205\n",
            "mini Batch 362 Loss: 2.6216988563537598\n",
            "mini Batch 363 Loss: 2.680065631866455\n",
            "mini Batch 364 Loss: 3.424808979034424\n",
            "mini Batch 365 Loss: 3.596552848815918\n",
            "mini Batch 366 Loss: 3.50585675239563\n",
            "mini Batch 367 Loss: 1.5577280521392822\n",
            "mini Batch 368 Loss: 3.262906074523926\n",
            "mini Batch 369 Loss: 2.2179760932922363\n",
            "mini Batch 370 Loss: 3.432823657989502\n",
            "mini Batch 371 Loss: 2.0499539375305176\n",
            "mini Batch 372 Loss: 2.6132802963256836\n",
            "mini Batch 373 Loss: 1.4934918880462646\n",
            "mini Batch 374 Loss: 3.321337938308716\n",
            "mini Batch 375 Loss: 3.1303799152374268\n",
            "mini Batch 376 Loss: 1.5273675918579102\n",
            "mini Batch 377 Loss: 1.3056765794754028\n",
            "mini Batch 378 Loss: 1.5515897274017334\n",
            "mini Batch 379 Loss: 2.6984214782714844\n",
            "mini Batch 380 Loss: 1.402540683746338\n",
            "mini Batch 381 Loss: 1.3951363563537598\n",
            "mini Batch 382 Loss: 0.8745207786560059\n",
            "mini Batch 383 Loss: 0.5243059396743774\n",
            "mini Batch 384 Loss: 3.636942148208618\n",
            "mini Batch 385 Loss: 0.9922802448272705\n",
            "mini Batch 386 Loss: 4.0274176597595215\n",
            "mini Batch 387 Loss: 2.4126415252685547\n",
            "mini Batch 388 Loss: 1.5551762580871582\n",
            "mini Batch 389 Loss: 1.9861974716186523\n",
            "mini Batch 390 Loss: 3.340320348739624\n",
            "mini Batch 391 Loss: 1.508415699005127\n",
            "mini Batch 392 Loss: 2.445371627807617\n",
            "mini Batch 393 Loss: 2.131889820098877\n",
            "mini Batch 394 Loss: 1.9696745872497559\n",
            "mini Batch 395 Loss: 2.2448196411132812\n",
            "mini Batch 396 Loss: 5.5551042556762695\n",
            "mini Batch 397 Loss: 2.3162477016448975\n",
            "mini Batch 398 Loss: 1.786056399345398\n",
            "mini Batch 399 Loss: 2.8014447689056396\n",
            "mini Batch 400 Loss: 2.828558921813965\n",
            "mini Batch 401 Loss: 2.4636683464050293\n",
            "Training Batch: 401 | Training Loss: 2.4636683464050293\n",
            "dist a: tensor([14.7710, 11.6351, 12.0537, 11.6931, 13.5764, 10.1048, 15.6176,  7.2411,\n",
            "        15.5097, 18.7778, 15.2546,  7.9260, 13.9051, 10.0937, 16.2919, 16.6440,\n",
            "        12.1072,  6.5336,  9.2363, 11.1532, 12.2798,  8.0875, 11.8983,  9.1442,\n",
            "         7.9024, 12.2026,  9.9448, 12.7233, 10.9588,  6.4682, 11.0988,  8.2143,\n",
            "        15.6352,  9.2746, 10.4584, 10.5614, 13.2419, 10.8695, 16.3750,  9.4506,\n",
            "        10.9973,  9.2961,  7.5053,  8.6054, 10.3883,  9.6428,  7.3150, 17.2029,\n",
            "         7.5562, 15.7905,  8.9681, 14.8180, 10.2818, 22.7009, 12.2917, 16.0588,\n",
            "        16.8906, 10.0146,  9.4154, 15.9959, 12.2810, 13.3009,  8.4748, 15.6334],\n",
            "       device='cuda:0'), dist b: tensor([20.4937, 21.0896, 11.7322, 12.3871, 15.1771,  9.0880, 11.3340, 12.6061,\n",
            "        11.1602, 13.5182,  9.6000, 23.4123, 21.6503, 21.6375,  9.4766, 13.8607,\n",
            "        10.3931, 13.7356,  9.2734, 14.6889, 11.0546, 17.5808, 15.2516, 16.3749,\n",
            "        20.0767, 19.5690, 12.3757, 19.3314, 19.8062, 15.4965, 12.8702, 20.8736,\n",
            "        18.9243, 14.8357, 16.8828, 17.8162, 21.4956, 11.8837, 10.2800, 11.4289,\n",
            "        13.8196, 11.9702, 16.3783, 12.6288, 20.9792, 13.0171, 14.4457, 20.6812,\n",
            "        17.0096, 10.7477,  9.6289, 10.6154,  9.5635, 18.4792, 18.5086, 19.7491,\n",
            "        20.4590, 15.8316, 11.3375, 12.0582, 14.5621, 15.9007, 11.6964, 12.2398],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "Training Batch: 401 | Model saved to: /content/drive/My Drive/test6/model_epoch_2_batch_401.pt\n",
            "mini Batch 402 Loss: 2.049966812133789\n",
            "mini Batch 403 Loss: 2.404953718185425\n",
            "mini Batch 404 Loss: 2.7097525596618652\n",
            "mini Batch 405 Loss: 1.9992234706878662\n",
            "mini Batch 406 Loss: 2.495997428894043\n",
            "mini Batch 407 Loss: 1.488412618637085\n",
            "mini Batch 408 Loss: 2.2565882205963135\n",
            "mini Batch 409 Loss: 2.1422626972198486\n",
            "mini Batch 410 Loss: 1.87615966796875\n",
            "mini Batch 411 Loss: 3.5412650108337402\n",
            "mini Batch 412 Loss: 2.1010854244232178\n",
            "mini Batch 413 Loss: 2.7211623191833496\n",
            "mini Batch 414 Loss: 2.796452045440674\n",
            "mini Batch 415 Loss: 3.452789306640625\n",
            "mini Batch 416 Loss: 2.546549081802368\n",
            "mini Batch 417 Loss: 3.508175849914551\n",
            "mini Batch 418 Loss: 2.882539749145508\n",
            "mini Batch 419 Loss: 1.6428961753845215\n",
            "mini Batch 420 Loss: 2.8918297290802\n",
            "mini Batch 421 Loss: 3.8748879432678223\n",
            "mini Batch 422 Loss: 2.1883015632629395\n",
            "mini Batch 423 Loss: 1.9920027256011963\n",
            "mini Batch 424 Loss: 2.6298999786376953\n",
            "mini Batch 425 Loss: 1.7667677402496338\n",
            "mini Batch 426 Loss: 1.8748531341552734\n",
            "mini Batch 427 Loss: 1.623814344406128\n",
            "mini Batch 428 Loss: 3.799440860748291\n",
            "mini Batch 429 Loss: 2.990410804748535\n",
            "mini Batch 430 Loss: 2.8522276878356934\n",
            "mini Batch 431 Loss: 2.6079015731811523\n",
            "mini Batch 432 Loss: 1.839803695678711\n",
            "mini Batch 433 Loss: 3.4428772926330566\n",
            "mini Batch 434 Loss: 3.1944050788879395\n",
            "mini Batch 435 Loss: 2.2209672927856445\n",
            "mini Batch 436 Loss: 1.3803086280822754\n",
            "mini Batch 437 Loss: 3.5475454330444336\n",
            "mini Batch 438 Loss: 1.1681314706802368\n",
            "mini Batch 439 Loss: 2.637223243713379\n",
            "mini Batch 440 Loss: 3.6891016960144043\n",
            "mini Batch 441 Loss: 2.432835340499878\n",
            "mini Batch 442 Loss: 2.01173734664917\n",
            "mini Batch 443 Loss: 0.5813618302345276\n",
            "mini Batch 444 Loss: 2.7043538093566895\n",
            "mini Batch 445 Loss: 2.331507682800293\n",
            "mini Batch 446 Loss: 1.8153573274612427\n",
            "mini Batch 447 Loss: 1.4561653137207031\n",
            "mini Batch 448 Loss: 0.702571451663971\n",
            "mini Batch 449 Loss: 2.1172242164611816\n",
            "mini Batch 450 Loss: 2.061923027038574\n",
            "mini Batch 451 Loss: 1.1200387477874756\n",
            "mini Batch 452 Loss: 2.2689273357391357\n",
            "mini Batch 453 Loss: 0.715978741645813\n",
            "mini Batch 454 Loss: 3.1702933311462402\n",
            "mini Batch 455 Loss: 1.2594921588897705\n",
            "mini Batch 456 Loss: 3.36474871635437\n",
            "mini Batch 457 Loss: 2.3686561584472656\n",
            "mini Batch 458 Loss: 1.6650581359863281\n",
            "mini Batch 459 Loss: 2.6424243450164795\n",
            "mini Batch 460 Loss: 1.3581266403198242\n",
            "mini Batch 461 Loss: 2.5547966957092285\n",
            "mini Batch 462 Loss: 3.066582202911377\n",
            "mini Batch 463 Loss: 2.0049192905426025\n",
            "mini Batch 464 Loss: 2.112229347229004\n",
            "mini Batch 465 Loss: 1.6756317615509033\n",
            "mini Batch 466 Loss: 3.165855884552002\n",
            "mini Batch 467 Loss: 2.094083786010742\n",
            "mini Batch 468 Loss: 1.5219285488128662\n",
            "mini Batch 469 Loss: 2.051840305328369\n",
            "mini Batch 470 Loss: 0.9609974026679993\n",
            "mini Batch 471 Loss: 3.9003467559814453\n",
            "mini Batch 472 Loss: 3.2555184364318848\n",
            "mini Batch 473 Loss: 1.9503381252288818\n",
            "mini Batch 474 Loss: 0.8147597312927246\n",
            "mini Batch 475 Loss: 1.2042365074157715\n",
            "mini Batch 476 Loss: 4.520587921142578\n",
            "mini Batch 477 Loss: 2.7287588119506836\n",
            "mini Batch 478 Loss: 2.626856803894043\n",
            "mini Batch 479 Loss: 3.48305344581604\n",
            "mini Batch 480 Loss: 2.8249619007110596\n",
            "mini Batch 481 Loss: 0.4423636794090271\n",
            "mini Batch 482 Loss: 0.21995237469673157\n",
            "mini Batch 483 Loss: 1.2360715866088867\n",
            "mini Batch 484 Loss: 2.239518165588379\n",
            "mini Batch 485 Loss: 1.844313621520996\n",
            "mini Batch 486 Loss: 2.276963710784912\n",
            "mini Batch 487 Loss: 1.0219448804855347\n",
            "mini Batch 488 Loss: 2.601947069168091\n",
            "mini Batch 489 Loss: 2.379047393798828\n",
            "mini Batch 490 Loss: 3.35369873046875\n",
            "mini Batch 491 Loss: 2.037841796875\n",
            "mini Batch 492 Loss: 1.3327937126159668\n",
            "mini Batch 493 Loss: 1.7617747783660889\n",
            "mini Batch 494 Loss: 3.8600215911865234\n",
            "mini Batch 495 Loss: 2.614412784576416\n",
            "mini Batch 496 Loss: 2.1656980514526367\n",
            "mini Batch 497 Loss: 1.4940487146377563\n",
            "mini Batch 498 Loss: 3.281464099884033\n",
            "mini Batch 499 Loss: 2.7284703254699707\n",
            "mini Batch 500 Loss: 1.8416434526443481\n",
            "mini Batch 501 Loss: 0.52679443359375\n",
            "mini Batch 502 Loss: 3.6928677558898926\n",
            "mini Batch 503 Loss: 3.8078691959381104\n",
            "mini Batch 504 Loss: 2.67107892036438\n",
            "mini Batch 505 Loss: 3.058475971221924\n",
            "mini Batch 506 Loss: 2.2455523014068604\n",
            "mini Batch 507 Loss: 3.1269590854644775\n",
            "mini Batch 508 Loss: 2.77406644821167\n",
            "mini Batch 509 Loss: 2.9657163619995117\n",
            "mini Batch 510 Loss: 2.5175905227661133\n",
            "mini Batch 511 Loss: 2.842423677444458\n",
            "mini Batch 512 Loss: 4.94985294342041\n",
            "mini Batch 513 Loss: 3.8592019081115723\n",
            "mini Batch 514 Loss: 3.849771022796631\n",
            "mini Batch 515 Loss: 3.9858686923980713\n",
            "mini Batch 516 Loss: 2.557591438293457\n",
            "mini Batch 517 Loss: 2.3220176696777344\n",
            "mini Batch 518 Loss: 2.5377790927886963\n",
            "mini Batch 519 Loss: 3.268195152282715\n",
            "mini Batch 520 Loss: 1.7203785181045532\n",
            "mini Batch 521 Loss: 2.0964691638946533\n",
            "mini Batch 522 Loss: 2.548156499862671\n",
            "mini Batch 523 Loss: 0.9348384141921997\n",
            "mini Batch 524 Loss: 2.0709195137023926\n",
            "mini Batch 525 Loss: 2.226100444793701\n",
            "mini Batch 526 Loss: 3.416743040084839\n",
            "mini Batch 527 Loss: 1.2306106090545654\n",
            "mini Batch 528 Loss: 1.5549952983856201\n",
            "mini Batch 529 Loss: 1.719588041305542\n",
            "mini Batch 530 Loss: 2.5830631256103516\n",
            "mini Batch 531 Loss: 1.6734561920166016\n",
            "mini Batch 532 Loss: 1.8560163974761963\n",
            "mini Batch 533 Loss: 2.3218462467193604\n",
            "mini Batch 534 Loss: 4.160417556762695\n",
            "mini Batch 535 Loss: 1.2751513719558716\n",
            "mini Batch 536 Loss: 2.0560784339904785\n",
            "mini Batch 537 Loss: 0.48320668935775757\n",
            "mini Batch 538 Loss: 1.9830461740493774\n",
            "mini Batch 539 Loss: 2.4497532844543457\n",
            "mini Batch 540 Loss: 1.948238730430603\n",
            "mini Batch 541 Loss: 1.1780891418457031\n",
            "mini Batch 542 Loss: 1.1879428625106812\n",
            "mini Batch 543 Loss: 1.7469207048416138\n",
            "mini Batch 544 Loss: 0.1493004411458969\n",
            "mini Batch 545 Loss: 2.9309749603271484\n",
            "mini Batch 546 Loss: 3.770183801651001\n",
            "mini Batch 547 Loss: 3.7189769744873047\n",
            "mini Batch 548 Loss: 2.1557512283325195\n",
            "mini Batch 549 Loss: 1.7599663734436035\n",
            "mini Batch 550 Loss: 2.052353858947754\n",
            "mini Batch 551 Loss: 6.827028274536133\n",
            "mini Batch 552 Loss: 3.349782943725586\n",
            "mini Batch 553 Loss: 2.523238182067871\n",
            "mini Batch 554 Loss: 3.7440907955169678\n",
            "mini Batch 555 Loss: 3.761867046356201\n",
            "mini Batch 556 Loss: 2.544520854949951\n",
            "mini Batch 557 Loss: 1.9703776836395264\n",
            "mini Batch 558 Loss: 2.356358289718628\n",
            "mini Batch 559 Loss: 1.5078113079071045\n",
            "mini Batch 560 Loss: 1.0884904861450195\n",
            "mini Batch 561 Loss: 1.5586950778961182\n",
            "mini Batch 562 Loss: 0.784412145614624\n",
            "mini Batch 563 Loss: 1.8563863039016724\n",
            "mini Batch 564 Loss: 1.5346148014068604\n",
            "mini Batch 565 Loss: 1.351173996925354\n",
            "mini Batch 566 Loss: 2.794334888458252\n",
            "mini Batch 567 Loss: 2.243206024169922\n",
            "mini Batch 568 Loss: 0.6431894302368164\n",
            "mini Batch 569 Loss: 2.64522385597229\n",
            "mini Batch 570 Loss: 2.6527891159057617\n",
            "mini Batch 571 Loss: 0.9630063772201538\n",
            "mini Batch 572 Loss: 1.0567705631256104\n",
            "mini Batch 573 Loss: 1.1336021423339844\n",
            "mini Batch 574 Loss: 1.3744561672210693\n",
            "mini Batch 575 Loss: 1.5887553691864014\n",
            "mini Batch 576 Loss: 1.2913004159927368\n",
            "mini Batch 577 Loss: 1.206308364868164\n",
            "mini Batch 578 Loss: 3.0031657218933105\n",
            "mini Batch 579 Loss: 0.6806767582893372\n",
            "mini Batch 580 Loss: 2.244536876678467\n",
            "mini Batch 581 Loss: 2.972147226333618\n",
            "mini Batch 582 Loss: 1.9169142246246338\n",
            "mini Batch 583 Loss: 0.5831413865089417\n",
            "mini Batch 584 Loss: 1.3851337432861328\n",
            "mini Batch 585 Loss: 2.509897232055664\n",
            "mini Batch 586 Loss: 3.565127372741699\n",
            "mini Batch 587 Loss: 1.9852862358093262\n",
            "mini Batch 588 Loss: 4.858640193939209\n",
            "mini Batch 589 Loss: 1.1279629468917847\n",
            "mini Batch 590 Loss: 1.1332451105117798\n",
            "mini Batch 591 Loss: 0.3556424677371979\n",
            "mini Batch 592 Loss: 3.473540782928467\n",
            "mini Batch 593 Loss: 0.8891241550445557\n",
            "mini Batch 594 Loss: 0.8203111886978149\n",
            "mini Batch 595 Loss: 2.728851318359375\n",
            "mini Batch 596 Loss: 1.500281810760498\n",
            "mini Batch 597 Loss: 0.878149151802063\n",
            "mini Batch 598 Loss: 3.037433624267578\n",
            "mini Batch 599 Loss: 2.3277125358581543\n",
            "mini Batch 600 Loss: 2.00736141204834\n",
            "mini Batch 601 Loss: 2.0466346740722656\n",
            "Training Batch: 601 | Training Loss: 2.0466346740722656\n",
            "dist a: tensor([10.2428,  9.0956,  8.7577,  9.9344, 16.9463,  8.2913, 13.5230, 14.0577,\n",
            "        11.8843, 14.0054, 11.1096, 11.2611, 11.4494,  8.5152, 11.4492, 15.8751,\n",
            "        18.3513, 13.5371,  7.2439,  9.7446,  9.6149, 14.0971, 18.7954, 15.2549,\n",
            "        15.8105, 15.4002, 15.2737, 11.4376, 12.1518, 12.4058, 17.6637, 13.6845,\n",
            "        12.6688, 16.2986, 12.7422, 11.8777, 23.1227,  8.7834, 17.8468, 14.3470,\n",
            "        11.0677, 12.1988,  7.7970,  9.1906, 12.8216, 15.2610, 11.3143, 11.7273,\n",
            "        10.2773, 16.9965, 11.6798,  7.2195, 15.8357, 14.0865, 12.5886, 13.3901,\n",
            "        10.8244,  9.3146,  9.3050, 15.8025, 10.2622, 12.0195,  9.6078, 12.0080],\n",
            "       device='cuda:0'), dist b: tensor([14.3530, 18.8671, 10.8724, 16.3281, 20.7268, 17.0584, 13.9048, 13.4073,\n",
            "        11.0469, 20.5994, 18.5005, 19.7902, 20.0626, 22.1646, 11.2586, 17.2616,\n",
            "         9.9724, 19.1568, 11.8341, 20.5155, 12.8860, 14.4301, 11.7131, 16.8106,\n",
            "        14.8692,  8.7945, 16.3223,  8.7020, 14.4069, 15.3128, 15.1075, 21.9031,\n",
            "        19.9717, 15.6868, 14.7330, 10.0881, 17.0371, 23.0371, 13.8793, 17.4663,\n",
            "        16.0385, 18.2703, 16.7796, 16.4188, 17.1347, 15.4276, 24.2304, 18.5183,\n",
            "        14.4661, 17.9759,  9.5236, 12.6502, 15.2673, 13.2423, 13.8208, 14.4632,\n",
            "        21.6695, 20.3880,  7.8854, 11.0642,  7.8211, 16.4762, 11.9726, 18.7286],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.703125 \n",
            "tensor(0.7031)\n",
            "Training Batch: 601 | Model saved to: /content/drive/My Drive/test6/model_epoch_2_batch_601.pt\n",
            "mini Batch 602 Loss: 3.4303739070892334\n",
            "mini Batch 603 Loss: 1.1684705018997192\n",
            "mini Batch 604 Loss: 0.9031144976615906\n",
            "mini Batch 605 Loss: 1.2940075397491455\n",
            "mini Batch 606 Loss: 0.7570974230766296\n",
            "mini Batch 607 Loss: 1.4960827827453613\n",
            "mini Batch 608 Loss: 0.7604336738586426\n",
            "mini Batch 609 Loss: 3.9948151111602783\n",
            "mini Batch 610 Loss: 0.6581010818481445\n",
            "mini Batch 611 Loss: 2.5400397777557373\n",
            "mini Batch 612 Loss: 1.4609854221343994\n",
            "mini Batch 613 Loss: 2.522139072418213\n",
            "mini Batch 614 Loss: 3.4817328453063965\n",
            "mini Batch 615 Loss: 3.6362195014953613\n",
            "mini Batch 616 Loss: 2.7149605751037598\n",
            "mini Batch 617 Loss: 2.461261510848999\n",
            "mini Batch 618 Loss: 3.1373181343078613\n",
            "mini Batch 619 Loss: 4.409317970275879\n",
            "mini Batch 620 Loss: 2.498063564300537\n",
            "mini Batch 621 Loss: 2.6875646114349365\n",
            "mini Batch 622 Loss: 2.3776628971099854\n",
            "mini Batch 623 Loss: 3.17018461227417\n",
            "mini Batch 624 Loss: 3.704345703125\n",
            "mini Batch 625 Loss: 2.7674410343170166\n",
            "mini Batch 626 Loss: 2.675321102142334\n",
            "mini Batch 627 Loss: 3.385406494140625\n",
            "mini Batch 628 Loss: 1.762223482131958\n",
            "mini Batch 629 Loss: 4.070940017700195\n",
            "mini Batch 630 Loss: 2.405904769897461\n",
            "mini Batch 631 Loss: 2.4162099361419678\n",
            "mini Batch 632 Loss: 2.536315441131592\n",
            "mini Batch 633 Loss: 3.8909835815429688\n",
            "mini Batch 634 Loss: 3.923567056655884\n",
            "mini Batch 635 Loss: 2.896872043609619\n",
            "mini Batch 636 Loss: 2.653592109680176\n",
            "mini Batch 637 Loss: 1.9488369226455688\n",
            "mini Batch 638 Loss: 1.7733526229858398\n",
            "mini Batch 639 Loss: 1.7482187747955322\n",
            "mini Batch 640 Loss: 1.5551762580871582\n",
            "mini Batch 641 Loss: 2.2043800354003906\n",
            "mini Batch 642 Loss: 2.214383602142334\n",
            "mini Batch 643 Loss: 1.8935863971710205\n",
            "mini Batch 644 Loss: 2.873013973236084\n",
            "mini Batch 645 Loss: 3.348482131958008\n",
            "mini Batch 646 Loss: 1.7369633913040161\n",
            "mini Batch 647 Loss: 2.792985200881958\n",
            "mini Batch 648 Loss: 0.5507742762565613\n",
            "mini Batch 649 Loss: 3.512207508087158\n",
            "mini Batch 650 Loss: 2.1293725967407227\n",
            "mini Batch 651 Loss: 1.8313891887664795\n",
            "mini Batch 652 Loss: 1.6327202320098877\n",
            "mini Batch 653 Loss: 1.4344435930252075\n",
            "mini Batch 654 Loss: 2.4862051010131836\n",
            "mini Batch 655 Loss: 1.278795599937439\n",
            "mini Batch 656 Loss: 1.4787131547927856\n",
            "mini Batch 657 Loss: 1.4301936626434326\n",
            "mini Batch 658 Loss: 0.2960413694381714\n",
            "mini Batch 659 Loss: 2.4335227012634277\n",
            "mini Batch 660 Loss: 2.192838191986084\n",
            "mini Batch 661 Loss: 2.5315024852752686\n",
            "mini Batch 662 Loss: 1.155184268951416\n",
            "mini Batch 663 Loss: 0.3066926598548889\n",
            "mini Batch 664 Loss: 3.902761936187744\n",
            "mini Batch 665 Loss: 3.481435775756836\n",
            "mini Batch 666 Loss: 4.145164966583252\n",
            "mini Batch 667 Loss: 3.770451784133911\n",
            "mini Batch 668 Loss: 3.2492709159851074\n",
            "mini Batch 669 Loss: 0.6246854662895203\n",
            "mini Batch 670 Loss: 1.6767613887786865\n",
            "mini Batch 671 Loss: 2.900796890258789\n",
            "mini Batch 672 Loss: 2.6016526222229004\n",
            "mini Batch 673 Loss: 1.7118561267852783\n",
            "mini Batch 674 Loss: 1.2945749759674072\n",
            "mini Batch 675 Loss: 3.862272262573242\n",
            "mini Batch 676 Loss: 3.27618408203125\n",
            "mini Batch 677 Loss: 3.399014472961426\n",
            "mini Batch 678 Loss: 3.715223789215088\n",
            "mini Batch 679 Loss: 2.6964926719665527\n",
            "mini Batch 680 Loss: 2.1909260749816895\n",
            "mini Batch 681 Loss: 1.8273446559906006\n",
            "mini Batch 682 Loss: 2.3185348510742188\n",
            "mini Batch 683 Loss: 2.336224317550659\n",
            "mini Batch 684 Loss: 1.7913469076156616\n",
            "mini Batch 685 Loss: 3.016582727432251\n",
            "mini Batch 686 Loss: 2.365035057067871\n",
            "mini Batch 687 Loss: 0.7386802434921265\n",
            "mini Batch 688 Loss: 2.7299516201019287\n",
            "mini Batch 689 Loss: 1.8850451707839966\n",
            "mini Batch 690 Loss: 3.2654995918273926\n",
            "mini Batch 691 Loss: 0.3565216064453125\n",
            "mini Batch 692 Loss: 1.4677186012268066\n",
            "mini Batch 693 Loss: 1.0650465488433838\n",
            "mini Batch 694 Loss: 0.2161451131105423\n",
            "mini Batch 695 Loss: 3.8161683082580566\n",
            "mini Batch 696 Loss: 1.3813928365707397\n",
            "mini Batch 697 Loss: 1.7133698463439941\n",
            "mini Batch 698 Loss: 3.6213879585266113\n",
            "mini Batch 699 Loss: 1.0121212005615234\n",
            "mini Batch 700 Loss: 3.067171573638916\n",
            "mini Batch 701 Loss: 2.0279884338378906\n",
            "mini Batch 702 Loss: 1.9804267883300781\n",
            "mini Batch 703 Loss: 1.6661672592163086\n",
            "mini Batch 704 Loss: 0.470487117767334\n",
            "mini Batch 705 Loss: 1.042802333831787\n",
            "mini Batch 706 Loss: 2.129901885986328\n",
            "mini Batch 707 Loss: 3.445594072341919\n",
            "mini Batch 708 Loss: 2.0414981842041016\n",
            "mini Batch 709 Loss: 2.0988378524780273\n",
            "mini Batch 710 Loss: 1.4260821342468262\n",
            "mini Batch 711 Loss: 2.794614315032959\n",
            "mini Batch 712 Loss: 2.261029005050659\n",
            "mini Batch 713 Loss: 3.2158725261688232\n",
            "mini Batch 714 Loss: 0.8504030108451843\n",
            "mini Batch 715 Loss: 1.5338773727416992\n",
            "mini Batch 716 Loss: 1.3466525077819824\n",
            "mini Batch 717 Loss: 2.1644439697265625\n",
            "mini Batch 718 Loss: 1.0941250324249268\n",
            "mini Batch 719 Loss: 1.8607639074325562\n",
            "mini Batch 720 Loss: 1.8453313112258911\n",
            "mini Batch 721 Loss: 1.5614075660705566\n",
            "mini Batch 722 Loss: 0.2425222247838974\n",
            "mini Batch 723 Loss: 1.3542466163635254\n",
            "mini Batch 724 Loss: 3.3759374618530273\n",
            "mini Batch 725 Loss: 1.004469871520996\n",
            "mini Batch 726 Loss: 3.058983564376831\n",
            "mini Batch 727 Loss: 1.1226649284362793\n",
            "mini Batch 728 Loss: 0.2115340679883957\n",
            "mini Batch 729 Loss: 3.623248338699341\n",
            "mini Batch 730 Loss: 0.09831918776035309\n",
            "mini Batch 731 Loss: 0.1023859977722168\n",
            "mini Batch 732 Loss: 0.5194622278213501\n",
            "mini Batch 733 Loss: 0.5616205930709839\n",
            "mini Batch 734 Loss: 2.1635794639587402\n",
            "mini Batch 735 Loss: 0.2332342565059662\n",
            "mini Batch 736 Loss: 0.41953131556510925\n",
            "mini Batch 737 Loss: 2.3010923862457275\n",
            "mini Batch 738 Loss: 0.1133037656545639\n",
            "mini Batch 739 Loss: 0.7494556903839111\n",
            "mini Batch 740 Loss: 0.4787038564682007\n",
            "mini Batch 741 Loss: 0.2917487919330597\n",
            "mini Batch 742 Loss: 2.8571159839630127\n",
            "mini Batch 743 Loss: 2.998631000518799\n",
            "mini Batch 744 Loss: 1.6486258506774902\n",
            "mini Batch 745 Loss: 2.367326021194458\n",
            "mini Batch 746 Loss: 3.7259488105773926\n",
            "mini Batch 747 Loss: 4.099343299865723\n",
            "mini Batch 748 Loss: 2.7628791332244873\n",
            "mini Batch 749 Loss: 4.141095161437988\n",
            "mini Batch 750 Loss: 3.7330262660980225\n",
            "mini Batch 751 Loss: 4.249314308166504\n",
            "mini Batch 752 Loss: 2.6034226417541504\n",
            "mini Batch 753 Loss: 1.3472208976745605\n",
            "mini Batch 754 Loss: 1.2821274995803833\n",
            "mini Batch 755 Loss: 1.2967619895935059\n",
            "mini Batch 756 Loss: 0.4837020933628082\n",
            "mini Batch 757 Loss: 3.2984113693237305\n",
            "mini Batch 758 Loss: 1.92353093624115\n",
            "mini Batch 759 Loss: 4.346443176269531\n",
            "mini Batch 760 Loss: 1.1049622297286987\n",
            "mini Batch 761 Loss: 0.9248001575469971\n",
            "mini Batch 762 Loss: 0.4378938674926758\n",
            "mini Batch 763 Loss: 1.249875545501709\n",
            "mini Batch 764 Loss: 1.6670870780944824\n",
            "mini Batch 765 Loss: 2.5710885524749756\n",
            "mini Batch 766 Loss: 2.728550672531128\n",
            "mini Batch 767 Loss: 3.012692928314209\n",
            "mini Batch 768 Loss: 2.9974708557128906\n",
            "mini Batch 769 Loss: 3.1369428634643555\n",
            "mini Batch 770 Loss: 4.003783702850342\n",
            "mini Batch 771 Loss: 2.1479759216308594\n",
            "mini Batch 772 Loss: 2.152529716491699\n",
            "mini Batch 773 Loss: 0.7652570009231567\n",
            "mini Batch 774 Loss: 0.30777111649513245\n",
            "mini Batch 775 Loss: 3.994509696960449\n",
            "mini Batch 776 Loss: 7.029247283935547\n",
            "mini Batch 777 Loss: 3.5833230018615723\n",
            "mini Batch 778 Loss: 1.7519197463989258\n",
            "mini Batch 779 Loss: 2.1164441108703613\n",
            "mini Batch 780 Loss: 3.8223278522491455\n",
            "mini Batch 781 Loss: 2.673915147781372\n",
            "mini Batch 782 Loss: 1.8825361728668213\n",
            "mini Batch 783 Loss: 1.3619911670684814\n",
            "mini Batch 784 Loss: 1.6911121606826782\n",
            "mini Batch 785 Loss: 1.4822862148284912\n",
            "mini Batch 786 Loss: 3.2758145332336426\n",
            "mini Batch 787 Loss: 2.3108582496643066\n",
            "mini Batch 788 Loss: 1.9721436500549316\n",
            "mini Batch 789 Loss: 2.3532791137695312\n",
            "mini Batch 790 Loss: 3.197474241256714\n",
            "mini Batch 791 Loss: 1.0264174938201904\n",
            "mini Batch 792 Loss: 1.0104610919952393\n",
            "mini Batch 793 Loss: 3.2499217987060547\n",
            "mini Batch 794 Loss: 0.31592679023742676\n",
            "mini Batch 795 Loss: 0.09598389267921448\n",
            "mini Batch 796 Loss: 0.17533573508262634\n",
            "mini Batch 797 Loss: 2.226952075958252\n",
            "mini Batch 798 Loss: 1.484139323234558\n",
            "mini Batch 799 Loss: 2.2710225582122803\n",
            "mini Batch 800 Loss: 1.0955662727355957\n",
            "mini Batch 801 Loss: 1.8078978061676025\n",
            "Training Batch: 801 | Training Loss: 1.8078978061676025\n",
            "dist a: tensor([13.2095, 10.2481, 15.1609, 15.4923, 25.8173, 17.3441, 12.9165, 14.1588,\n",
            "        12.6319, 14.4508, 12.9329, 15.2542, 19.5524, 17.5550, 19.5420, 13.4373,\n",
            "        14.4429, 13.1717, 12.6255, 24.4420, 11.6824, 20.1419, 12.0813, 13.7081,\n",
            "        10.9011, 13.7458, 12.6960, 10.0033, 13.7067,  7.6648, 16.9669, 11.2496,\n",
            "        12.6224, 12.1908, 15.6205, 15.5735, 19.4337, 20.4882, 11.5015, 18.1751,\n",
            "        11.8741, 18.8626, 10.3186, 11.8745, 24.2218,  8.9169, 12.6195, 10.7913,\n",
            "        15.9633, 12.0940, 10.3014, 14.1610, 18.0997, 13.7141, 18.0197, 18.0925,\n",
            "        13.2547, 11.4232, 17.7514, 17.4080, 11.3178, 19.5831, 14.4094, 15.8378],\n",
            "       device='cuda:0'), dist b: tensor([13.2254, 16.4181, 14.8628, 16.5385, 21.5053, 21.1195, 23.1855, 14.5893,\n",
            "        18.3531, 19.4643, 21.7501, 17.9879, 22.5462, 22.5845, 19.5889, 17.4079,\n",
            "        20.1799,  8.1586, 12.0970, 11.8560, 19.5452, 14.9965, 17.6860, 16.9489,\n",
            "        10.5998, 21.6161, 17.3151, 18.9400, 17.4155, 18.1928, 10.9251, 17.6076,\n",
            "        17.3607, 15.7199, 17.2813, 12.7258, 18.5552, 18.7255, 20.4681, 19.9025,\n",
            "        12.6807, 12.5998,  9.4989, 22.1907, 17.9734, 19.3698, 19.3917, 18.6013,\n",
            "        13.4310, 11.2108, 15.7614, 16.4571, 21.0111, 19.1666, 17.3288, 19.2047,\n",
            "        17.7948, 24.9182, 12.8917, 18.8989, 19.9985, 16.2734, 15.7342, 20.6677],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.703125 \n",
            "tensor(0.7031)\n",
            "Training Batch: 801 | Model saved to: /content/drive/My Drive/test6/model_epoch_2_batch_801.pt\n",
            "mini Batch 802 Loss: 1.1392687559127808\n",
            "mini Batch 803 Loss: 3.288407325744629\n",
            "mini Batch 804 Loss: 1.3718535900115967\n",
            "mini Batch 805 Loss: 0.32764238119125366\n",
            "mini Batch 806 Loss: 3.922633647918701\n",
            "mini Batch 807 Loss: 3.860006332397461\n",
            "mini Batch 808 Loss: 2.8079588413238525\n",
            "mini Batch 809 Loss: 1.9924691915512085\n",
            "mini Batch 810 Loss: 3.1198410987854004\n",
            "mini Batch 811 Loss: 0.529104471206665\n",
            "mini Batch 812 Loss: 2.4130592346191406\n",
            "mini Batch 813 Loss: 2.1721601486206055\n",
            "mini Batch 814 Loss: 1.448433756828308\n",
            "mini Batch 815 Loss: 1.7059859037399292\n",
            "mini Batch 816 Loss: 1.0803391933441162\n",
            "mini Batch 817 Loss: 1.4148749113082886\n",
            "mini Batch 818 Loss: 3.229344129562378\n",
            "mini Batch 819 Loss: 3.039541006088257\n",
            "mini Batch 820 Loss: 3.0909242630004883\n",
            "mini Batch 821 Loss: 2.90692400932312\n",
            "mini Batch 822 Loss: 0.5595266222953796\n",
            "mini Batch 823 Loss: 5.227084159851074\n",
            "mini Batch 824 Loss: 4.0575408935546875\n",
            "mini Batch 825 Loss: 3.989853858947754\n",
            "mini Batch 826 Loss: 4.432598114013672\n",
            "mini Batch 827 Loss: 2.683732509613037\n",
            "mini Batch 828 Loss: 3.6244044303894043\n",
            "mini Batch 829 Loss: 2.4165842533111572\n",
            "mini Batch 830 Loss: 4.521815776824951\n",
            "mini Batch 831 Loss: 4.783395767211914\n",
            "mini Batch 832 Loss: 4.284660816192627\n",
            "mini Batch 833 Loss: 3.64048433303833\n",
            "mini Batch 834 Loss: 4.064761638641357\n",
            "mini Batch 835 Loss: 3.7575087547302246\n",
            "mini Batch 836 Loss: 3.22318172454834\n",
            "mini Batch 837 Loss: 3.9829366207122803\n",
            "mini Batch 838 Loss: 3.58791446685791\n",
            "mini Batch 839 Loss: 3.1039435863494873\n",
            "mini Batch 840 Loss: 2.6015448570251465\n",
            "mini Batch 841 Loss: 3.4319543838500977\n",
            "mini Batch 842 Loss: 2.1899309158325195\n",
            "mini Batch 843 Loss: 2.606088161468506\n",
            "mini Batch 844 Loss: 3.693620204925537\n",
            "mini Batch 845 Loss: 2.391127109527588\n",
            "mini Batch 846 Loss: 3.403812885284424\n",
            "mini Batch 847 Loss: 1.9301681518554688\n",
            "mini Batch 848 Loss: 3.6903958320617676\n",
            "mini Batch 849 Loss: 4.05846643447876\n",
            "mini Batch 850 Loss: 2.4364264011383057\n",
            "mini Batch 851 Loss: 2.830676794052124\n",
            "mini Batch 852 Loss: 3.2897708415985107\n",
            "mini Batch 853 Loss: 3.342569351196289\n",
            "mini Batch 854 Loss: 2.8323354721069336\n",
            "mini Batch 855 Loss: 1.8361746072769165\n",
            "mini Batch 856 Loss: 2.581977367401123\n",
            "mini Batch 857 Loss: 3.2958788871765137\n",
            "mini Batch 858 Loss: 2.922302722930908\n",
            "mini Batch 859 Loss: 3.3724567890167236\n",
            "mini Batch 860 Loss: 3.3662045001983643\n",
            "mini Batch 861 Loss: 2.7349696159362793\n",
            "mini Batch 862 Loss: 3.8252596855163574\n",
            "mini Batch 863 Loss: 3.064760684967041\n",
            "mini Batch 864 Loss: 3.9380335807800293\n",
            "mini Batch 865 Loss: 2.863471508026123\n",
            "mini Batch 866 Loss: 3.070037364959717\n",
            "mini Batch 867 Loss: 3.2782013416290283\n",
            "mini Batch 868 Loss: 3.0268993377685547\n",
            "mini Batch 869 Loss: 2.293747663497925\n",
            "mini Batch 870 Loss: 2.7996816635131836\n",
            "mini Batch 871 Loss: 2.3966259956359863\n",
            "mini Batch 872 Loss: 2.4701647758483887\n",
            "mini Batch 873 Loss: 2.6189815998077393\n",
            "mini Batch 874 Loss: 2.5338075160980225\n",
            "mini Batch 875 Loss: 3.225796699523926\n",
            "mini Batch 876 Loss: 3.388728141784668\n",
            "mini Batch 877 Loss: 2.3756394386291504\n",
            "mini Batch 878 Loss: 2.288006544113159\n",
            "mini Batch 879 Loss: 3.214937925338745\n",
            "mini Batch 880 Loss: 2.8707146644592285\n",
            "mini Batch 881 Loss: 3.0944838523864746\n",
            "mini Batch 882 Loss: 2.4655354022979736\n",
            "mini Batch 883 Loss: 2.300513744354248\n",
            "mini Batch 884 Loss: 2.211426258087158\n",
            "mini Batch 885 Loss: 2.6297459602355957\n",
            "mini Batch 886 Loss: 2.2363243103027344\n",
            "mini Batch 887 Loss: 4.040462017059326\n",
            "mini Batch 888 Loss: 3.8777263164520264\n",
            "mini Batch 889 Loss: 2.6465015411376953\n",
            "mini Batch 890 Loss: 3.3883824348449707\n",
            "mini Batch 891 Loss: 2.1265816688537598\n",
            "mini Batch 892 Loss: 1.665126085281372\n",
            "mini Batch 893 Loss: 2.4826550483703613\n",
            "mini Batch 894 Loss: 2.800734519958496\n",
            "mini Batch 895 Loss: 1.9677881002426147\n",
            "mini Batch 896 Loss: 2.9898664951324463\n",
            "mini Batch 897 Loss: 1.8886239528656006\n",
            "mini Batch 898 Loss: 3.359069347381592\n",
            "mini Batch 899 Loss: 2.207082748413086\n",
            "mini Batch 900 Loss: 2.4480743408203125\n",
            "mini Batch 901 Loss: 2.38856840133667\n",
            "mini Batch 902 Loss: 2.680814743041992\n",
            "mini Batch 903 Loss: 2.242978572845459\n",
            "mini Batch 904 Loss: 1.585066795349121\n",
            "mini Batch 905 Loss: 2.3196821212768555\n",
            "mini Batch 906 Loss: 1.6732878684997559\n",
            "mini Batch 907 Loss: 2.3219945430755615\n",
            "mini Batch 908 Loss: 1.6074968576431274\n",
            "mini Batch 909 Loss: 3.6315016746520996\n",
            "mini Batch 910 Loss: 1.745736837387085\n",
            "mini Batch 911 Loss: 2.241323232650757\n",
            "mini Batch 912 Loss: 2.3888282775878906\n",
            "mini Batch 913 Loss: 2.0521786212921143\n",
            "mini Batch 914 Loss: 2.3703973293304443\n",
            "mini Batch 915 Loss: 1.5694879293441772\n",
            "mini Batch 916 Loss: 1.3804478645324707\n",
            "mini Batch 917 Loss: 1.8570796251296997\n",
            "mini Batch 918 Loss: 1.1158256530761719\n",
            "mini Batch 919 Loss: 0.7767279744148254\n",
            "mini Batch 920 Loss: 2.5045275688171387\n",
            "mini Batch 921 Loss: 2.096630811691284\n",
            "[2] average loss per epoch: 2.482\n",
            "Saved model checkpoint to /content/drive/My Drive/test6/model_epoch2.pt\n",
            "dist a: tensor([16.1418, 11.0411, 15.7126, 11.8793, 12.7136, 16.3551], device='cuda:0'), dist b: tensor([15.9286, 25.2985, 13.0000, 21.3601, 10.9351, 13.8500], device='cuda:0')\n",
            "random batch accuracy: 0.3333333432674408 \n",
            "tensor(0.3333)\n",
            "================== START PREDICTION ==================\n",
            "batch id predict after epoch:  1\n",
            "batch id predict after epoch:  2\n",
            "batch id predict after epoch:  3\n",
            "batch id predict after epoch:  4\n",
            "batch id predict after epoch:  5\n",
            "batch id predict after epoch:  6\n",
            "batch id predict after epoch:  7\n",
            "batch id predict after epoch:  8\n",
            "batch id predict after epoch:  9\n",
            "batch id predict after epoch:  10\n",
            "batch id predict after epoch:  11\n",
            "batch id predict after epoch:  12\n",
            "batch id predict after epoch:  13\n",
            "batch id predict after epoch:  14\n",
            "batch id predict after epoch:  15\n",
            "batch id predict after epoch:  16\n",
            "batch id predict after epoch:  17\n",
            "batch id predict after epoch:  18\n",
            "batch id predict after epoch:  19\n",
            "batch id predict after epoch:  20\n",
            "batch id predict after epoch:  21\n",
            "batch id predict after epoch:  22\n",
            "batch id predict after epoch:  23\n",
            "batch id predict after epoch:  24\n",
            "batch id predict after epoch:  25\n",
            "batch id predict after epoch:  26\n",
            "batch id predict after epoch:  27\n",
            "batch id predict after epoch:  28\n",
            "batch id predict after epoch:  29\n",
            "batch id predict after epoch:  30\n",
            "batch id predict after epoch:  31\n",
            "batch id predict after epoch:  32\n",
            "batch id predict after epoch:  33\n",
            "batch id predict after epoch:  34\n",
            "batch id predict after epoch:  35\n",
            "batch id predict after epoch:  36\n",
            "batch id predict after epoch:  37\n",
            "batch id predict after epoch:  38\n",
            "batch id predict after epoch:  39\n",
            "batch id predict after epoch:  40\n",
            "batch id predict after epoch:  41\n",
            "batch id predict after epoch:  42\n",
            "batch id predict after epoch:  43\n",
            "batch id predict after epoch:  44\n",
            "batch id predict after epoch:  45\n",
            "batch id predict after epoch:  46\n",
            "batch id predict after epoch:  47\n",
            "batch id predict after epoch:  48\n",
            "batch id predict after epoch:  49\n",
            "batch id predict after epoch:  50\n",
            "batch id predict after epoch:  51\n",
            "batch id predict after epoch:  52\n",
            "batch id predict after epoch:  53\n",
            "batch id predict after epoch:  54\n",
            "batch id predict after epoch:  55\n",
            "batch id predict after epoch:  56\n",
            "batch id predict after epoch:  57\n",
            "batch id predict after epoch:  58\n",
            "batch id predict after epoch:  59\n",
            "batch id predict after epoch:  60\n",
            "batch id predict after epoch:  61\n",
            "batch id predict after epoch:  62\n",
            "batch id predict after epoch:  63\n",
            "batch id predict after epoch:  64\n",
            "batch id predict after epoch:  65\n",
            "batch id predict after epoch:  66\n",
            "batch id predict after epoch:  67\n",
            "batch id predict after epoch:  68\n",
            "batch id predict after epoch:  69\n",
            "batch id predict after epoch:  70\n",
            "batch id predict after epoch:  71\n",
            "batch id predict after epoch:  72\n",
            "batch id predict after epoch:  73\n",
            "batch id predict after epoch:  74\n",
            "batch id predict after epoch:  75\n",
            "batch id predict after epoch:  76\n",
            "batch id predict after epoch:  77\n",
            "batch id predict after epoch:  78\n",
            "batch id predict after epoch:  79\n",
            "batch id predict after epoch:  80\n",
            "batch id predict after epoch:  81\n",
            "batch id predict after epoch:  82\n",
            "batch id predict after epoch:  83\n",
            "batch id predict after epoch:  84\n",
            "batch id predict after epoch:  85\n",
            "batch id predict after epoch:  86\n",
            "batch id predict after epoch:  87\n",
            "batch id predict after epoch:  88\n",
            "batch id predict after epoch:  89\n",
            "batch id predict after epoch:  90\n",
            "batch id predict after epoch:  91\n",
            "batch id predict after epoch:  92\n",
            "batch id predict after epoch:  93\n",
            "batch id predict after epoch:  94\n",
            "batch id predict after epoch:  95\n",
            "batch id predict after epoch:  96\n",
            "batch id predict after epoch:  97\n",
            "batch id predict after epoch:  98\n",
            "batch id predict after epoch:  99\n",
            "batch id predict after epoch:  100\n",
            "batch id predict after epoch:  101\n",
            "batch id predict after epoch:  102\n",
            "batch id predict after epoch:  103\n",
            "batch id predict after epoch:  104\n",
            "batch id predict after epoch:  105\n",
            "batch id predict after epoch:  106\n",
            "batch id predict after epoch:  107\n",
            "batch id predict after epoch:  108\n",
            "batch id predict after epoch:  109\n",
            "batch id predict after epoch:  110\n",
            "batch id predict after epoch:  111\n",
            "batch id predict after epoch:  112\n",
            "batch id predict after epoch:  113\n",
            "batch id predict after epoch:  114\n",
            "batch id predict after epoch:  115\n",
            "batch id predict after epoch:  116\n",
            "batch id predict after epoch:  117\n",
            "batch id predict after epoch:  118\n",
            "batch id predict after epoch:  119\n",
            "batch id predict after epoch:  120\n",
            "batch id predict after epoch:  121\n",
            "batch id predict after epoch:  122\n",
            "batch id predict after epoch:  123\n",
            "batch id predict after epoch:  124\n",
            "batch id predict after epoch:  125\n",
            "batch id predict after epoch:  126\n",
            "batch id predict after epoch:  127\n",
            "batch id predict after epoch:  128\n",
            "batch id predict after epoch:  129\n",
            "batch id predict after epoch:  130\n",
            "batch id predict after epoch:  131\n",
            "batch id predict after epoch:  132\n",
            "batch id predict after epoch:  133\n",
            "batch id predict after epoch:  134\n",
            "batch id predict after epoch:  135\n",
            "batch id predict after epoch:  136\n",
            "batch id predict after epoch:  137\n",
            "batch id predict after epoch:  138\n",
            "batch id predict after epoch:  139\n",
            "batch id predict after epoch:  140\n",
            "batch id predict after epoch:  141\n",
            "batch id predict after epoch:  142\n",
            "batch id predict after epoch:  143\n",
            "batch id predict after epoch:  144\n",
            "batch id predict after epoch:  145\n",
            "batch id predict after epoch:  146\n",
            "batch id predict after epoch:  147\n",
            "batch id predict after epoch:  148\n",
            "batch id predict after epoch:  149\n",
            "batch id predict after epoch:  150\n",
            "batch id predict after epoch:  151\n",
            "batch id predict after epoch:  152\n",
            "batch id predict after epoch:  153\n",
            "batch id predict after epoch:  154\n",
            "batch id predict after epoch:  155\n",
            "batch id predict after epoch:  156\n",
            "batch id predict after epoch:  157\n",
            "batch id predict after epoch:  158\n",
            "batch id predict after epoch:  159\n",
            "batch id predict after epoch:  160\n",
            "batch id predict after epoch:  161\n",
            "batch id predict after epoch:  162\n",
            "batch id predict after epoch:  163\n",
            "batch id predict after epoch:  164\n",
            "batch id predict after epoch:  165\n",
            "batch id predict after epoch:  166\n",
            "batch id predict after epoch:  167\n",
            "batch id predict after epoch:  168\n",
            "batch id predict after epoch:  169\n",
            "batch id predict after epoch:  170\n",
            "batch id predict after epoch:  171\n",
            "batch id predict after epoch:  172\n",
            "batch id predict after epoch:  173\n",
            "batch id predict after epoch:  174\n",
            "batch id predict after epoch:  175\n",
            "batch id predict after epoch:  176\n",
            "batch id predict after epoch:  177\n",
            "batch id predict after epoch:  178\n",
            "batch id predict after epoch:  179\n",
            "batch id predict after epoch:  180\n",
            "batch id predict after epoch:  181\n",
            "batch id predict after epoch:  182\n",
            "batch id predict after epoch:  183\n",
            "batch id predict after epoch:  184\n",
            "batch id predict after epoch:  185\n",
            "batch id predict after epoch:  186\n",
            "batch id predict after epoch:  187\n",
            "batch id predict after epoch:  188\n",
            "batch id predict after epoch:  189\n",
            "batch id predict after epoch:  190\n",
            "batch id predict after epoch:  191\n",
            "batch id predict after epoch:  192\n",
            "batch id predict after epoch:  193\n",
            "batch id predict after epoch:  194\n",
            "batch id predict after epoch:  195\n",
            "batch id predict after epoch:  196\n",
            "batch id predict after epoch:  197\n",
            "batch id predict after epoch:  198\n",
            "batch id predict after epoch:  199\n",
            "batch id predict after epoch:  200\n",
            "batch id predict after epoch:  201\n",
            "batch id predict after epoch:  202\n",
            "batch id predict after epoch:  203\n",
            "batch id predict after epoch:  204\n",
            "batch id predict after epoch:  205\n",
            "batch id predict after epoch:  206\n",
            "batch id predict after epoch:  207\n",
            "batch id predict after epoch:  208\n",
            "batch id predict after epoch:  209\n",
            "batch id predict after epoch:  210\n",
            "batch id predict after epoch:  211\n",
            "batch id predict after epoch:  212\n",
            "batch id predict after epoch:  213\n",
            "batch id predict after epoch:  214\n",
            "batch id predict after epoch:  215\n",
            "batch id predict after epoch:  216\n",
            "batch id predict after epoch:  217\n",
            "batch id predict after epoch:  218\n",
            "batch id predict after epoch:  219\n",
            "batch id predict after epoch:  220\n",
            "batch id predict after epoch:  221\n",
            "batch id predict after epoch:  222\n",
            "batch id predict after epoch:  223\n",
            "batch id predict after epoch:  224\n",
            "batch id predict after epoch:  225\n",
            "batch id predict after epoch:  226\n",
            "batch id predict after epoch:  227\n",
            "batch id predict after epoch:  228\n",
            "batch id predict after epoch:  229\n",
            "batch id predict after epoch:  230\n",
            "batch id predict after epoch:  231\n",
            "batch id predict after epoch:  232\n",
            "batch id predict after epoch:  233\n",
            "batch id predict after epoch:  234\n",
            "batch id predict after epoch:  235\n",
            "batch id predict after epoch:  236\n",
            "batch id predict after epoch:  237\n",
            "batch id predict after epoch:  238\n",
            "batch id predict after epoch:  239\n",
            "batch id predict after epoch:  240\n",
            "batch id predict after epoch:  241\n",
            "batch id predict after epoch:  242\n",
            "batch id predict after epoch:  243\n",
            "batch id predict after epoch:  244\n",
            "batch id predict after epoch:  245\n",
            "batch id predict after epoch:  246\n",
            "batch id predict after epoch:  247\n",
            "batch id predict after epoch:  248\n",
            "batch id predict after epoch:  249\n",
            "batch id predict after epoch:  250\n",
            "batch id predict after epoch:  251\n",
            "batch id predict after epoch:  252\n",
            "batch id predict after epoch:  253\n",
            "batch id predict after epoch:  254\n",
            "batch id predict after epoch:  255\n",
            "batch id predict after epoch:  256\n",
            "batch id predict after epoch:  257\n",
            "batch id predict after epoch:  258\n",
            "batch id predict after epoch:  259\n",
            "batch id predict after epoch:  260\n",
            "batch id predict after epoch:  261\n",
            "batch id predict after epoch:  262\n",
            "batch id predict after epoch:  263\n",
            "batch id predict after epoch:  264\n",
            "batch id predict after epoch:  265\n",
            "batch id predict after epoch:  266\n",
            "batch id predict after epoch:  267\n",
            "batch id predict after epoch:  268\n",
            "batch id predict after epoch:  269\n",
            "batch id predict after epoch:  270\n",
            "batch id predict after epoch:  271\n",
            "batch id predict after epoch:  272\n",
            "batch id predict after epoch:  273\n",
            "batch id predict after epoch:  274\n",
            "batch id predict after epoch:  275\n",
            "batch id predict after epoch:  276\n",
            "batch id predict after epoch:  277\n",
            "batch id predict after epoch:  278\n",
            "batch id predict after epoch:  279\n",
            "batch id predict after epoch:  280\n",
            "batch id predict after epoch:  281\n",
            "batch id predict after epoch:  282\n",
            "batch id predict after epoch:  283\n",
            "batch id predict after epoch:  284\n",
            "batch id predict after epoch:  285\n",
            "batch id predict after epoch:  286\n",
            "batch id predict after epoch:  287\n",
            "batch id predict after epoch:  288\n",
            "batch id predict after epoch:  289\n",
            "batch id predict after epoch:  290\n",
            "batch id predict after epoch:  291\n",
            "batch id predict after epoch:  292\n",
            "batch id predict after epoch:  293\n",
            "batch id predict after epoch:  294\n",
            "batch id predict after epoch:  295\n",
            "batch id predict after epoch:  296\n",
            "batch id predict after epoch:  297\n",
            "batch id predict after epoch:  298\n",
            "batch id predict after epoch:  299\n",
            "batch id predict after epoch:  300\n",
            "batch id predict after epoch:  301\n",
            "batch id predict after epoch:  302\n",
            "batch id predict after epoch:  303\n",
            "batch id predict after epoch:  304\n",
            "batch id predict after epoch:  305\n",
            "batch id predict after epoch:  306\n",
            "batch id predict after epoch:  307\n",
            "batch id predict after epoch:  308\n",
            "batch id predict after epoch:  309\n",
            "batch id predict after epoch:  310\n",
            "batch id predict after epoch:  311\n",
            "batch id predict after epoch:  312\n",
            "batch id predict after epoch:  313\n",
            "batch id predict after epoch:  314\n",
            "batch id predict after epoch:  315\n",
            "batch id predict after epoch:  316\n",
            "batch id predict after epoch:  317\n",
            "batch id predict after epoch:  318\n",
            "batch id predict after epoch:  319\n",
            "batch id predict after epoch:  320\n",
            "batch id predict after epoch:  321\n",
            "batch id predict after epoch:  322\n",
            "batch id predict after epoch:  323\n",
            "batch id predict after epoch:  324\n",
            "batch id predict after epoch:  325\n",
            "batch id predict after epoch:  326\n",
            "batch id predict after epoch:  327\n",
            "batch id predict after epoch:  328\n",
            "batch id predict after epoch:  329\n",
            "batch id predict after epoch:  330\n",
            "batch id predict after epoch:  331\n",
            "batch id predict after epoch:  332\n",
            "batch id predict after epoch:  333\n",
            "batch id predict after epoch:  334\n",
            "batch id predict after epoch:  335\n",
            "batch id predict after epoch:  336\n",
            "batch id predict after epoch:  337\n",
            "batch id predict after epoch:  338\n",
            "batch id predict after epoch:  339\n",
            "batch id predict after epoch:  340\n",
            "batch id predict after epoch:  341\n",
            "batch id predict after epoch:  342\n",
            "batch id predict after epoch:  343\n",
            "batch id predict after epoch:  344\n",
            "batch id predict after epoch:  345\n",
            "batch id predict after epoch:  346\n",
            "batch id predict after epoch:  347\n",
            "batch id predict after epoch:  348\n",
            "batch id predict after epoch:  349\n",
            "batch id predict after epoch:  350\n",
            "batch id predict after epoch:  351\n",
            "batch id predict after epoch:  352\n",
            "batch id predict after epoch:  353\n",
            "batch id predict after epoch:  354\n",
            "batch id predict after epoch:  355\n",
            "batch id predict after epoch:  356\n",
            "batch id predict after epoch:  357\n",
            "batch id predict after epoch:  358\n",
            "batch id predict after epoch:  359\n",
            "batch id predict after epoch:  360\n",
            "batch id predict after epoch:  361\n",
            "batch id predict after epoch:  362\n",
            "batch id predict after epoch:  363\n",
            "batch id predict after epoch:  364\n",
            "batch id predict after epoch:  365\n",
            "batch id predict after epoch:  366\n",
            "batch id predict after epoch:  367\n",
            "batch id predict after epoch:  368\n",
            "batch id predict after epoch:  369\n",
            "batch id predict after epoch:  370\n",
            "batch id predict after epoch:  371\n",
            "batch id predict after epoch:  372\n",
            "batch id predict after epoch:  373\n",
            "batch id predict after epoch:  374\n",
            "batch id predict after epoch:  375\n",
            "batch id predict after epoch:  376\n",
            "batch id predict after epoch:  377\n",
            "batch id predict after epoch:  378\n",
            "batch id predict after epoch:  379\n",
            "batch id predict after epoch:  380\n",
            "batch id predict after epoch:  381\n",
            "batch id predict after epoch:  382\n",
            "batch id predict after epoch:  383\n",
            "batch id predict after epoch:  384\n",
            "batch id predict after epoch:  385\n",
            "batch id predict after epoch:  386\n",
            "batch id predict after epoch:  387\n",
            "batch id predict after epoch:  388\n",
            "batch id predict after epoch:  389\n",
            "batch id predict after epoch:  390\n",
            "batch id predict after epoch:  391\n",
            "batch id predict after epoch:  392\n",
            "batch id predict after epoch:  393\n",
            "batch id predict after epoch:  394\n",
            "batch id predict after epoch:  395\n",
            "batch id predict after epoch:  396\n",
            "batch id predict after epoch:  397\n",
            "batch id predict after epoch:  398\n",
            "batch id predict after epoch:  399\n",
            "batch id predict after epoch:  400\n",
            "batch id predict after epoch:  401\n",
            "batch id predict after epoch:  402\n",
            "batch id predict after epoch:  403\n",
            "batch id predict after epoch:  404\n",
            "batch id predict after epoch:  405\n",
            "batch id predict after epoch:  406\n",
            "batch id predict after epoch:  407\n",
            "batch id predict after epoch:  408\n",
            "batch id predict after epoch:  409\n",
            "batch id predict after epoch:  410\n",
            "batch id predict after epoch:  411\n",
            "batch id predict after epoch:  412\n",
            "batch id predict after epoch:  413\n",
            "batch id predict after epoch:  414\n",
            "batch id predict after epoch:  415\n",
            "batch id predict after epoch:  416\n",
            "batch id predict after epoch:  417\n",
            "batch id predict after epoch:  418\n",
            "batch id predict after epoch:  419\n",
            "batch id predict after epoch:  420\n",
            "batch id predict after epoch:  421\n",
            "batch id predict after epoch:  422\n",
            "batch id predict after epoch:  423\n",
            "batch id predict after epoch:  424\n",
            "batch id predict after epoch:  425\n",
            "batch id predict after epoch:  426\n",
            "batch id predict after epoch:  427\n",
            "batch id predict after epoch:  428\n",
            "batch id predict after epoch:  429\n",
            "batch id predict after epoch:  430\n",
            "batch id predict after epoch:  431\n",
            "batch id predict after epoch:  432\n",
            "batch id predict after epoch:  433\n",
            "batch id predict after epoch:  434\n",
            "batch id predict after epoch:  435\n",
            "batch id predict after epoch:  436\n",
            "batch id predict after epoch:  437\n",
            "batch id predict after epoch:  438\n",
            "batch id predict after epoch:  439\n",
            "batch id predict after epoch:  440\n",
            "batch id predict after epoch:  441\n",
            "batch id predict after epoch:  442\n",
            "batch id predict after epoch:  443\n",
            "batch id predict after epoch:  444\n",
            "batch id predict after epoch:  445\n",
            "batch id predict after epoch:  446\n",
            "batch id predict after epoch:  447\n",
            "batch id predict after epoch:  448\n",
            "batch id predict after epoch:  449\n",
            "batch id predict after epoch:  450\n",
            "batch id predict after epoch:  451\n",
            "batch id predict after epoch:  452\n",
            "batch id predict after epoch:  453\n",
            "batch id predict after epoch:  454\n",
            "batch id predict after epoch:  455\n",
            "batch id predict after epoch:  456\n",
            "batch id predict after epoch:  457\n",
            "batch id predict after epoch:  458\n",
            "batch id predict after epoch:  459\n",
            "batch id predict after epoch:  460\n",
            "batch id predict after epoch:  461\n",
            "batch id predict after epoch:  462\n",
            "batch id predict after epoch:  463\n",
            "batch id predict after epoch:  464\n",
            "batch id predict after epoch:  465\n",
            "batch id predict after epoch:  466\n",
            "batch id predict after epoch:  467\n",
            "batch id predict after epoch:  468\n",
            "batch id predict after epoch:  469\n",
            "batch id predict after epoch:  470\n",
            "batch id predict after epoch:  471\n",
            "batch id predict after epoch:  472\n",
            "batch id predict after epoch:  473\n",
            "batch id predict after epoch:  474\n",
            "batch id predict after epoch:  475\n",
            "batch id predict after epoch:  476\n",
            "batch id predict after epoch:  477\n",
            "batch id predict after epoch:  478\n",
            "batch id predict after epoch:  479\n",
            "batch id predict after epoch:  480\n",
            "batch id predict after epoch:  481\n",
            "batch id predict after epoch:  482\n",
            "batch id predict after epoch:  483\n",
            "batch id predict after epoch:  484\n",
            "batch id predict after epoch:  485\n",
            "batch id predict after epoch:  486\n",
            "batch id predict after epoch:  487\n",
            "batch id predict after epoch:  488\n",
            "batch id predict after epoch:  489\n",
            "batch id predict after epoch:  490\n",
            "batch id predict after epoch:  491\n",
            "batch id predict after epoch:  492\n",
            "batch id predict after epoch:  493\n",
            "batch id predict after epoch:  494\n",
            "batch id predict after epoch:  495\n",
            "batch id predict after epoch:  496\n",
            "batch id predict after epoch:  497\n",
            "batch id predict after epoch:  498\n",
            "batch id predict after epoch:  499\n",
            "batch id predict after epoch:  500\n",
            "batch id predict after epoch:  501\n",
            "batch id predict after epoch:  502\n",
            "batch id predict after epoch:  503\n",
            "batch id predict after epoch:  504\n",
            "batch id predict after epoch:  505\n",
            "batch id predict after epoch:  506\n",
            "batch id predict after epoch:  507\n",
            "batch id predict after epoch:  508\n",
            "batch id predict after epoch:  509\n",
            "batch id predict after epoch:  510\n",
            "batch id predict after epoch:  511\n",
            "batch id predict after epoch:  512\n",
            "batch id predict after epoch:  513\n",
            "batch id predict after epoch:  514\n",
            "batch id predict after epoch:  515\n",
            "batch id predict after epoch:  516\n",
            "batch id predict after epoch:  517\n",
            "batch id predict after epoch:  518\n",
            "batch id predict after epoch:  519\n",
            "batch id predict after epoch:  520\n",
            "batch id predict after epoch:  521\n",
            "batch id predict after epoch:  522\n",
            "batch id predict after epoch:  523\n",
            "batch id predict after epoch:  524\n",
            "batch id predict after epoch:  525\n",
            "batch id predict after epoch:  526\n",
            "batch id predict after epoch:  527\n",
            "batch id predict after epoch:  528\n",
            "batch id predict after epoch:  529\n",
            "batch id predict after epoch:  530\n",
            "batch id predict after epoch:  531\n",
            "batch id predict after epoch:  532\n",
            "batch id predict after epoch:  533\n",
            "batch id predict after epoch:  534\n",
            "batch id predict after epoch:  535\n",
            "batch id predict after epoch:  536\n",
            "batch id predict after epoch:  537\n",
            "batch id predict after epoch:  538\n",
            "batch id predict after epoch:  539\n",
            "batch id predict after epoch:  540\n",
            "batch id predict after epoch:  541\n",
            "batch id predict after epoch:  542\n",
            "batch id predict after epoch:  543\n",
            "batch id predict after epoch:  544\n",
            "batch id predict after epoch:  545\n",
            "batch id predict after epoch:  546\n",
            "batch id predict after epoch:  547\n",
            "batch id predict after epoch:  548\n",
            "batch id predict after epoch:  549\n",
            "batch id predict after epoch:  550\n",
            "batch id predict after epoch:  551\n",
            "batch id predict after epoch:  552\n",
            "batch id predict after epoch:  553\n",
            "batch id predict after epoch:  554\n",
            "batch id predict after epoch:  555\n",
            "batch id predict after epoch:  556\n",
            "batch id predict after epoch:  557\n",
            "batch id predict after epoch:  558\n",
            "batch id predict after epoch:  559\n",
            "batch id predict after epoch:  560\n",
            "batch id predict after epoch:  561\n",
            "batch id predict after epoch:  562\n",
            "batch id predict after epoch:  563\n",
            "batch id predict after epoch:  564\n",
            "batch id predict after epoch:  565\n",
            "batch id predict after epoch:  566\n",
            "batch id predict after epoch:  567\n",
            "batch id predict after epoch:  568\n",
            "batch id predict after epoch:  569\n",
            "batch id predict after epoch:  570\n",
            "batch id predict after epoch:  571\n",
            "batch id predict after epoch:  572\n",
            "batch id predict after epoch:  573\n",
            "batch id predict after epoch:  574\n",
            "batch id predict after epoch:  575\n",
            "batch id predict after epoch:  576\n",
            "batch id predict after epoch:  577\n",
            "batch id predict after epoch:  578\n",
            "batch id predict after epoch:  579\n",
            "batch id predict after epoch:  580\n",
            "batch id predict after epoch:  581\n",
            "batch id predict after epoch:  582\n",
            "batch id predict after epoch:  583\n",
            "batch id predict after epoch:  584\n",
            "batch id predict after epoch:  585\n",
            "batch id predict after epoch:  586\n",
            "batch id predict after epoch:  587\n",
            "batch id predict after epoch:  588\n",
            "batch id predict after epoch:  589\n",
            "batch id predict after epoch:  590\n",
            "batch id predict after epoch:  591\n",
            "batch id predict after epoch:  592\n",
            "batch id predict after epoch:  593\n",
            "batch id predict after epoch:  594\n",
            "batch id predict after epoch:  595\n",
            "batch id predict after epoch:  596\n",
            "batch id predict after epoch:  597\n",
            "batch id predict after epoch:  598\n",
            "batch id predict after epoch:  599\n",
            "batch id predict after epoch:  600\n",
            "batch id predict after epoch:  601\n",
            "batch id predict after epoch:  602\n",
            "batch id predict after epoch:  603\n",
            "batch id predict after epoch:  604\n",
            "batch id predict after epoch:  605\n",
            "batch id predict after epoch:  606\n",
            "batch id predict after epoch:  607\n",
            "batch id predict after epoch:  608\n",
            "batch id predict after epoch:  609\n",
            "batch id predict after epoch:  610\n",
            "batch id predict after epoch:  611\n",
            "batch id predict after epoch:  612\n",
            "batch id predict after epoch:  613\n",
            "batch id predict after epoch:  614\n",
            "batch id predict after epoch:  615\n",
            "batch id predict after epoch:  616\n",
            "batch id predict after epoch:  617\n",
            "batch id predict after epoch:  618\n",
            "batch id predict after epoch:  619\n",
            "batch id predict after epoch:  620\n",
            "batch id predict after epoch:  621\n",
            "batch id predict after epoch:  622\n",
            "batch id predict after epoch:  623\n",
            "batch id predict after epoch:  624\n",
            "batch id predict after epoch:  625\n",
            "batch id predict after epoch:  626\n",
            "batch id predict after epoch:  627\n",
            "batch id predict after epoch:  628\n",
            "batch id predict after epoch:  629\n",
            "batch id predict after epoch:  630\n",
            "batch id predict after epoch:  631\n",
            "batch id predict after epoch:  632\n",
            "batch id predict after epoch:  633\n",
            "batch id predict after epoch:  634\n",
            "batch id predict after epoch:  635\n",
            "batch id predict after epoch:  636\n",
            "batch id predict after epoch:  637\n",
            "batch id predict after epoch:  638\n",
            "batch id predict after epoch:  639\n",
            "batch id predict after epoch:  640\n",
            "batch id predict after epoch:  641\n",
            "batch id predict after epoch:  642\n",
            "batch id predict after epoch:  643\n",
            "batch id predict after epoch:  644\n",
            "batch id predict after epoch:  645\n",
            "batch id predict after epoch:  646\n",
            "batch id predict after epoch:  647\n",
            "batch id predict after epoch:  648\n",
            "batch id predict after epoch:  649\n",
            "batch id predict after epoch:  650\n",
            "batch id predict after epoch:  651\n",
            "batch id predict after epoch:  652\n",
            "batch id predict after epoch:  653\n",
            "batch id predict after epoch:  654\n",
            "batch id predict after epoch:  655\n",
            "batch id predict after epoch:  656\n",
            "batch id predict after epoch:  657\n",
            "batch id predict after epoch:  658\n",
            "batch id predict after epoch:  659\n",
            "batch id predict after epoch:  660\n",
            "batch id predict after epoch:  661\n",
            "batch id predict after epoch:  662\n",
            "batch id predict after epoch:  663\n",
            "batch id predict after epoch:  664\n",
            "batch id predict after epoch:  665\n",
            "batch id predict after epoch:  666\n",
            "batch id predict after epoch:  667\n",
            "batch id predict after epoch:  668\n",
            "batch id predict after epoch:  669\n",
            "batch id predict after epoch:  670\n",
            "batch id predict after epoch:  671\n",
            "batch id predict after epoch:  672\n",
            "batch id predict after epoch:  673\n",
            "batch id predict after epoch:  674\n",
            "batch id predict after epoch:  675\n",
            "batch id predict after epoch:  676\n",
            "batch id predict after epoch:  677\n",
            "batch id predict after epoch:  678\n",
            "batch id predict after epoch:  679\n",
            "batch id predict after epoch:  680\n",
            "batch id predict after epoch:  681\n",
            "batch id predict after epoch:  682\n",
            "batch id predict after epoch:  683\n",
            "batch id predict after epoch:  684\n",
            "batch id predict after epoch:  685\n",
            "batch id predict after epoch:  686\n",
            "batch id predict after epoch:  687\n",
            "batch id predict after epoch:  688\n",
            "batch id predict after epoch:  689\n",
            "batch id predict after epoch:  690\n",
            "batch id predict after epoch:  691\n",
            "batch id predict after epoch:  692\n",
            "batch id predict after epoch:  693\n",
            "batch id predict after epoch:  694\n",
            "batch id predict after epoch:  695\n",
            "batch id predict after epoch:  696\n",
            "batch id predict after epoch:  697\n",
            "batch id predict after epoch:  698\n",
            "batch id predict after epoch:  699\n",
            "batch id predict after epoch:  700\n",
            "batch id predict after epoch:  701\n",
            "batch id predict after epoch:  702\n",
            "batch id predict after epoch:  703\n",
            "batch id predict after epoch:  704\n",
            "batch id predict after epoch:  705\n",
            "batch id predict after epoch:  706\n",
            "batch id predict after epoch:  707\n",
            "batch id predict after epoch:  708\n",
            "batch id predict after epoch:  709\n",
            "batch id predict after epoch:  710\n",
            "batch id predict after epoch:  711\n",
            "batch id predict after epoch:  712\n",
            "batch id predict after epoch:  713\n",
            "batch id predict after epoch:  714\n",
            "batch id predict after epoch:  715\n",
            "batch id predict after epoch:  716\n",
            "batch id predict after epoch:  717\n",
            "batch id predict after epoch:  718\n",
            "batch id predict after epoch:  719\n",
            "batch id predict after epoch:  720\n",
            "batch id predict after epoch:  721\n",
            "batch id predict after epoch:  722\n",
            "batch id predict after epoch:  723\n",
            "batch id predict after epoch:  724\n",
            "batch id predict after epoch:  725\n",
            "batch id predict after epoch:  726\n",
            "batch id predict after epoch:  727\n",
            "batch id predict after epoch:  728\n",
            "batch id predict after epoch:  729\n",
            "batch id predict after epoch:  730\n",
            "batch id predict after epoch:  731\n",
            "batch id predict after epoch:  732\n",
            "batch id predict after epoch:  733\n",
            "batch id predict after epoch:  734\n",
            "batch id predict after epoch:  735\n",
            "batch id predict after epoch:  736\n",
            "batch id predict after epoch:  737\n",
            "batch id predict after epoch:  738\n",
            "batch id predict after epoch:  739\n",
            "batch id predict after epoch:  740\n",
            "batch id predict after epoch:  741\n",
            "batch id predict after epoch:  742\n",
            "batch id predict after epoch:  743\n",
            "batch id predict after epoch:  744\n",
            "batch id predict after epoch:  745\n",
            "batch id predict after epoch:  746\n",
            "batch id predict after epoch:  747\n",
            "batch id predict after epoch:  748\n",
            "batch id predict after epoch:  749\n",
            "batch id predict after epoch:  750\n",
            "batch id predict after epoch:  751\n",
            "batch id predict after epoch:  752\n",
            "batch id predict after epoch:  753\n",
            "batch id predict after epoch:  754\n",
            "batch id predict after epoch:  755\n",
            "batch id predict after epoch:  756\n",
            "batch id predict after epoch:  757\n",
            "batch id predict after epoch:  758\n",
            "batch id predict after epoch:  759\n",
            "batch id predict after epoch:  760\n",
            "batch id predict after epoch:  761\n",
            "batch id predict after epoch:  762\n",
            "batch id predict after epoch:  763\n",
            "batch id predict after epoch:  764\n",
            "batch id predict after epoch:  765\n",
            "batch id predict after epoch:  766\n",
            "batch id predict after epoch:  767\n",
            "batch id predict after epoch:  768\n",
            "batch id predict after epoch:  769\n",
            "batch id predict after epoch:  770\n",
            "batch id predict after epoch:  771\n",
            "batch id predict after epoch:  772\n",
            "batch id predict after epoch:  773\n",
            "batch id predict after epoch:  774\n",
            "batch id predict after epoch:  775\n",
            "batch id predict after epoch:  776\n",
            "batch id predict after epoch:  777\n",
            "batch id predict after epoch:  778\n",
            "batch id predict after epoch:  779\n",
            "batch id predict after epoch:  780\n",
            "batch id predict after epoch:  781\n",
            "batch id predict after epoch:  782\n",
            "batch id predict after epoch:  783\n",
            "batch id predict after epoch:  784\n",
            "batch id predict after epoch:  785\n",
            "batch id predict after epoch:  786\n",
            "batch id predict after epoch:  787\n",
            "batch id predict after epoch:  788\n",
            "batch id predict after epoch:  789\n",
            "batch id predict after epoch:  790\n",
            "batch id predict after epoch:  791\n",
            "batch id predict after epoch:  792\n",
            "batch id predict after epoch:  793\n",
            "batch id predict after epoch:  794\n",
            "batch id predict after epoch:  795\n",
            "batch id predict after epoch:  796\n",
            "batch id predict after epoch:  797\n",
            "batch id predict after epoch:  798\n",
            "batch id predict after epoch:  799\n",
            "batch id predict after epoch:  800\n",
            "batch id predict after epoch:  801\n",
            "batch id predict after epoch:  802\n",
            "batch id predict after epoch:  803\n",
            "batch id predict after epoch:  804\n",
            "batch id predict after epoch:  805\n",
            "batch id predict after epoch:  806\n",
            "batch id predict after epoch:  807\n",
            "batch id predict after epoch:  808\n",
            "batch id predict after epoch:  809\n",
            "batch id predict after epoch:  810\n",
            "batch id predict after epoch:  811\n",
            "batch id predict after epoch:  812\n",
            "batch id predict after epoch:  813\n",
            "batch id predict after epoch:  814\n",
            "batch id predict after epoch:  815\n",
            "batch id predict after epoch:  816\n",
            "batch id predict after epoch:  817\n",
            "batch id predict after epoch:  818\n",
            "batch id predict after epoch:  819\n",
            "batch id predict after epoch:  820\n",
            "batch id predict after epoch:  821\n",
            "batch id predict after epoch:  822\n",
            "batch id predict after epoch:  823\n",
            "batch id predict after epoch:  824\n",
            "batch id predict after epoch:  825\n",
            "batch id predict after epoch:  826\n",
            "batch id predict after epoch:  827\n",
            "batch id predict after epoch:  828\n",
            "batch id predict after epoch:  829\n",
            "batch id predict after epoch:  830\n",
            "batch id predict after epoch:  831\n",
            "batch id predict after epoch:  832\n",
            "batch id predict after epoch:  833\n",
            "batch id predict after epoch:  834\n",
            "batch id predict after epoch:  835\n",
            "batch id predict after epoch:  836\n",
            "batch id predict after epoch:  837\n",
            "batch id predict after epoch:  838\n",
            "batch id predict after epoch:  839\n",
            "batch id predict after epoch:  840\n",
            "batch id predict after epoch:  841\n",
            "batch id predict after epoch:  842\n",
            "batch id predict after epoch:  843\n",
            "batch id predict after epoch:  844\n",
            "batch id predict after epoch:  845\n",
            "batch id predict after epoch:  846\n",
            "batch id predict after epoch:  847\n",
            "batch id predict after epoch:  848\n",
            "batch id predict after epoch:  849\n",
            "batch id predict after epoch:  850\n",
            "batch id predict after epoch:  851\n",
            "batch id predict after epoch:  852\n",
            "batch id predict after epoch:  853\n",
            "batch id predict after epoch:  854\n",
            "batch id predict after epoch:  855\n",
            "batch id predict after epoch:  856\n",
            "batch id predict after epoch:  857\n",
            "batch id predict after epoch:  858\n",
            "batch id predict after epoch:  859\n",
            "batch id predict after epoch:  860\n",
            "batch id predict after epoch:  861\n",
            "batch id predict after epoch:  862\n",
            "batch id predict after epoch:  863\n",
            "batch id predict after epoch:  864\n",
            "batch id predict after epoch:  865\n",
            "batch id predict after epoch:  866\n",
            "batch id predict after epoch:  867\n",
            "batch id predict after epoch:  868\n",
            "batch id predict after epoch:  869\n",
            "batch id predict after epoch:  870\n",
            "batch id predict after epoch:  871\n",
            "batch id predict after epoch:  872\n",
            "batch id predict after epoch:  873\n",
            "batch id predict after epoch:  874\n",
            "batch id predict after epoch:  875\n",
            "batch id predict after epoch:  876\n",
            "batch id predict after epoch:  877\n",
            "batch id predict after epoch:  878\n",
            "batch id predict after epoch:  879\n",
            "batch id predict after epoch:  880\n",
            "batch id predict after epoch:  881\n",
            "batch id predict after epoch:  882\n",
            "batch id predict after epoch:  883\n",
            "batch id predict after epoch:  884\n",
            "batch id predict after epoch:  885\n",
            "batch id predict after epoch:  886\n",
            "batch id predict after epoch:  887\n",
            "batch id predict after epoch:  888\n",
            "batch id predict after epoch:  889\n",
            "batch id predict after epoch:  890\n",
            "batch id predict after epoch:  891\n",
            "batch id predict after epoch:  892\n",
            "batch id predict after epoch:  893\n",
            "batch id predict after epoch:  894\n",
            "batch id predict after epoch:  895\n",
            "batch id predict after epoch:  896\n",
            "batch id predict after epoch:  897\n",
            "batch id predict after epoch:  898\n",
            "batch id predict after epoch:  899\n",
            "batch id predict after epoch:  900\n",
            "batch id predict after epoch:  901\n",
            "batch id predict after epoch:  902\n",
            "batch id predict after epoch:  903\n",
            "batch id predict after epoch:  904\n",
            "batch id predict after epoch:  905\n",
            "batch id predict after epoch:  906\n",
            "batch id predict after epoch:  907\n",
            "batch id predict after epoch:  908\n",
            "batch id predict after epoch:  909\n",
            "batch id predict after epoch:  910\n",
            "batch id predict after epoch:  911\n",
            "batch id predict after epoch:  912\n",
            "batch id predict after epoch:  913\n",
            "batch id predict after epoch:  914\n",
            "batch id predict after epoch:  915\n",
            "batch id predict after epoch:  916\n",
            "batch id predict after epoch:  917\n",
            "batch id predict after epoch:  918\n",
            "batch id predict after epoch:  919\n",
            "batch id predict after epoch:  920\n",
            "batch id predict after epoch:  921\n",
            "batch id predict after epoch:  922\n",
            "batch id predict after epoch:  923\n",
            "batch id predict after epoch:  924\n",
            "batch id predict after epoch:  925\n",
            "batch id predict after epoch:  926\n",
            "batch id predict after epoch:  927\n",
            "batch id predict after epoch:  928\n",
            "batch id predict after epoch:  929\n",
            "batch id predict after epoch:  930\n",
            "batch id predict after epoch:  931\n",
            "[1 0 0 ... 1 1 1]\n",
            "mini Batch 1 Loss: 2.39915132522583\n",
            "mini Batch 2 Loss: 2.901273250579834\n",
            "mini Batch 3 Loss: 1.5715007781982422\n",
            "mini Batch 4 Loss: 3.828395366668701\n",
            "mini Batch 5 Loss: 2.2063777446746826\n",
            "mini Batch 6 Loss: 1.1265572309494019\n",
            "mini Batch 7 Loss: 1.0771090984344482\n",
            "mini Batch 8 Loss: 1.6584351062774658\n",
            "mini Batch 9 Loss: 1.0263530015945435\n",
            "mini Batch 10 Loss: 1.0045273303985596\n",
            "mini Batch 11 Loss: 2.1643433570861816\n",
            "mini Batch 12 Loss: 1.4977506399154663\n",
            "mini Batch 13 Loss: 3.772189140319824\n",
            "mini Batch 14 Loss: 3.271629571914673\n",
            "mini Batch 15 Loss: 1.8087430000305176\n",
            "mini Batch 16 Loss: 0.3594397306442261\n",
            "mini Batch 17 Loss: 1.8682467937469482\n",
            "mini Batch 18 Loss: 2.1166157722473145\n",
            "mini Batch 19 Loss: 0.7192089557647705\n",
            "mini Batch 20 Loss: 1.061156988143921\n",
            "mini Batch 21 Loss: 0.4860781729221344\n",
            "mini Batch 22 Loss: 1.4595394134521484\n",
            "mini Batch 23 Loss: 2.230722188949585\n",
            "mini Batch 24 Loss: 3.6154661178588867\n",
            "mini Batch 25 Loss: 0.593725860118866\n",
            "mini Batch 26 Loss: 0.7815927863121033\n",
            "mini Batch 27 Loss: 1.1067051887512207\n",
            "mini Batch 28 Loss: 1.4676904678344727\n",
            "mini Batch 29 Loss: 2.378321409225464\n",
            "mini Batch 30 Loss: 0.7484186291694641\n",
            "mini Batch 31 Loss: 0.9063457250595093\n",
            "mini Batch 32 Loss: 2.568962335586548\n",
            "mini Batch 33 Loss: 0.016636580228805542\n",
            "mini Batch 34 Loss: 5.186555862426758\n",
            "mini Batch 35 Loss: 0.604575514793396\n",
            "mini Batch 36 Loss: 2.288355827331543\n",
            "mini Batch 37 Loss: 0.6839380264282227\n",
            "mini Batch 38 Loss: 0.8209174275398254\n",
            "mini Batch 39 Loss: 4.0375823974609375\n",
            "mini Batch 40 Loss: 2.68239164352417\n",
            "mini Batch 41 Loss: 2.496159553527832\n",
            "mini Batch 42 Loss: 1.1307859420776367\n",
            "mini Batch 43 Loss: 2.080357074737549\n",
            "mini Batch 44 Loss: 1.3711367845535278\n",
            "mini Batch 45 Loss: 4.796309947967529\n",
            "mini Batch 46 Loss: 0.09286943078041077\n",
            "mini Batch 47 Loss: 3.040414810180664\n",
            "mini Batch 48 Loss: 2.4866857528686523\n",
            "mini Batch 49 Loss: 0.137058287858963\n",
            "mini Batch 50 Loss: 1.6010578870773315\n",
            "mini Batch 51 Loss: 2.1058497428894043\n",
            "mini Batch 52 Loss: 1.5491752624511719\n",
            "mini Batch 53 Loss: 3.5826191902160645\n",
            "mini Batch 54 Loss: 3.284956455230713\n",
            "mini Batch 55 Loss: 2.6912879943847656\n",
            "mini Batch 56 Loss: 2.7348568439483643\n",
            "mini Batch 57 Loss: 0.2647255063056946\n",
            "mini Batch 58 Loss: 3.0195541381835938\n",
            "mini Batch 59 Loss: 0.5505658388137817\n",
            "mini Batch 60 Loss: 2.356595039367676\n",
            "mini Batch 61 Loss: 2.2290492057800293\n",
            "mini Batch 62 Loss: 2.696364402770996\n",
            "mini Batch 63 Loss: 2.9471845626831055\n",
            "mini Batch 64 Loss: 0.5982453227043152\n",
            "mini Batch 65 Loss: 1.3132011890411377\n",
            "mini Batch 66 Loss: 1.7535779476165771\n",
            "mini Batch 67 Loss: 0.4980309009552002\n",
            "mini Batch 68 Loss: 2.138293743133545\n",
            "mini Batch 69 Loss: 2.202934980392456\n",
            "mini Batch 70 Loss: 2.7336173057556152\n",
            "mini Batch 71 Loss: 2.847926616668701\n",
            "mini Batch 72 Loss: 2.307526111602783\n",
            "mini Batch 73 Loss: 2.588843822479248\n",
            "mini Batch 74 Loss: 1.440220832824707\n",
            "mini Batch 75 Loss: 2.645921230316162\n",
            "mini Batch 76 Loss: 0.4213518500328064\n",
            "mini Batch 77 Loss: 0.23037466406822205\n",
            "mini Batch 78 Loss: 1.5694938898086548\n",
            "mini Batch 79 Loss: 1.2333672046661377\n",
            "mini Batch 80 Loss: 2.0537333488464355\n",
            "mini Batch 81 Loss: 0.6710336804389954\n",
            "mini Batch 82 Loss: 2.1060070991516113\n",
            "mini Batch 83 Loss: 2.4025373458862305\n",
            "mini Batch 84 Loss: 2.028061866760254\n",
            "mini Batch 85 Loss: 1.3651971817016602\n",
            "mini Batch 86 Loss: 1.7174072265625\n",
            "mini Batch 87 Loss: 0.7619912624359131\n",
            "mini Batch 88 Loss: 0.5233217477798462\n",
            "mini Batch 89 Loss: 1.4881222248077393\n",
            "mini Batch 90 Loss: 0.6849883198738098\n",
            "mini Batch 91 Loss: 1.749877691268921\n",
            "mini Batch 92 Loss: 1.3076586723327637\n",
            "mini Batch 93 Loss: 3.2783961296081543\n",
            "mini Batch 94 Loss: 0.20811347663402557\n",
            "mini Batch 95 Loss: 0.8288136124610901\n",
            "mini Batch 96 Loss: 1.8964853286743164\n",
            "mini Batch 97 Loss: 0.9536447525024414\n",
            "mini Batch 98 Loss: 1.9867671728134155\n",
            "mini Batch 99 Loss: 2.097465753555298\n",
            "mini Batch 100 Loss: 1.0195355415344238\n",
            "mini Batch 101 Loss: 0.4470457434654236\n",
            "mini Batch 102 Loss: 3.920665740966797\n",
            "mini Batch 103 Loss: 0.7150280475616455\n",
            "mini Batch 104 Loss: 0.2717360258102417\n",
            "mini Batch 105 Loss: 1.0268408060073853\n",
            "mini Batch 106 Loss: 0.23672376573085785\n",
            "mini Batch 107 Loss: 0.8111666440963745\n",
            "mini Batch 108 Loss: 1.3753724098205566\n",
            "mini Batch 109 Loss: 0.9417389035224915\n",
            "mini Batch 110 Loss: 2.2149534225463867\n",
            "mini Batch 111 Loss: 1.79573392868042\n",
            "mini Batch 112 Loss: 1.6411092281341553\n",
            "mini Batch 113 Loss: 3.7260050773620605\n",
            "mini Batch 114 Loss: 3.390138626098633\n",
            "mini Batch 115 Loss: 0.339002788066864\n",
            "mini Batch 116 Loss: 2.047011375427246\n",
            "mini Batch 117 Loss: 4.041474342346191\n",
            "mini Batch 118 Loss: 1.7505810260772705\n",
            "mini Batch 119 Loss: 2.169008731842041\n",
            "mini Batch 120 Loss: 2.20672345161438\n",
            "mini Batch 121 Loss: 2.57997727394104\n",
            "mini Batch 122 Loss: 0.6131589412689209\n",
            "mini Batch 123 Loss: 1.2117739915847778\n",
            "mini Batch 124 Loss: 3.6861777305603027\n",
            "mini Batch 125 Loss: 2.1432113647460938\n",
            "mini Batch 126 Loss: 4.965949058532715\n",
            "mini Batch 127 Loss: 0.027505844831466675\n",
            "mini Batch 128 Loss: 3.83486270904541\n",
            "mini Batch 129 Loss: 0.30621930956840515\n",
            "mini Batch 130 Loss: 0.9706486463546753\n",
            "mini Batch 131 Loss: 1.4695382118225098\n",
            "mini Batch 132 Loss: 1.778552532196045\n",
            "mini Batch 133 Loss: 0.8020153045654297\n",
            "mini Batch 134 Loss: 0.6487495303153992\n",
            "mini Batch 135 Loss: 1.0589019060134888\n",
            "mini Batch 136 Loss: 0.6902889609336853\n",
            "mini Batch 137 Loss: 1.0751895904541016\n",
            "mini Batch 138 Loss: 1.5392879247665405\n",
            "mini Batch 139 Loss: 0.9762324094772339\n",
            "mini Batch 140 Loss: 1.176149606704712\n",
            "mini Batch 141 Loss: 1.3368947505950928\n",
            "mini Batch 142 Loss: 0.7912119626998901\n",
            "mini Batch 143 Loss: 1.5148794651031494\n",
            "mini Batch 144 Loss: 1.2153254747390747\n",
            "mini Batch 145 Loss: 1.4196631908416748\n",
            "mini Batch 146 Loss: 0.16675233840942383\n",
            "mini Batch 147 Loss: 1.1335957050323486\n",
            "mini Batch 148 Loss: 1.9867520332336426\n",
            "mini Batch 149 Loss: 0.3066083788871765\n",
            "mini Batch 150 Loss: 1.6582603454589844\n",
            "mini Batch 151 Loss: 2.9257404804229736\n",
            "mini Batch 152 Loss: 3.196165084838867\n",
            "mini Batch 153 Loss: 1.3944251537322998\n",
            "mini Batch 154 Loss: 2.8051815032958984\n",
            "mini Batch 155 Loss: 1.2228453159332275\n",
            "mini Batch 156 Loss: 1.6003789901733398\n",
            "mini Batch 157 Loss: 0.9378525018692017\n",
            "mini Batch 158 Loss: 1.3307552337646484\n",
            "mini Batch 159 Loss: 4.71171760559082\n",
            "mini Batch 160 Loss: 3.8539278507232666\n",
            "mini Batch 161 Loss: 3.377748966217041\n",
            "mini Batch 162 Loss: 2.0886597633361816\n",
            "mini Batch 163 Loss: 4.421169281005859\n",
            "mini Batch 164 Loss: 3.749995708465576\n",
            "mini Batch 165 Loss: 2.3056650161743164\n",
            "mini Batch 166 Loss: 2.888711452484131\n",
            "mini Batch 167 Loss: 3.5681605339050293\n",
            "mini Batch 168 Loss: 3.646972894668579\n",
            "mini Batch 169 Loss: 2.8223538398742676\n",
            "mini Batch 170 Loss: 3.45804762840271\n",
            "mini Batch 171 Loss: 4.002712249755859\n",
            "mini Batch 172 Loss: 3.017672538757324\n",
            "mini Batch 173 Loss: 3.1107449531555176\n",
            "mini Batch 174 Loss: 2.288158893585205\n",
            "mini Batch 175 Loss: 2.726606845855713\n",
            "mini Batch 176 Loss: 2.938821792602539\n",
            "mini Batch 177 Loss: 2.8213307857513428\n",
            "mini Batch 178 Loss: 3.958812713623047\n",
            "mini Batch 179 Loss: 2.8957810401916504\n",
            "mini Batch 180 Loss: 3.2174177169799805\n",
            "mini Batch 181 Loss: 3.786012887954712\n",
            "mini Batch 182 Loss: 3.1476454734802246\n",
            "mini Batch 183 Loss: 3.151308059692383\n",
            "mini Batch 184 Loss: 2.802896022796631\n",
            "mini Batch 185 Loss: 3.706779956817627\n",
            "mini Batch 186 Loss: 2.7450904846191406\n",
            "mini Batch 187 Loss: 3.2367563247680664\n",
            "mini Batch 188 Loss: 2.8197262287139893\n",
            "mini Batch 189 Loss: 2.348271131515503\n",
            "mini Batch 190 Loss: 2.067708730697632\n",
            "mini Batch 191 Loss: 2.8023898601531982\n",
            "mini Batch 192 Loss: 2.7433135509490967\n",
            "mini Batch 193 Loss: 2.9351885318756104\n",
            "mini Batch 194 Loss: 3.7138521671295166\n",
            "mini Batch 195 Loss: 3.62644624710083\n",
            "mini Batch 196 Loss: 2.7155802249908447\n",
            "mini Batch 197 Loss: 2.3309621810913086\n",
            "mini Batch 198 Loss: 2.605184555053711\n",
            "mini Batch 199 Loss: 3.0090389251708984\n",
            "mini Batch 200 Loss: 3.1384263038635254\n",
            "mini Batch 201 Loss: 3.1223177909851074\n",
            "Training Batch: 201 | Training Loss: 3.1223177909851074\n",
            "dist a: tensor([12.6031, 19.2567, 20.5540, 19.3963, 15.9956, 15.4851, 15.4236, 18.3751,\n",
            "        15.6737, 27.5127, 21.3725, 15.0087, 26.4150, 14.3377, 17.8565, 20.3816,\n",
            "        20.3148, 11.8547, 15.4357, 23.1636, 22.2310, 16.2816, 13.7484, 15.3531,\n",
            "        21.0783, 30.6612, 11.8554, 20.0985, 20.3305, 11.9184, 13.8287, 13.8709,\n",
            "        17.0610, 16.1140, 25.2236, 23.2298, 16.1175, 12.7761, 17.1545, 24.1530,\n",
            "        14.5693, 15.0872, 21.3059, 20.4083, 32.9821, 11.3312, 24.5871, 23.1222,\n",
            "        22.0002, 12.0887, 26.7318, 20.2711, 21.9133, 13.5685, 31.1497, 18.4735,\n",
            "        20.2180, 26.8063, 20.4469, 17.0747, 22.4774, 27.4905, 24.5703, 25.7696],\n",
            "       device='cuda:0'), dist b: tensor([42.8054, 30.8110, 32.6723, 18.2945, 21.8333, 20.9599, 31.2688, 25.8606,\n",
            "        25.7739, 31.4140, 13.8682, 22.4247, 21.0690, 20.2709, 28.2554, 15.3396,\n",
            "        19.3725, 33.9707, 18.1077, 29.5068, 26.1330, 33.8867, 20.6081, 19.0256,\n",
            "        18.2416, 35.2941, 11.6843, 25.3507, 17.6123, 19.1842, 23.3626, 13.1195,\n",
            "        38.5050, 24.8280, 26.1359, 19.8209, 28.0420, 13.7449, 21.9447, 23.1824,\n",
            "        16.0834, 20.6945, 15.6461, 20.9345, 19.4723, 29.4610, 29.3292, 27.7879,\n",
            "        24.5561, 25.3581, 36.8310, 19.4015, 19.1957, 19.3528, 13.8245, 24.6874,\n",
            "        25.6524, 29.0301, 16.9645, 25.8728, 19.3337, 17.2349, 22.9673, 26.7096],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.6875 \n",
            "tensor(0.6875)\n",
            "Training Batch: 201 | Model saved to: /content/drive/My Drive/test6/model_epoch_3_batch_201.pt\n",
            "mini Batch 202 Loss: 2.4859962463378906\n",
            "mini Batch 203 Loss: 2.683323860168457\n",
            "mini Batch 204 Loss: 2.8069653511047363\n",
            "mini Batch 205 Loss: 2.781191110610962\n",
            "mini Batch 206 Loss: 4.07762336730957\n",
            "mini Batch 207 Loss: 1.8302603960037231\n",
            "mini Batch 208 Loss: 2.1523349285125732\n",
            "mini Batch 209 Loss: 2.351966381072998\n",
            "mini Batch 210 Loss: 2.0409598350524902\n",
            "mini Batch 211 Loss: 3.722926139831543\n",
            "mini Batch 212 Loss: 1.8790929317474365\n",
            "mini Batch 213 Loss: 3.449423313140869\n",
            "mini Batch 214 Loss: 2.3955273628234863\n",
            "mini Batch 215 Loss: 1.3797039985656738\n",
            "mini Batch 216 Loss: 4.178650856018066\n",
            "mini Batch 217 Loss: 2.423208475112915\n",
            "mini Batch 218 Loss: 0.40073227882385254\n",
            "mini Batch 219 Loss: 1.4832243919372559\n",
            "mini Batch 220 Loss: 1.408089518547058\n",
            "mini Batch 221 Loss: 1.3952598571777344\n",
            "mini Batch 222 Loss: 2.42970609664917\n",
            "mini Batch 223 Loss: 0.851706862449646\n",
            "mini Batch 224 Loss: 1.2142877578735352\n",
            "mini Batch 225 Loss: 4.0594940185546875\n",
            "mini Batch 226 Loss: 1.5820449590682983\n",
            "mini Batch 227 Loss: 1.8427950143814087\n",
            "mini Batch 228 Loss: 0.3632751703262329\n",
            "mini Batch 229 Loss: 2.2664084434509277\n",
            "mini Batch 230 Loss: 0.6502856016159058\n",
            "mini Batch 231 Loss: 0.8349804878234863\n",
            "mini Batch 232 Loss: 0.34769219160079956\n",
            "mini Batch 233 Loss: 3.0109481811523438\n",
            "mini Batch 234 Loss: 2.9456725120544434\n",
            "mini Batch 235 Loss: 2.0446054935455322\n",
            "mini Batch 236 Loss: 1.6824756860733032\n",
            "mini Batch 237 Loss: 1.9804480075836182\n",
            "mini Batch 238 Loss: 1.5223634243011475\n",
            "mini Batch 239 Loss: 2.769620180130005\n",
            "mini Batch 240 Loss: 3.371744155883789\n",
            "mini Batch 241 Loss: 2.6001954078674316\n",
            "mini Batch 242 Loss: 2.2974987030029297\n",
            "mini Batch 243 Loss: 1.3463902473449707\n",
            "mini Batch 244 Loss: 0.9725711345672607\n",
            "mini Batch 245 Loss: 0.11854642629623413\n",
            "mini Batch 246 Loss: 0.4855669140815735\n",
            "mini Batch 247 Loss: 2.099146842956543\n",
            "mini Batch 248 Loss: 2.478026866912842\n",
            "mini Batch 249 Loss: 1.5379104614257812\n",
            "mini Batch 250 Loss: 2.363619327545166\n",
            "mini Batch 251 Loss: 0.10009489953517914\n",
            "mini Batch 252 Loss: 3.8761444091796875\n",
            "mini Batch 253 Loss: 1.5048611164093018\n",
            "mini Batch 254 Loss: 0.8485380411148071\n",
            "mini Batch 255 Loss: 1.7627938985824585\n",
            "mini Batch 256 Loss: 2.0143632888793945\n",
            "mini Batch 257 Loss: 1.2586774826049805\n",
            "mini Batch 258 Loss: 1.3062729835510254\n",
            "mini Batch 259 Loss: 2.296293020248413\n",
            "mini Batch 260 Loss: 1.8561946153640747\n",
            "mini Batch 261 Loss: 0.43365567922592163\n",
            "mini Batch 262 Loss: 0.22411994636058807\n",
            "mini Batch 263 Loss: 2.748110294342041\n",
            "mini Batch 264 Loss: 1.3932042121887207\n",
            "mini Batch 265 Loss: 1.7577614784240723\n",
            "mini Batch 266 Loss: 1.586901307106018\n",
            "mini Batch 267 Loss: 2.216283082962036\n",
            "mini Batch 268 Loss: 1.0350538492202759\n",
            "mini Batch 269 Loss: 0.9267369508743286\n",
            "mini Batch 270 Loss: 1.086169719696045\n",
            "mini Batch 271 Loss: 0.49914348125457764\n",
            "mini Batch 272 Loss: 2.858109951019287\n",
            "mini Batch 273 Loss: 0.5197256803512573\n",
            "mini Batch 274 Loss: 2.014774799346924\n",
            "mini Batch 275 Loss: 0.6185091733932495\n",
            "mini Batch 276 Loss: 0.15679265558719635\n",
            "mini Batch 277 Loss: 2.859272003173828\n",
            "mini Batch 278 Loss: 0.06718289852142334\n",
            "mini Batch 279 Loss: 3.1968259811401367\n",
            "mini Batch 280 Loss: 0.567857027053833\n",
            "mini Batch 281 Loss: 1.674994945526123\n",
            "mini Batch 282 Loss: 0.4359399378299713\n",
            "mini Batch 283 Loss: 1.4834789037704468\n",
            "mini Batch 284 Loss: 2.3794827461242676\n",
            "mini Batch 285 Loss: 2.3425216674804688\n",
            "mini Batch 286 Loss: 3.133035659790039\n",
            "mini Batch 287 Loss: 3.0915894508361816\n",
            "mini Batch 288 Loss: 1.0101056098937988\n",
            "mini Batch 289 Loss: 1.5212860107421875\n",
            "mini Batch 290 Loss: 0.8182513117790222\n",
            "mini Batch 291 Loss: 0.07689997553825378\n",
            "mini Batch 292 Loss: 3.8044192790985107\n",
            "mini Batch 293 Loss: 1.9626989364624023\n",
            "mini Batch 294 Loss: 1.1390290260314941\n",
            "mini Batch 295 Loss: 1.8269909620285034\n",
            "mini Batch 296 Loss: 0.35561835765838623\n",
            "mini Batch 297 Loss: 7.260863304138184\n",
            "mini Batch 298 Loss: 0.5095070004463196\n",
            "mini Batch 299 Loss: 1.690437912940979\n",
            "mini Batch 300 Loss: 0.5192388296127319\n",
            "mini Batch 301 Loss: 2.4851808547973633\n",
            "mini Batch 302 Loss: 1.771315336227417\n",
            "mini Batch 303 Loss: 2.298823595046997\n",
            "mini Batch 304 Loss: 1.1181762218475342\n",
            "mini Batch 305 Loss: 1.7488367557525635\n",
            "mini Batch 306 Loss: 0.9957054257392883\n",
            "mini Batch 307 Loss: 1.6131092309951782\n",
            "mini Batch 308 Loss: 2.3810312747955322\n",
            "mini Batch 309 Loss: 2.4520785808563232\n",
            "mini Batch 310 Loss: 1.1981079578399658\n",
            "mini Batch 311 Loss: 2.286552906036377\n",
            "mini Batch 312 Loss: 1.2286216020584106\n",
            "mini Batch 313 Loss: 2.3996739387512207\n",
            "mini Batch 314 Loss: 2.6411736011505127\n",
            "mini Batch 315 Loss: 0.9776738882064819\n",
            "mini Batch 316 Loss: 1.7433557510375977\n",
            "mini Batch 317 Loss: 3.4946916103363037\n",
            "mini Batch 318 Loss: 4.546505451202393\n",
            "mini Batch 319 Loss: 2.274186134338379\n",
            "mini Batch 320 Loss: 2.31516170501709\n",
            "mini Batch 321 Loss: 1.1010687351226807\n",
            "mini Batch 322 Loss: 1.8706634044647217\n",
            "mini Batch 323 Loss: 1.0518012046813965\n",
            "mini Batch 324 Loss: 1.5721839666366577\n",
            "mini Batch 325 Loss: 1.3618026971817017\n",
            "mini Batch 326 Loss: 0.4745572507381439\n",
            "mini Batch 327 Loss: 1.634303092956543\n",
            "mini Batch 328 Loss: 1.9503705501556396\n",
            "mini Batch 329 Loss: 0.8893583416938782\n",
            "mini Batch 330 Loss: 1.8381261825561523\n",
            "mini Batch 331 Loss: 2.6383309364318848\n",
            "mini Batch 332 Loss: 0.46383294463157654\n",
            "mini Batch 333 Loss: 1.141386866569519\n",
            "mini Batch 334 Loss: 1.824622392654419\n",
            "mini Batch 335 Loss: 1.1708416938781738\n",
            "mini Batch 336 Loss: 0.40378159284591675\n",
            "mini Batch 337 Loss: 1.238966941833496\n",
            "mini Batch 338 Loss: 1.0467867851257324\n",
            "mini Batch 339 Loss: 2.944492816925049\n",
            "mini Batch 340 Loss: 1.3277273178100586\n",
            "mini Batch 341 Loss: 0.5420331954956055\n",
            "mini Batch 342 Loss: 0.7947242856025696\n",
            "mini Batch 343 Loss: 3.8406622409820557\n",
            "mini Batch 344 Loss: 0.5595241785049438\n",
            "mini Batch 345 Loss: 0.1573101282119751\n",
            "mini Batch 346 Loss: 1.7979531288146973\n",
            "mini Batch 347 Loss: 2.5902626514434814\n",
            "mini Batch 348 Loss: 2.590121269226074\n",
            "mini Batch 349 Loss: 2.00563907623291\n",
            "mini Batch 350 Loss: 1.4935886859893799\n",
            "mini Batch 351 Loss: 0.25209885835647583\n",
            "mini Batch 352 Loss: 4.680152893066406\n",
            "mini Batch 353 Loss: 1.3827431201934814\n",
            "mini Batch 354 Loss: 0.04937992990016937\n",
            "mini Batch 355 Loss: 3.5965569019317627\n",
            "mini Batch 356 Loss: 1.6167172193527222\n",
            "mini Batch 357 Loss: 1.3595635890960693\n",
            "mini Batch 358 Loss: 0.39329785108566284\n",
            "mini Batch 359 Loss: 3.2654333114624023\n",
            "mini Batch 360 Loss: 1.140871524810791\n",
            "mini Batch 361 Loss: 0.9150373935699463\n",
            "mini Batch 362 Loss: 2.2074360847473145\n",
            "mini Batch 363 Loss: 4.9358415603637695\n",
            "mini Batch 364 Loss: 2.6912922859191895\n",
            "mini Batch 365 Loss: 2.258848190307617\n",
            "mini Batch 366 Loss: 0.6656772494316101\n",
            "mini Batch 367 Loss: 1.6917521953582764\n",
            "mini Batch 368 Loss: 1.267606258392334\n",
            "mini Batch 369 Loss: 2.2098679542541504\n",
            "mini Batch 370 Loss: 0.37567102909088135\n",
            "mini Batch 371 Loss: 2.692631721496582\n",
            "mini Batch 372 Loss: 0.3953305780887604\n",
            "mini Batch 373 Loss: 2.26112699508667\n",
            "mini Batch 374 Loss: 1.7590346336364746\n",
            "mini Batch 375 Loss: 1.692946434020996\n",
            "mini Batch 376 Loss: 0.5393919944763184\n",
            "mini Batch 377 Loss: 0.9017849564552307\n",
            "mini Batch 378 Loss: 0.8066663146018982\n",
            "mini Batch 379 Loss: 0.345225989818573\n",
            "mini Batch 380 Loss: 0.21671386063098907\n",
            "mini Batch 381 Loss: 1.1427291631698608\n",
            "mini Batch 382 Loss: 0.4805179834365845\n",
            "mini Batch 383 Loss: 0.15784411132335663\n",
            "mini Batch 384 Loss: 0.20355060696601868\n",
            "mini Batch 385 Loss: 0.6853252649307251\n",
            "mini Batch 386 Loss: 3.71267032623291\n",
            "mini Batch 387 Loss: 0.0\n",
            "mini Batch 388 Loss: 2.5350093841552734\n",
            "mini Batch 389 Loss: 1.10710608959198\n",
            "mini Batch 390 Loss: 0.14721383154392242\n",
            "mini Batch 391 Loss: 0.11532615125179291\n",
            "mini Batch 392 Loss: 0.4211829900741577\n",
            "mini Batch 393 Loss: 0.025240719318389893\n",
            "mini Batch 394 Loss: 3.8911216259002686\n",
            "mini Batch 395 Loss: 3.3413147926330566\n",
            "mini Batch 396 Loss: 3.0072100162506104\n",
            "mini Batch 397 Loss: 0.13163182139396667\n",
            "mini Batch 398 Loss: 0.2088441401720047\n",
            "mini Batch 399 Loss: 5.10374641418457\n",
            "mini Batch 400 Loss: 0.652583658695221\n",
            "mini Batch 401 Loss: 1.3518846035003662\n",
            "Training Batch: 401 | Training Loss: 1.3518846035003662\n",
            "dist a: tensor([ 9.8099, 13.2229, 13.7595, 10.5795, 13.5781, 12.4331, 11.5253, 12.3422,\n",
            "        10.8949, 16.6513, 22.3899, 11.3428, 19.2627, 10.2249, 13.4609, 11.2520,\n",
            "        18.1311, 10.2032, 10.0318, 20.1488, 15.2207,  9.2534, 10.4331,  9.9889,\n",
            "        16.0546, 21.9381,  9.5594, 13.4285, 14.2420, 10.4627, 11.5808,  9.6030,\n",
            "         8.6704,  9.2660, 14.9025,  9.4625,  8.8025,  7.6138, 13.0046, 16.4759,\n",
            "         9.5983,  9.4934, 16.6858,  9.4256, 21.9140, 12.8135, 21.2930, 25.5708,\n",
            "        18.8984,  7.5370, 19.7361, 16.9912, 14.2826, 10.4041, 21.2872, 14.4379,\n",
            "        10.7562, 19.5555, 22.1442,  9.9574, 17.1477, 19.9686, 20.9621, 20.0808],\n",
            "       device='cuda:0'), dist b: tensor([24.4806, 19.0108, 23.5764, 12.0984, 16.7835, 16.7275, 19.3827, 19.1439,\n",
            "        18.2658, 22.7990, 13.6012, 17.1614, 19.2745, 18.2465, 22.7155, 13.4544,\n",
            "        16.8336, 25.7690, 16.0792, 26.4181, 23.4965, 21.0524, 17.6058, 18.2133,\n",
            "        16.0329, 23.6003,  7.8212, 25.8804, 10.7190, 16.0015, 18.4622,  9.1310,\n",
            "        25.5642, 20.2978, 18.3191, 13.9863, 18.3760,  7.9611, 18.1855, 17.9807,\n",
            "        10.1571, 13.4587,  9.7729, 17.1631, 15.3775, 24.3251, 20.5836, 21.0719,\n",
            "        22.7076, 22.1731, 25.5335, 12.6852, 14.8856, 13.3450, 10.0044, 19.1580,\n",
            "        21.4705, 15.4293, 12.4157, 18.6258, 12.8625, 13.7573, 21.4679, 13.4715],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "Training Batch: 401 | Model saved to: /content/drive/My Drive/test6/model_epoch_3_batch_401.pt\n",
            "mini Batch 402 Loss: 0.508043646812439\n",
            "mini Batch 403 Loss: 1.724822998046875\n",
            "mini Batch 404 Loss: 1.961266279220581\n",
            "mini Batch 405 Loss: 1.3347570896148682\n",
            "mini Batch 406 Loss: 2.1991629600524902\n",
            "mini Batch 407 Loss: 1.4400699138641357\n",
            "mini Batch 408 Loss: 0.48156023025512695\n",
            "mini Batch 409 Loss: 1.281659483909607\n",
            "mini Batch 410 Loss: 1.3979992866516113\n",
            "mini Batch 411 Loss: 2.9705090522766113\n",
            "mini Batch 412 Loss: 2.770388603210449\n",
            "mini Batch 413 Loss: 3.4761486053466797\n",
            "mini Batch 414 Loss: 2.20707106590271\n",
            "mini Batch 415 Loss: 3.1245365142822266\n",
            "mini Batch 416 Loss: 3.544530153274536\n",
            "mini Batch 417 Loss: 3.5737152099609375\n",
            "mini Batch 418 Loss: 3.0638508796691895\n",
            "mini Batch 419 Loss: 3.3471484184265137\n",
            "mini Batch 420 Loss: 3.630404472351074\n",
            "mini Batch 421 Loss: 2.5257978439331055\n",
            "mini Batch 422 Loss: 2.600633144378662\n",
            "mini Batch 423 Loss: 2.70833683013916\n",
            "mini Batch 424 Loss: 3.733224391937256\n",
            "mini Batch 425 Loss: 3.381326913833618\n",
            "mini Batch 426 Loss: 2.831003189086914\n",
            "mini Batch 427 Loss: 1.693814992904663\n",
            "mini Batch 428 Loss: 0.7063666582107544\n",
            "mini Batch 429 Loss: 1.1123926639556885\n",
            "mini Batch 430 Loss: 1.7857173681259155\n",
            "mini Batch 431 Loss: 3.1732053756713867\n",
            "mini Batch 432 Loss: 3.4868836402893066\n",
            "mini Batch 433 Loss: 1.780443549156189\n",
            "mini Batch 434 Loss: 1.0213524103164673\n",
            "mini Batch 435 Loss: 0.3741688132286072\n",
            "mini Batch 436 Loss: 0.6441315412521362\n",
            "mini Batch 437 Loss: 1.4358729124069214\n",
            "mini Batch 438 Loss: 3.109973669052124\n",
            "mini Batch 439 Loss: 2.639725685119629\n",
            "mini Batch 440 Loss: 2.7454957962036133\n",
            "mini Batch 441 Loss: 2.0770673751831055\n",
            "mini Batch 442 Loss: 1.2844780683517456\n",
            "mini Batch 443 Loss: 2.1876819133758545\n",
            "mini Batch 444 Loss: 1.0599277019500732\n",
            "mini Batch 445 Loss: 1.7633205652236938\n",
            "mini Batch 446 Loss: 1.0673911571502686\n",
            "mini Batch 447 Loss: 0.0\n",
            "mini Batch 448 Loss: 1.4212442636489868\n",
            "mini Batch 449 Loss: 0.14654132723808289\n",
            "mini Batch 450 Loss: 0.13874921202659607\n",
            "mini Batch 451 Loss: 0.7338100671768188\n",
            "mini Batch 452 Loss: 0.8384983539581299\n",
            "mini Batch 453 Loss: 0.8284034729003906\n",
            "mini Batch 454 Loss: 0.8945832252502441\n",
            "mini Batch 455 Loss: 2.511920690536499\n",
            "mini Batch 456 Loss: 0.5637065172195435\n",
            "mini Batch 457 Loss: 2.951131582260132\n",
            "mini Batch 458 Loss: 0.6000199317932129\n",
            "mini Batch 459 Loss: 0.11717802286148071\n",
            "mini Batch 460 Loss: 1.6351723670959473\n",
            "mini Batch 461 Loss: 6.0696210861206055\n",
            "mini Batch 462 Loss: 3.456420660018921\n",
            "mini Batch 463 Loss: 3.8985509872436523\n",
            "mini Batch 464 Loss: 3.5076847076416016\n",
            "mini Batch 465 Loss: 3.220496416091919\n",
            "mini Batch 466 Loss: 2.8117902278900146\n",
            "mini Batch 467 Loss: 2.7132301330566406\n",
            "mini Batch 468 Loss: 2.5489039421081543\n",
            "mini Batch 469 Loss: 3.724470615386963\n",
            "mini Batch 470 Loss: 3.7254762649536133\n",
            "mini Batch 471 Loss: 3.2331793308258057\n",
            "mini Batch 472 Loss: 3.217848777770996\n",
            "mini Batch 473 Loss: 3.4660305976867676\n",
            "mini Batch 474 Loss: 3.153376817703247\n",
            "mini Batch 475 Loss: 3.6604933738708496\n",
            "mini Batch 476 Loss: 3.972940683364868\n",
            "mini Batch 477 Loss: 3.889197587966919\n",
            "mini Batch 478 Loss: 3.5737390518188477\n",
            "mini Batch 479 Loss: 2.1988863945007324\n",
            "mini Batch 480 Loss: 2.8492331504821777\n",
            "mini Batch 481 Loss: 2.9440221786499023\n",
            "mini Batch 482 Loss: 3.789419651031494\n",
            "mini Batch 483 Loss: 3.229433298110962\n",
            "mini Batch 484 Loss: 2.928234577178955\n",
            "mini Batch 485 Loss: 3.8223061561584473\n",
            "mini Batch 486 Loss: 3.2686853408813477\n",
            "mini Batch 487 Loss: 3.4085135459899902\n",
            "mini Batch 488 Loss: 2.598121166229248\n",
            "mini Batch 489 Loss: 3.222275733947754\n",
            "mini Batch 490 Loss: 3.1938247680664062\n",
            "mini Batch 491 Loss: 3.784494638442993\n",
            "mini Batch 492 Loss: 2.845308780670166\n",
            "mini Batch 493 Loss: 3.3395729064941406\n",
            "mini Batch 494 Loss: 3.395519733428955\n",
            "mini Batch 495 Loss: 3.103271484375\n",
            "mini Batch 496 Loss: 3.3029136657714844\n",
            "mini Batch 497 Loss: 3.369713544845581\n",
            "mini Batch 498 Loss: 3.4496493339538574\n",
            "mini Batch 499 Loss: 2.468506336212158\n",
            "mini Batch 500 Loss: 3.105180025100708\n",
            "mini Batch 501 Loss: 2.07421875\n",
            "mini Batch 502 Loss: 3.9491727352142334\n",
            "mini Batch 503 Loss: 3.5063135623931885\n",
            "mini Batch 504 Loss: 2.4824280738830566\n",
            "mini Batch 505 Loss: 2.6498374938964844\n",
            "mini Batch 506 Loss: 2.919985294342041\n",
            "mini Batch 507 Loss: 2.940019130706787\n",
            "mini Batch 508 Loss: 4.045828819274902\n",
            "mini Batch 509 Loss: 2.7571516036987305\n",
            "mini Batch 510 Loss: 2.9393301010131836\n",
            "mini Batch 511 Loss: 3.255114793777466\n",
            "mini Batch 512 Loss: 2.849264144897461\n",
            "mini Batch 513 Loss: 3.1653640270233154\n",
            "mini Batch 514 Loss: 3.213662624359131\n",
            "mini Batch 515 Loss: 2.3749277591705322\n",
            "mini Batch 516 Loss: 2.797807216644287\n",
            "mini Batch 517 Loss: 3.0342984199523926\n",
            "mini Batch 518 Loss: 3.3309712409973145\n",
            "mini Batch 519 Loss: 2.1921472549438477\n",
            "mini Batch 520 Loss: 3.617640972137451\n",
            "mini Batch 521 Loss: 2.6810708045959473\n",
            "mini Batch 522 Loss: 2.2910220623016357\n",
            "mini Batch 523 Loss: 2.753002405166626\n",
            "mini Batch 524 Loss: 2.7164573669433594\n",
            "mini Batch 525 Loss: 3.059530258178711\n",
            "mini Batch 526 Loss: 2.1259474754333496\n",
            "mini Batch 527 Loss: 2.5318028926849365\n",
            "mini Batch 528 Loss: 2.5883514881134033\n",
            "mini Batch 529 Loss: 2.941084623336792\n",
            "mini Batch 530 Loss: 2.9440650939941406\n",
            "mini Batch 531 Loss: 3.332882881164551\n",
            "mini Batch 532 Loss: 2.070223331451416\n",
            "mini Batch 533 Loss: 2.8251101970672607\n",
            "mini Batch 534 Loss: 2.7185401916503906\n",
            "mini Batch 535 Loss: 1.9720104932785034\n",
            "mini Batch 536 Loss: 3.077176570892334\n",
            "mini Batch 537 Loss: 3.5876669883728027\n",
            "mini Batch 538 Loss: 2.131983757019043\n",
            "mini Batch 539 Loss: 2.2054243087768555\n",
            "mini Batch 540 Loss: 2.0646684169769287\n",
            "mini Batch 541 Loss: 2.1151492595672607\n",
            "mini Batch 542 Loss: 2.488342761993408\n",
            "mini Batch 543 Loss: 2.4998459815979004\n",
            "mini Batch 544 Loss: 2.2411060333251953\n",
            "mini Batch 545 Loss: 2.492030620574951\n",
            "mini Batch 546 Loss: 3.0072543621063232\n",
            "mini Batch 547 Loss: 2.5130538940429688\n",
            "mini Batch 548 Loss: 3.6113271713256836\n",
            "mini Batch 549 Loss: 2.7138407230377197\n",
            "mini Batch 550 Loss: 3.517261028289795\n",
            "mini Batch 551 Loss: 1.5461854934692383\n",
            "mini Batch 552 Loss: 2.4197254180908203\n",
            "mini Batch 553 Loss: 2.4495291709899902\n",
            "mini Batch 554 Loss: 2.77687406539917\n",
            "mini Batch 555 Loss: 1.8636085987091064\n",
            "mini Batch 556 Loss: 2.4508533477783203\n",
            "mini Batch 557 Loss: 1.9305644035339355\n",
            "mini Batch 558 Loss: 1.7078742980957031\n",
            "mini Batch 559 Loss: 3.691661834716797\n",
            "mini Batch 560 Loss: 1.938401460647583\n",
            "mini Batch 561 Loss: 2.2483434677124023\n",
            "mini Batch 562 Loss: 1.6177055835723877\n",
            "mini Batch 563 Loss: 2.0953640937805176\n",
            "mini Batch 564 Loss: 3.010869026184082\n",
            "mini Batch 565 Loss: 3.2262749671936035\n",
            "mini Batch 566 Loss: 3.8876752853393555\n",
            "mini Batch 567 Loss: 2.4905688762664795\n",
            "mini Batch 568 Loss: 2.986664295196533\n",
            "mini Batch 569 Loss: 3.308685302734375\n",
            "mini Batch 570 Loss: 3.4781415462493896\n",
            "mini Batch 571 Loss: 2.6848196983337402\n",
            "mini Batch 572 Loss: 2.8635313510894775\n",
            "mini Batch 573 Loss: 2.0618643760681152\n",
            "mini Batch 574 Loss: 1.6976597309112549\n",
            "mini Batch 575 Loss: 1.5815774202346802\n",
            "mini Batch 576 Loss: 1.3414669036865234\n",
            "mini Batch 577 Loss: 1.8889483213424683\n",
            "mini Batch 578 Loss: 1.769702672958374\n",
            "mini Batch 579 Loss: 1.8446319103240967\n",
            "mini Batch 580 Loss: 0.9680831432342529\n",
            "mini Batch 581 Loss: 1.2191270589828491\n",
            "mini Batch 582 Loss: 2.4408936500549316\n",
            "mini Batch 583 Loss: 2.008667230606079\n",
            "mini Batch 584 Loss: 3.1956865787506104\n",
            "mini Batch 585 Loss: 2.5364620685577393\n",
            "mini Batch 586 Loss: 2.9490368366241455\n",
            "mini Batch 587 Loss: 1.664028525352478\n",
            "mini Batch 588 Loss: 1.9425923824310303\n",
            "mini Batch 589 Loss: 4.145161151885986\n",
            "mini Batch 590 Loss: 0.8568755388259888\n",
            "mini Batch 591 Loss: 1.3069424629211426\n",
            "mini Batch 592 Loss: 1.6498340368270874\n",
            "mini Batch 593 Loss: 2.278879165649414\n",
            "mini Batch 594 Loss: 3.3001043796539307\n",
            "mini Batch 595 Loss: 2.999645233154297\n",
            "mini Batch 596 Loss: 2.7921600341796875\n",
            "mini Batch 597 Loss: 1.0855556726455688\n",
            "mini Batch 598 Loss: 1.5370084047317505\n",
            "mini Batch 599 Loss: 2.1836750507354736\n",
            "mini Batch 600 Loss: 2.7932167053222656\n",
            "mini Batch 601 Loss: 2.333700656890869\n",
            "Training Batch: 601 | Training Loss: 2.333700656890869\n",
            "dist a: tensor([15.1439, 15.3928, 16.2647, 17.0705, 12.7482, 12.6832, 17.8240, 18.6447,\n",
            "        11.5758, 16.4914, 16.7260, 12.4328, 21.8306,  9.7281, 11.8970, 13.4452,\n",
            "        24.5927, 11.5694, 10.6305, 22.6556, 16.9766, 12.3781, 12.3921, 13.6924,\n",
            "        20.5882, 24.5190,  9.6078, 15.0018, 16.8534, 12.3379, 15.3083, 11.1062,\n",
            "        12.5116, 13.3662, 21.6527, 12.1986, 11.2769, 10.1375, 13.3241, 17.1953,\n",
            "        11.8102, 15.4700, 16.2640, 11.5019, 22.1152,  9.7257, 17.2204, 18.5779,\n",
            "        16.1460, 10.6159, 27.9634, 15.5286, 17.9884,  9.6491, 22.3102, 13.5657,\n",
            "        14.9917, 19.2200, 18.3300, 13.8352, 17.8374, 16.8937, 20.9181, 19.3903],\n",
            "       device='cuda:0'), dist b: tensor([31.7801, 18.5542, 23.3114, 13.7028, 17.7643, 16.0085, 27.3528, 19.2003,\n",
            "        20.7964, 21.6816, 16.6335, 19.4824, 19.4226, 13.4331, 20.3187, 17.2724,\n",
            "        21.8567, 25.6495, 17.8676, 19.7766, 22.6926, 23.8475, 19.5882, 18.2981,\n",
            "        18.2538, 26.2017, 10.1943, 23.4432, 11.9834, 18.0936, 18.1034, 10.2414,\n",
            "        26.2096, 21.9240, 20.2910, 16.6801, 20.9327, 12.6238, 16.8717, 24.6309,\n",
            "        11.4262, 18.7956, 10.6117, 15.9742, 19.7099, 19.7083, 21.2906, 22.2815,\n",
            "        24.0334, 21.9169, 30.9896, 12.0723, 17.9793, 12.6183,  9.5653, 17.3079,\n",
            "        17.1207, 16.5769, 13.7295, 23.5542, 13.7962, 16.1010, 22.8987, 18.5202],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.6875 \n",
            "tensor(0.6875)\n",
            "Training Batch: 601 | Model saved to: /content/drive/My Drive/test6/model_epoch_3_batch_601.pt\n",
            "mini Batch 602 Loss: 2.2546703815460205\n",
            "mini Batch 603 Loss: 2.251372814178467\n",
            "mini Batch 604 Loss: 3.2070541381835938\n",
            "mini Batch 605 Loss: 3.122241497039795\n",
            "mini Batch 606 Loss: 2.3513591289520264\n",
            "mini Batch 607 Loss: 0.9000540971755981\n",
            "mini Batch 608 Loss: 2.805856704711914\n",
            "mini Batch 609 Loss: 0.9887793660163879\n",
            "mini Batch 610 Loss: 2.6219425201416016\n",
            "mini Batch 611 Loss: 1.4194486141204834\n",
            "mini Batch 612 Loss: 1.1794495582580566\n",
            "mini Batch 613 Loss: 0.9494326114654541\n",
            "mini Batch 614 Loss: 2.9233765602111816\n",
            "mini Batch 615 Loss: 4.015933036804199\n",
            "mini Batch 616 Loss: 0.8178739547729492\n",
            "mini Batch 617 Loss: 2.0328211784362793\n",
            "mini Batch 618 Loss: 1.7494065761566162\n",
            "mini Batch 619 Loss: 1.6436119079589844\n",
            "mini Batch 620 Loss: 2.1179726123809814\n",
            "mini Batch 621 Loss: 3.95029878616333\n",
            "mini Batch 622 Loss: 3.3137710094451904\n",
            "mini Batch 623 Loss: 1.9266678094863892\n",
            "mini Batch 624 Loss: 4.53006649017334\n",
            "mini Batch 625 Loss: 4.741334915161133\n",
            "mini Batch 626 Loss: 1.455548882484436\n",
            "mini Batch 627 Loss: 2.4057912826538086\n",
            "mini Batch 628 Loss: 2.8264265060424805\n",
            "mini Batch 629 Loss: 2.460508346557617\n",
            "mini Batch 630 Loss: 2.7632927894592285\n",
            "mini Batch 631 Loss: 1.6192609071731567\n",
            "mini Batch 632 Loss: 2.2837283611297607\n",
            "mini Batch 633 Loss: 1.1721198558807373\n",
            "mini Batch 634 Loss: 1.3447222709655762\n",
            "mini Batch 635 Loss: 1.751733660697937\n",
            "mini Batch 636 Loss: 1.7119271755218506\n",
            "mini Batch 637 Loss: 0.48603391647338867\n",
            "mini Batch 638 Loss: 1.1248748302459717\n",
            "mini Batch 639 Loss: 1.5546488761901855\n",
            "mini Batch 640 Loss: 2.406041145324707\n",
            "mini Batch 641 Loss: 1.8577532768249512\n",
            "mini Batch 642 Loss: 2.4622936248779297\n",
            "mini Batch 643 Loss: 2.6234676837921143\n",
            "mini Batch 644 Loss: 3.288539171218872\n",
            "mini Batch 645 Loss: 2.1866703033447266\n",
            "mini Batch 646 Loss: 1.797196865081787\n",
            "mini Batch 647 Loss: 2.390270233154297\n",
            "mini Batch 648 Loss: 2.610349178314209\n",
            "mini Batch 649 Loss: 0.5369981527328491\n",
            "mini Batch 650 Loss: 0.42542415857315063\n",
            "mini Batch 651 Loss: 2.4068329334259033\n",
            "mini Batch 652 Loss: 0.18059536814689636\n",
            "mini Batch 653 Loss: 0.5959863662719727\n",
            "mini Batch 654 Loss: 2.7453560829162598\n",
            "mini Batch 655 Loss: 0.17865119874477386\n",
            "mini Batch 656 Loss: 2.6929218769073486\n",
            "mini Batch 657 Loss: 1.6985390186309814\n",
            "mini Batch 658 Loss: 0.8699058294296265\n",
            "mini Batch 659 Loss: 4.323630332946777\n",
            "mini Batch 660 Loss: 2.3180575370788574\n",
            "mini Batch 661 Loss: 2.0535242557525635\n",
            "mini Batch 662 Loss: 1.4831100702285767\n",
            "mini Batch 663 Loss: 0.8385511040687561\n",
            "mini Batch 664 Loss: 0.6982022523880005\n",
            "mini Batch 665 Loss: 0.7896480560302734\n",
            "mini Batch 666 Loss: 3.7041115760803223\n",
            "mini Batch 667 Loss: 0.04201166331768036\n",
            "mini Batch 668 Loss: 3.1823933124542236\n",
            "mini Batch 669 Loss: 2.5725960731506348\n",
            "mini Batch 670 Loss: 1.736222743988037\n",
            "mini Batch 671 Loss: 1.510077714920044\n",
            "mini Batch 672 Loss: 0.9842387437820435\n",
            "mini Batch 673 Loss: 3.188997745513916\n",
            "mini Batch 674 Loss: 2.930241584777832\n",
            "mini Batch 675 Loss: 0.7997413277626038\n",
            "mini Batch 676 Loss: 2.9910783767700195\n",
            "mini Batch 677 Loss: 2.4900894165039062\n",
            "mini Batch 678 Loss: 2.262709379196167\n",
            "mini Batch 679 Loss: 2.0433549880981445\n",
            "mini Batch 680 Loss: 0.7850823402404785\n",
            "mini Batch 681 Loss: 1.9205372333526611\n",
            "mini Batch 682 Loss: 2.1159348487854004\n",
            "mini Batch 683 Loss: 3.854165554046631\n",
            "mini Batch 684 Loss: 1.8873029947280884\n",
            "mini Batch 685 Loss: 2.4674205780029297\n",
            "mini Batch 686 Loss: 0.2311771661043167\n",
            "mini Batch 687 Loss: 3.259950637817383\n",
            "mini Batch 688 Loss: 1.1462445259094238\n",
            "mini Batch 689 Loss: 1.522965669631958\n",
            "mini Batch 690 Loss: 0.8996485471725464\n",
            "mini Batch 691 Loss: 2.449108839035034\n",
            "mini Batch 692 Loss: 3.3874895572662354\n",
            "mini Batch 693 Loss: 1.3259308338165283\n",
            "mini Batch 694 Loss: 2.533019781112671\n",
            "mini Batch 695 Loss: 2.9557552337646484\n",
            "mini Batch 696 Loss: 2.3194069862365723\n",
            "mini Batch 697 Loss: 0.0972311794757843\n",
            "mini Batch 698 Loss: 0.9552789926528931\n",
            "mini Batch 699 Loss: 0.1994091421365738\n",
            "mini Batch 700 Loss: 0.6880965232849121\n",
            "mini Batch 701 Loss: 0.3027157187461853\n",
            "mini Batch 702 Loss: 0.8359878063201904\n",
            "mini Batch 703 Loss: 2.118704319000244\n",
            "mini Batch 704 Loss: 1.770214557647705\n",
            "mini Batch 705 Loss: 3.1656010150909424\n",
            "mini Batch 706 Loss: 2.093489408493042\n",
            "mini Batch 707 Loss: 2.547616720199585\n",
            "mini Batch 708 Loss: 3.2925379276275635\n",
            "mini Batch 709 Loss: 0.6851048469543457\n",
            "mini Batch 710 Loss: 1.4456502199172974\n",
            "mini Batch 711 Loss: 1.4021601676940918\n",
            "mini Batch 712 Loss: 3.1305227279663086\n",
            "mini Batch 713 Loss: 0.525557816028595\n",
            "mini Batch 714 Loss: 2.159290313720703\n",
            "mini Batch 715 Loss: 2.264453649520874\n",
            "mini Batch 716 Loss: 0.49853983521461487\n",
            "mini Batch 717 Loss: 2.9864931106567383\n",
            "mini Batch 718 Loss: 0.4094029664993286\n",
            "mini Batch 719 Loss: 1.0414661169052124\n",
            "mini Batch 720 Loss: 0.1095542162656784\n",
            "mini Batch 721 Loss: 5.161582946777344\n",
            "mini Batch 722 Loss: 1.6506476402282715\n",
            "mini Batch 723 Loss: 3.7131874561309814\n",
            "mini Batch 724 Loss: 1.3085229396820068\n",
            "mini Batch 725 Loss: 0.3480316400527954\n",
            "mini Batch 726 Loss: 2.5114502906799316\n",
            "mini Batch 727 Loss: 0.5866033434867859\n",
            "mini Batch 728 Loss: 0.8089661598205566\n",
            "mini Batch 729 Loss: 0.0\n",
            "mini Batch 730 Loss: 0.7061574459075928\n",
            "mini Batch 731 Loss: 0.9309165477752686\n",
            "mini Batch 732 Loss: 2.0274882316589355\n",
            "mini Batch 733 Loss: 1.406298279762268\n",
            "mini Batch 734 Loss: 0.9839748740196228\n",
            "mini Batch 735 Loss: 0.885948896408081\n",
            "mini Batch 736 Loss: 2.133514404296875\n",
            "mini Batch 737 Loss: 8.150074005126953\n",
            "mini Batch 738 Loss: 2.907151699066162\n",
            "mini Batch 739 Loss: 3.6154892444610596\n",
            "mini Batch 740 Loss: 2.4627671241760254\n",
            "mini Batch 741 Loss: 3.420060634613037\n",
            "mini Batch 742 Loss: 0.5625122785568237\n",
            "mini Batch 743 Loss: 3.06119966506958\n",
            "mini Batch 744 Loss: 1.641119122505188\n",
            "mini Batch 745 Loss: 0.6153257489204407\n",
            "mini Batch 746 Loss: 1.4993371963500977\n",
            "mini Batch 747 Loss: 1.222051739692688\n",
            "mini Batch 748 Loss: 1.5705680847167969\n",
            "mini Batch 749 Loss: 1.1214923858642578\n",
            "mini Batch 750 Loss: 1.7435568571090698\n",
            "mini Batch 751 Loss: 0.2911164164543152\n",
            "mini Batch 752 Loss: 1.6924527883529663\n",
            "mini Batch 753 Loss: 2.4028878211975098\n",
            "mini Batch 754 Loss: 1.4298806190490723\n",
            "mini Batch 755 Loss: 1.26283860206604\n",
            "mini Batch 756 Loss: 0.49651476740837097\n",
            "mini Batch 757 Loss: 3.2640833854675293\n",
            "mini Batch 758 Loss: 0.9055811762809753\n",
            "mini Batch 759 Loss: 1.9397799968719482\n",
            "mini Batch 760 Loss: 0.9549246430397034\n",
            "mini Batch 761 Loss: 0.8899719715118408\n",
            "mini Batch 762 Loss: 1.0750372409820557\n",
            "mini Batch 763 Loss: 0.09087672829627991\n",
            "mini Batch 764 Loss: 2.8985652923583984\n",
            "mini Batch 765 Loss: 1.2057265043258667\n",
            "mini Batch 766 Loss: 3.869685173034668\n",
            "mini Batch 767 Loss: 0.35736799240112305\n",
            "mini Batch 768 Loss: 1.1322693824768066\n",
            "mini Batch 769 Loss: 0.5193493366241455\n",
            "mini Batch 770 Loss: 0.0942067950963974\n",
            "mini Batch 771 Loss: 0.4603862464427948\n",
            "mini Batch 772 Loss: 0.15146754682064056\n",
            "mini Batch 773 Loss: 2.5540432929992676\n",
            "mini Batch 774 Loss: 1.2421505451202393\n",
            "mini Batch 775 Loss: 1.1404258012771606\n",
            "mini Batch 776 Loss: 0.19376879930496216\n",
            "mini Batch 777 Loss: 0.24007026851177216\n",
            "mini Batch 778 Loss: 1.2340035438537598\n",
            "mini Batch 779 Loss: 2.084054946899414\n",
            "mini Batch 780 Loss: 0.5814981460571289\n",
            "mini Batch 781 Loss: 0.5680757761001587\n",
            "mini Batch 782 Loss: 4.302796363830566\n",
            "mini Batch 783 Loss: 1.2978031635284424\n",
            "mini Batch 784 Loss: 0.9860752820968628\n",
            "mini Batch 785 Loss: 0.17481893301010132\n",
            "mini Batch 786 Loss: 0.14365838468074799\n",
            "mini Batch 787 Loss: 0.10121248662471771\n",
            "mini Batch 788 Loss: 2.7530248165130615\n",
            "mini Batch 789 Loss: 2.7254738807678223\n",
            "mini Batch 790 Loss: 2.574054718017578\n",
            "mini Batch 791 Loss: 2.6096692085266113\n",
            "mini Batch 792 Loss: 4.313941478729248\n",
            "mini Batch 793 Loss: 0.702969491481781\n",
            "mini Batch 794 Loss: 0.8498678803443909\n",
            "mini Batch 795 Loss: 0.29387331008911133\n",
            "mini Batch 796 Loss: 0.5909019708633423\n",
            "mini Batch 797 Loss: 2.746537446975708\n",
            "mini Batch 798 Loss: 2.4963483810424805\n",
            "mini Batch 799 Loss: 0.7255563735961914\n",
            "mini Batch 800 Loss: 1.6700732707977295\n",
            "mini Batch 801 Loss: 3.060981035232544\n",
            "Training Batch: 801 | Training Loss: 3.060981035232544\n",
            "dist a: tensor([ 8.5161, 14.2902, 10.9866, 10.6121, 11.6019,  8.7669,  9.4231, 13.3558,\n",
            "        11.0077, 15.3071, 14.3189, 10.6902, 13.8574,  9.8114, 10.3871, 11.1539,\n",
            "        16.3022,  7.8408,  8.0227, 16.8905, 14.7726,  8.7495,  8.0299,  9.3407,\n",
            "        13.9759, 20.4976,  7.0108, 12.8360, 14.9433,  7.5595,  9.6464,  9.5449,\n",
            "         9.5717, 14.0891, 11.3423, 11.4079,  9.4720,  7.3793, 10.0298, 13.5063,\n",
            "        10.8033,  9.1109, 15.1075, 11.3850, 19.6870,  8.8625, 16.8312, 19.7279,\n",
            "        13.4674,  6.2472, 18.1912, 11.0046, 11.7930,  8.3078, 15.2267,  9.4616,\n",
            "        12.7891, 18.7224, 14.7079, 12.6491, 12.7902, 14.2420, 13.4805, 13.8279],\n",
            "       device='cuda:0'), dist b: tensor([18.9336, 15.2602, 21.6280,  8.9755, 16.2802, 15.4638, 14.6538, 16.0994,\n",
            "        19.0614, 17.5267, 10.5262, 16.8986, 15.6691, 14.3727, 19.5711, 11.0871,\n",
            "        14.1766, 20.8472, 16.1595, 19.6161, 19.6097, 19.8390, 14.3469, 14.4145,\n",
            "        16.6941, 20.7698, 10.0591, 22.7658, 10.9168, 12.4733, 16.0158,  8.6675,\n",
            "        19.2040, 18.6478, 19.7020, 14.2408, 13.5906,  8.8682, 11.8960, 17.5171,\n",
            "        10.6815, 11.0106,  9.4408, 17.1260, 13.4611, 18.7873, 19.1576, 18.1526,\n",
            "        14.9828, 16.4049, 24.2374,  9.6901, 10.4799,  9.7016,  8.8631, 13.7583,\n",
            "        15.1158, 16.6573, 10.8127, 19.4697, 15.1293, 12.4670, 15.4521, 18.0396],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "Training Batch: 801 | Model saved to: /content/drive/My Drive/test6/model_epoch_3_batch_801.pt\n",
            "mini Batch 802 Loss: 3.0211873054504395\n",
            "mini Batch 803 Loss: 0.1219186931848526\n",
            "mini Batch 804 Loss: 0.6921478509902954\n",
            "mini Batch 805 Loss: 0.07678832113742828\n",
            "mini Batch 806 Loss: 0.057515427470207214\n",
            "mini Batch 807 Loss: 4.168464660644531\n",
            "mini Batch 808 Loss: 0.0\n",
            "mini Batch 809 Loss: 4.750199317932129\n",
            "mini Batch 810 Loss: 2.051330327987671\n",
            "mini Batch 811 Loss: 2.9257893562316895\n",
            "mini Batch 812 Loss: 0.675541877746582\n",
            "mini Batch 813 Loss: 1.347810983657837\n",
            "mini Batch 814 Loss: 0.05036379396915436\n",
            "mini Batch 815 Loss: 1.3317675590515137\n",
            "mini Batch 816 Loss: 1.500544786453247\n",
            "mini Batch 817 Loss: 2.729010820388794\n",
            "mini Batch 818 Loss: 8.135482788085938\n",
            "mini Batch 819 Loss: 2.5787293910980225\n",
            "mini Batch 820 Loss: 4.1568989753723145\n",
            "mini Batch 821 Loss: 4.923886299133301\n",
            "mini Batch 822 Loss: 4.039055347442627\n",
            "mini Batch 823 Loss: 3.6583566665649414\n",
            "mini Batch 824 Loss: 3.268432855606079\n",
            "mini Batch 825 Loss: 2.734766960144043\n",
            "mini Batch 826 Loss: 2.4111568927764893\n",
            "mini Batch 827 Loss: 2.1013741493225098\n",
            "mini Batch 828 Loss: 0.6225661039352417\n",
            "mini Batch 829 Loss: 1.3763827085494995\n",
            "mini Batch 830 Loss: 2.9035167694091797\n",
            "mini Batch 831 Loss: 3.0971322059631348\n",
            "mini Batch 832 Loss: 0.7172898650169373\n",
            "mini Batch 833 Loss: 1.6411185264587402\n",
            "mini Batch 834 Loss: 2.867614984512329\n",
            "mini Batch 835 Loss: 3.614959239959717\n",
            "mini Batch 836 Loss: 1.746551513671875\n",
            "mini Batch 837 Loss: 1.052657961845398\n",
            "mini Batch 838 Loss: 1.022085189819336\n",
            "mini Batch 839 Loss: 0.3640117049217224\n",
            "mini Batch 840 Loss: 1.5526909828186035\n",
            "mini Batch 841 Loss: 2.4929046630859375\n",
            "mini Batch 842 Loss: 0.4908576011657715\n",
            "mini Batch 843 Loss: 1.2246675491333008\n",
            "mini Batch 844 Loss: 0.9302064180374146\n",
            "mini Batch 845 Loss: 0.6152123212814331\n",
            "mini Batch 846 Loss: 1.051931381225586\n",
            "mini Batch 847 Loss: 1.6040443181991577\n",
            "mini Batch 848 Loss: 2.5866713523864746\n",
            "mini Batch 849 Loss: 2.0821571350097656\n",
            "mini Batch 850 Loss: 0.4593592584133148\n",
            "mini Batch 851 Loss: 0.5022940039634705\n",
            "mini Batch 852 Loss: 0.17223574221134186\n",
            "mini Batch 853 Loss: 4.141842842102051\n",
            "mini Batch 854 Loss: 2.870206356048584\n",
            "mini Batch 855 Loss: 2.8179798126220703\n",
            "mini Batch 856 Loss: 1.0177218914031982\n",
            "mini Batch 857 Loss: 2.972900867462158\n",
            "mini Batch 858 Loss: 1.2059073448181152\n",
            "mini Batch 859 Loss: 0.5325477123260498\n",
            "mini Batch 860 Loss: 0.2957962155342102\n",
            "mini Batch 861 Loss: 0.4580148160457611\n",
            "mini Batch 862 Loss: 0.15219081938266754\n",
            "mini Batch 863 Loss: 0.6046290397644043\n",
            "mini Batch 864 Loss: 2.0344629287719727\n",
            "mini Batch 865 Loss: 1.5428869724273682\n",
            "mini Batch 866 Loss: 2.287496566772461\n",
            "mini Batch 867 Loss: 2.666318893432617\n",
            "mini Batch 868 Loss: 0.16581012308597565\n",
            "mini Batch 869 Loss: 0.5456132888793945\n",
            "mini Batch 870 Loss: 0.680422306060791\n",
            "mini Batch 871 Loss: 1.0006837844848633\n",
            "mini Batch 872 Loss: 0.43252497911453247\n",
            "mini Batch 873 Loss: 1.1797130107879639\n",
            "mini Batch 874 Loss: 2.1843738555908203\n",
            "mini Batch 875 Loss: 1.8838863372802734\n",
            "mini Batch 876 Loss: 1.179667353630066\n",
            "mini Batch 877 Loss: 4.637996673583984\n",
            "mini Batch 878 Loss: 3.164468288421631\n",
            "mini Batch 879 Loss: 1.6272237300872803\n",
            "mini Batch 880 Loss: 1.0494015216827393\n",
            "mini Batch 881 Loss: 1.8525363206863403\n",
            "mini Batch 882 Loss: 1.7004218101501465\n",
            "mini Batch 883 Loss: 1.2916841506958008\n",
            "mini Batch 884 Loss: 2.463674545288086\n",
            "mini Batch 885 Loss: 0.4485514760017395\n",
            "mini Batch 886 Loss: 0.7951843738555908\n",
            "mini Batch 887 Loss: 0.3064325451850891\n",
            "mini Batch 888 Loss: 0.837253212928772\n",
            "mini Batch 889 Loss: 1.1852195262908936\n",
            "mini Batch 890 Loss: 3.2124476432800293\n",
            "mini Batch 891 Loss: 2.8033597469329834\n",
            "mini Batch 892 Loss: 0.7535848021507263\n",
            "mini Batch 893 Loss: 0.6349688768386841\n",
            "mini Batch 894 Loss: 0.9652913808822632\n",
            "mini Batch 895 Loss: 1.506826400756836\n",
            "mini Batch 896 Loss: 1.5021345615386963\n",
            "mini Batch 897 Loss: 0.5484753847122192\n",
            "mini Batch 898 Loss: 3.807555675506592\n",
            "mini Batch 899 Loss: 0.38810545206069946\n",
            "mini Batch 900 Loss: 0.13797786831855774\n",
            "mini Batch 901 Loss: 3.9331812858581543\n",
            "mini Batch 902 Loss: 0.7404656410217285\n",
            "mini Batch 903 Loss: 0.15418966114521027\n",
            "mini Batch 904 Loss: 3.497401237487793\n",
            "mini Batch 905 Loss: 0.4308509826660156\n",
            "mini Batch 906 Loss: 0.6050816774368286\n",
            "mini Batch 907 Loss: 2.7616286277770996\n",
            "mini Batch 908 Loss: 0.6310677528381348\n",
            "mini Batch 909 Loss: 0.9644947648048401\n",
            "mini Batch 910 Loss: 5.669586181640625\n",
            "mini Batch 911 Loss: 1.4277753829956055\n",
            "mini Batch 912 Loss: 0.7968599200248718\n",
            "mini Batch 913 Loss: 2.048626661300659\n",
            "mini Batch 914 Loss: 0.009536594152450562\n",
            "mini Batch 915 Loss: 0.4041753113269806\n",
            "mini Batch 916 Loss: 0.13793522119522095\n",
            "mini Batch 917 Loss: 10.840457916259766\n",
            "mini Batch 918 Loss: 2.679293632507324\n",
            "mini Batch 919 Loss: 0.8978274464607239\n",
            "mini Batch 920 Loss: 0.8020312190055847\n",
            "mini Batch 921 Loss: 0.31761011481285095\n",
            "[3] average loss per epoch: 1.981\n",
            "Saved model checkpoint to /content/drive/My Drive/test6/model_epoch3.pt\n",
            "dist a: tensor([11.7976, 15.4708, 14.6521, 11.8115, 13.8028, 12.8244, 14.0843, 14.7674,\n",
            "        10.3792, 16.3228, 11.0111, 11.2211, 18.2962, 10.9861, 13.1055, 14.3740,\n",
            "        15.5057,  8.2765, 13.2388, 20.3233, 14.4854, 10.0434,  9.8564, 13.4691,\n",
            "        18.7637, 21.9447,  9.4752, 15.4240, 15.8716,  9.3307, 13.3088, 11.5191,\n",
            "        13.3733, 12.9224, 15.2847, 12.5921, 12.2589,  9.9798, 14.6583, 13.7741,\n",
            "        12.3304, 11.4514, 15.1583, 13.7225, 26.1501, 10.7899, 19.9098, 22.1641,\n",
            "        17.0697,  7.9808, 25.7966, 18.2811, 13.1910, 10.9416, 20.9724, 12.2220,\n",
            "        13.3838, 19.2265, 19.4091, 15.9479, 16.4847, 20.5068, 17.6507, 18.6154],\n",
            "       device='cuda:0'), dist b: tensor([25.8087, 22.5502, 27.9674, 10.7905, 14.4893, 17.5049, 21.8423, 24.1149,\n",
            "        19.4463, 23.2087, 12.3942, 16.4618, 19.8739, 19.6273, 23.1223, 15.3104,\n",
            "        13.6103, 23.9047, 14.1824, 29.8157, 24.2490, 23.2062, 22.4695, 19.9811,\n",
            "        16.6382, 21.7587, 10.2440, 24.7161, 14.1749, 17.7477, 18.9707,  8.6539,\n",
            "        25.0726, 21.6719, 19.8766, 16.5477, 18.8220, 10.9382, 15.5657, 20.0934,\n",
            "        11.3494, 15.3251, 13.1146, 18.8352, 19.2501, 23.2077, 23.0836, 20.1707,\n",
            "        19.0908, 22.2169, 27.6740, 12.3625, 18.1993, 14.1720, 11.3446, 18.7875,\n",
            "        22.2228, 18.6667, 14.9759, 20.9668, 15.7924, 18.2874, 21.6339, 18.9503],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "================== START PREDICTION ==================\n",
            "batch id predict after epoch:  1\n",
            "batch id predict after epoch:  2\n",
            "batch id predict after epoch:  3\n",
            "batch id predict after epoch:  4\n",
            "batch id predict after epoch:  5\n",
            "batch id predict after epoch:  6\n",
            "batch id predict after epoch:  7\n",
            "batch id predict after epoch:  8\n",
            "batch id predict after epoch:  9\n",
            "batch id predict after epoch:  10\n",
            "batch id predict after epoch:  11\n",
            "batch id predict after epoch:  12\n",
            "batch id predict after epoch:  13\n",
            "batch id predict after epoch:  14\n",
            "batch id predict after epoch:  15\n",
            "batch id predict after epoch:  16\n",
            "batch id predict after epoch:  17\n",
            "batch id predict after epoch:  18\n",
            "batch id predict after epoch:  19\n",
            "batch id predict after epoch:  20\n",
            "batch id predict after epoch:  21\n",
            "batch id predict after epoch:  22\n",
            "batch id predict after epoch:  23\n",
            "batch id predict after epoch:  24\n",
            "batch id predict after epoch:  25\n",
            "batch id predict after epoch:  26\n",
            "batch id predict after epoch:  27\n",
            "batch id predict after epoch:  28\n",
            "batch id predict after epoch:  29\n",
            "batch id predict after epoch:  30\n",
            "batch id predict after epoch:  31\n",
            "batch id predict after epoch:  32\n",
            "batch id predict after epoch:  33\n",
            "batch id predict after epoch:  34\n",
            "batch id predict after epoch:  35\n",
            "batch id predict after epoch:  36\n",
            "batch id predict after epoch:  37\n",
            "batch id predict after epoch:  38\n",
            "batch id predict after epoch:  39\n",
            "batch id predict after epoch:  40\n",
            "batch id predict after epoch:  41\n",
            "batch id predict after epoch:  42\n",
            "batch id predict after epoch:  43\n",
            "batch id predict after epoch:  44\n",
            "batch id predict after epoch:  45\n",
            "batch id predict after epoch:  46\n",
            "batch id predict after epoch:  47\n",
            "batch id predict after epoch:  48\n",
            "batch id predict after epoch:  49\n",
            "batch id predict after epoch:  50\n",
            "batch id predict after epoch:  51\n",
            "batch id predict after epoch:  52\n",
            "batch id predict after epoch:  53\n",
            "batch id predict after epoch:  54\n",
            "batch id predict after epoch:  55\n",
            "batch id predict after epoch:  56\n",
            "batch id predict after epoch:  57\n",
            "batch id predict after epoch:  58\n",
            "batch id predict after epoch:  59\n",
            "batch id predict after epoch:  60\n",
            "batch id predict after epoch:  61\n",
            "batch id predict after epoch:  62\n",
            "batch id predict after epoch:  63\n",
            "batch id predict after epoch:  64\n",
            "batch id predict after epoch:  65\n",
            "batch id predict after epoch:  66\n",
            "batch id predict after epoch:  67\n",
            "batch id predict after epoch:  68\n",
            "batch id predict after epoch:  69\n",
            "batch id predict after epoch:  70\n",
            "batch id predict after epoch:  71\n",
            "batch id predict after epoch:  72\n",
            "batch id predict after epoch:  73\n",
            "batch id predict after epoch:  74\n",
            "batch id predict after epoch:  75\n",
            "batch id predict after epoch:  76\n",
            "batch id predict after epoch:  77\n",
            "batch id predict after epoch:  78\n",
            "batch id predict after epoch:  79\n",
            "batch id predict after epoch:  80\n",
            "batch id predict after epoch:  81\n",
            "batch id predict after epoch:  82\n",
            "batch id predict after epoch:  83\n",
            "batch id predict after epoch:  84\n",
            "batch id predict after epoch:  85\n",
            "batch id predict after epoch:  86\n",
            "batch id predict after epoch:  87\n",
            "batch id predict after epoch:  88\n",
            "batch id predict after epoch:  89\n",
            "batch id predict after epoch:  90\n",
            "batch id predict after epoch:  91\n",
            "batch id predict after epoch:  92\n",
            "batch id predict after epoch:  93\n",
            "batch id predict after epoch:  94\n",
            "batch id predict after epoch:  95\n",
            "batch id predict after epoch:  96\n",
            "batch id predict after epoch:  97\n",
            "batch id predict after epoch:  98\n",
            "batch id predict after epoch:  99\n",
            "batch id predict after epoch:  100\n",
            "batch id predict after epoch:  101\n",
            "batch id predict after epoch:  102\n",
            "batch id predict after epoch:  103\n",
            "batch id predict after epoch:  104\n",
            "batch id predict after epoch:  105\n",
            "batch id predict after epoch:  106\n",
            "batch id predict after epoch:  107\n",
            "batch id predict after epoch:  108\n",
            "batch id predict after epoch:  109\n",
            "batch id predict after epoch:  110\n",
            "batch id predict after epoch:  111\n",
            "batch id predict after epoch:  112\n",
            "batch id predict after epoch:  113\n",
            "batch id predict after epoch:  114\n",
            "batch id predict after epoch:  115\n",
            "batch id predict after epoch:  116\n",
            "batch id predict after epoch:  117\n",
            "batch id predict after epoch:  118\n",
            "batch id predict after epoch:  119\n",
            "batch id predict after epoch:  120\n",
            "batch id predict after epoch:  121\n",
            "batch id predict after epoch:  122\n",
            "batch id predict after epoch:  123\n",
            "batch id predict after epoch:  124\n",
            "batch id predict after epoch:  125\n",
            "batch id predict after epoch:  126\n",
            "batch id predict after epoch:  127\n",
            "batch id predict after epoch:  128\n",
            "batch id predict after epoch:  129\n",
            "batch id predict after epoch:  130\n",
            "batch id predict after epoch:  131\n",
            "batch id predict after epoch:  132\n",
            "batch id predict after epoch:  133\n",
            "batch id predict after epoch:  134\n",
            "batch id predict after epoch:  135\n",
            "batch id predict after epoch:  136\n",
            "batch id predict after epoch:  137\n",
            "batch id predict after epoch:  138\n",
            "batch id predict after epoch:  139\n",
            "batch id predict after epoch:  140\n",
            "batch id predict after epoch:  141\n",
            "batch id predict after epoch:  142\n",
            "batch id predict after epoch:  143\n",
            "batch id predict after epoch:  144\n",
            "batch id predict after epoch:  145\n",
            "batch id predict after epoch:  146\n",
            "batch id predict after epoch:  147\n",
            "batch id predict after epoch:  148\n",
            "batch id predict after epoch:  149\n",
            "batch id predict after epoch:  150\n",
            "batch id predict after epoch:  151\n",
            "batch id predict after epoch:  152\n",
            "batch id predict after epoch:  153\n",
            "batch id predict after epoch:  154\n",
            "batch id predict after epoch:  155\n",
            "batch id predict after epoch:  156\n",
            "batch id predict after epoch:  157\n",
            "batch id predict after epoch:  158\n",
            "batch id predict after epoch:  159\n",
            "batch id predict after epoch:  160\n",
            "batch id predict after epoch:  161\n",
            "batch id predict after epoch:  162\n",
            "batch id predict after epoch:  163\n",
            "batch id predict after epoch:  164\n",
            "batch id predict after epoch:  165\n",
            "batch id predict after epoch:  166\n",
            "batch id predict after epoch:  167\n",
            "batch id predict after epoch:  168\n",
            "batch id predict after epoch:  169\n",
            "batch id predict after epoch:  170\n",
            "batch id predict after epoch:  171\n",
            "batch id predict after epoch:  172\n",
            "batch id predict after epoch:  173\n",
            "batch id predict after epoch:  174\n",
            "batch id predict after epoch:  175\n",
            "batch id predict after epoch:  176\n",
            "batch id predict after epoch:  177\n",
            "batch id predict after epoch:  178\n",
            "batch id predict after epoch:  179\n",
            "batch id predict after epoch:  180\n",
            "batch id predict after epoch:  181\n",
            "batch id predict after epoch:  182\n",
            "batch id predict after epoch:  183\n",
            "batch id predict after epoch:  184\n",
            "batch id predict after epoch:  185\n",
            "batch id predict after epoch:  186\n",
            "batch id predict after epoch:  187\n",
            "batch id predict after epoch:  188\n",
            "batch id predict after epoch:  189\n",
            "batch id predict after epoch:  190\n",
            "batch id predict after epoch:  191\n",
            "batch id predict after epoch:  192\n",
            "batch id predict after epoch:  193\n",
            "batch id predict after epoch:  194\n",
            "batch id predict after epoch:  195\n",
            "batch id predict after epoch:  196\n",
            "batch id predict after epoch:  197\n",
            "batch id predict after epoch:  198\n",
            "batch id predict after epoch:  199\n",
            "batch id predict after epoch:  200\n",
            "batch id predict after epoch:  201\n",
            "batch id predict after epoch:  202\n",
            "batch id predict after epoch:  203\n",
            "batch id predict after epoch:  204\n",
            "batch id predict after epoch:  205\n",
            "batch id predict after epoch:  206\n",
            "batch id predict after epoch:  207\n",
            "batch id predict after epoch:  208\n",
            "batch id predict after epoch:  209\n",
            "batch id predict after epoch:  210\n",
            "batch id predict after epoch:  211\n",
            "batch id predict after epoch:  212\n",
            "batch id predict after epoch:  213\n",
            "batch id predict after epoch:  214\n",
            "batch id predict after epoch:  215\n",
            "batch id predict after epoch:  216\n",
            "batch id predict after epoch:  217\n",
            "batch id predict after epoch:  218\n",
            "batch id predict after epoch:  219\n",
            "batch id predict after epoch:  220\n",
            "batch id predict after epoch:  221\n",
            "batch id predict after epoch:  222\n",
            "batch id predict after epoch:  223\n",
            "batch id predict after epoch:  224\n",
            "batch id predict after epoch:  225\n",
            "batch id predict after epoch:  226\n",
            "batch id predict after epoch:  227\n",
            "batch id predict after epoch:  228\n",
            "batch id predict after epoch:  229\n",
            "batch id predict after epoch:  230\n",
            "batch id predict after epoch:  231\n",
            "batch id predict after epoch:  232\n",
            "batch id predict after epoch:  233\n",
            "batch id predict after epoch:  234\n",
            "batch id predict after epoch:  235\n",
            "batch id predict after epoch:  236\n",
            "batch id predict after epoch:  237\n",
            "batch id predict after epoch:  238\n",
            "batch id predict after epoch:  239\n",
            "batch id predict after epoch:  240\n",
            "batch id predict after epoch:  241\n",
            "batch id predict after epoch:  242\n",
            "batch id predict after epoch:  243\n",
            "batch id predict after epoch:  244\n",
            "batch id predict after epoch:  245\n",
            "batch id predict after epoch:  246\n",
            "batch id predict after epoch:  247\n",
            "batch id predict after epoch:  248\n",
            "batch id predict after epoch:  249\n",
            "batch id predict after epoch:  250\n",
            "batch id predict after epoch:  251\n",
            "batch id predict after epoch:  252\n",
            "batch id predict after epoch:  253\n",
            "batch id predict after epoch:  254\n",
            "batch id predict after epoch:  255\n",
            "batch id predict after epoch:  256\n",
            "batch id predict after epoch:  257\n",
            "batch id predict after epoch:  258\n",
            "batch id predict after epoch:  259\n",
            "batch id predict after epoch:  260\n",
            "batch id predict after epoch:  261\n",
            "batch id predict after epoch:  262\n",
            "batch id predict after epoch:  263\n",
            "batch id predict after epoch:  264\n",
            "batch id predict after epoch:  265\n",
            "batch id predict after epoch:  266\n",
            "batch id predict after epoch:  267\n",
            "batch id predict after epoch:  268\n",
            "batch id predict after epoch:  269\n",
            "batch id predict after epoch:  270\n",
            "batch id predict after epoch:  271\n",
            "batch id predict after epoch:  272\n",
            "batch id predict after epoch:  273\n",
            "batch id predict after epoch:  274\n",
            "batch id predict after epoch:  275\n",
            "batch id predict after epoch:  276\n",
            "batch id predict after epoch:  277\n",
            "batch id predict after epoch:  278\n",
            "batch id predict after epoch:  279\n",
            "batch id predict after epoch:  280\n",
            "batch id predict after epoch:  281\n",
            "batch id predict after epoch:  282\n",
            "batch id predict after epoch:  283\n",
            "batch id predict after epoch:  284\n",
            "batch id predict after epoch:  285\n",
            "batch id predict after epoch:  286\n",
            "batch id predict after epoch:  287\n",
            "batch id predict after epoch:  288\n",
            "batch id predict after epoch:  289\n",
            "batch id predict after epoch:  290\n",
            "batch id predict after epoch:  291\n",
            "batch id predict after epoch:  292\n",
            "batch id predict after epoch:  293\n",
            "batch id predict after epoch:  294\n",
            "batch id predict after epoch:  295\n",
            "batch id predict after epoch:  296\n",
            "batch id predict after epoch:  297\n",
            "batch id predict after epoch:  298\n",
            "batch id predict after epoch:  299\n",
            "batch id predict after epoch:  300\n",
            "batch id predict after epoch:  301\n",
            "batch id predict after epoch:  302\n",
            "batch id predict after epoch:  303\n",
            "batch id predict after epoch:  304\n",
            "batch id predict after epoch:  305\n",
            "batch id predict after epoch:  306\n",
            "batch id predict after epoch:  307\n",
            "batch id predict after epoch:  308\n",
            "batch id predict after epoch:  309\n",
            "batch id predict after epoch:  310\n",
            "batch id predict after epoch:  311\n",
            "batch id predict after epoch:  312\n",
            "batch id predict after epoch:  313\n",
            "batch id predict after epoch:  314\n",
            "batch id predict after epoch:  315\n",
            "batch id predict after epoch:  316\n",
            "batch id predict after epoch:  317\n",
            "batch id predict after epoch:  318\n",
            "batch id predict after epoch:  319\n",
            "batch id predict after epoch:  320\n",
            "batch id predict after epoch:  321\n",
            "batch id predict after epoch:  322\n",
            "batch id predict after epoch:  323\n",
            "batch id predict after epoch:  324\n",
            "batch id predict after epoch:  325\n",
            "batch id predict after epoch:  326\n",
            "batch id predict after epoch:  327\n",
            "batch id predict after epoch:  328\n",
            "batch id predict after epoch:  329\n",
            "batch id predict after epoch:  330\n",
            "batch id predict after epoch:  331\n",
            "batch id predict after epoch:  332\n",
            "batch id predict after epoch:  333\n",
            "batch id predict after epoch:  334\n",
            "batch id predict after epoch:  335\n",
            "batch id predict after epoch:  336\n",
            "batch id predict after epoch:  337\n",
            "batch id predict after epoch:  338\n",
            "batch id predict after epoch:  339\n",
            "batch id predict after epoch:  340\n",
            "batch id predict after epoch:  341\n",
            "batch id predict after epoch:  342\n",
            "batch id predict after epoch:  343\n",
            "batch id predict after epoch:  344\n",
            "batch id predict after epoch:  345\n",
            "batch id predict after epoch:  346\n",
            "batch id predict after epoch:  347\n",
            "batch id predict after epoch:  348\n",
            "batch id predict after epoch:  349\n",
            "batch id predict after epoch:  350\n",
            "batch id predict after epoch:  351\n",
            "batch id predict after epoch:  352\n",
            "batch id predict after epoch:  353\n",
            "batch id predict after epoch:  354\n",
            "batch id predict after epoch:  355\n",
            "batch id predict after epoch:  356\n",
            "batch id predict after epoch:  357\n",
            "batch id predict after epoch:  358\n",
            "batch id predict after epoch:  359\n",
            "batch id predict after epoch:  360\n",
            "batch id predict after epoch:  361\n",
            "batch id predict after epoch:  362\n",
            "batch id predict after epoch:  363\n",
            "batch id predict after epoch:  364\n",
            "batch id predict after epoch:  365\n",
            "batch id predict after epoch:  366\n",
            "batch id predict after epoch:  367\n",
            "batch id predict after epoch:  368\n",
            "batch id predict after epoch:  369\n",
            "batch id predict after epoch:  370\n",
            "batch id predict after epoch:  371\n",
            "batch id predict after epoch:  372\n",
            "batch id predict after epoch:  373\n",
            "batch id predict after epoch:  374\n",
            "batch id predict after epoch:  375\n",
            "batch id predict after epoch:  376\n",
            "batch id predict after epoch:  377\n",
            "batch id predict after epoch:  378\n",
            "batch id predict after epoch:  379\n",
            "batch id predict after epoch:  380\n",
            "batch id predict after epoch:  381\n",
            "batch id predict after epoch:  382\n",
            "batch id predict after epoch:  383\n",
            "batch id predict after epoch:  384\n",
            "batch id predict after epoch:  385\n",
            "batch id predict after epoch:  386\n",
            "batch id predict after epoch:  387\n",
            "batch id predict after epoch:  388\n",
            "batch id predict after epoch:  389\n",
            "batch id predict after epoch:  390\n",
            "batch id predict after epoch:  391\n",
            "batch id predict after epoch:  392\n",
            "batch id predict after epoch:  393\n",
            "batch id predict after epoch:  394\n",
            "batch id predict after epoch:  395\n",
            "batch id predict after epoch:  396\n",
            "batch id predict after epoch:  397\n",
            "batch id predict after epoch:  398\n",
            "batch id predict after epoch:  399\n",
            "batch id predict after epoch:  400\n",
            "batch id predict after epoch:  401\n",
            "batch id predict after epoch:  402\n",
            "batch id predict after epoch:  403\n",
            "batch id predict after epoch:  404\n",
            "batch id predict after epoch:  405\n",
            "batch id predict after epoch:  406\n",
            "batch id predict after epoch:  407\n",
            "batch id predict after epoch:  408\n",
            "batch id predict after epoch:  409\n",
            "batch id predict after epoch:  410\n",
            "batch id predict after epoch:  411\n",
            "batch id predict after epoch:  412\n",
            "batch id predict after epoch:  413\n",
            "batch id predict after epoch:  414\n",
            "batch id predict after epoch:  415\n",
            "batch id predict after epoch:  416\n",
            "batch id predict after epoch:  417\n",
            "batch id predict after epoch:  418\n",
            "batch id predict after epoch:  419\n",
            "batch id predict after epoch:  420\n",
            "batch id predict after epoch:  421\n",
            "batch id predict after epoch:  422\n",
            "batch id predict after epoch:  423\n",
            "batch id predict after epoch:  424\n",
            "batch id predict after epoch:  425\n",
            "batch id predict after epoch:  426\n",
            "batch id predict after epoch:  427\n",
            "batch id predict after epoch:  428\n",
            "batch id predict after epoch:  429\n",
            "batch id predict after epoch:  430\n",
            "batch id predict after epoch:  431\n",
            "batch id predict after epoch:  432\n",
            "batch id predict after epoch:  433\n",
            "batch id predict after epoch:  434\n",
            "batch id predict after epoch:  435\n",
            "batch id predict after epoch:  436\n",
            "batch id predict after epoch:  437\n",
            "batch id predict after epoch:  438\n",
            "batch id predict after epoch:  439\n",
            "batch id predict after epoch:  440\n",
            "batch id predict after epoch:  441\n",
            "batch id predict after epoch:  442\n",
            "batch id predict after epoch:  443\n",
            "batch id predict after epoch:  444\n",
            "batch id predict after epoch:  445\n",
            "batch id predict after epoch:  446\n",
            "batch id predict after epoch:  447\n",
            "batch id predict after epoch:  448\n",
            "batch id predict after epoch:  449\n",
            "batch id predict after epoch:  450\n",
            "batch id predict after epoch:  451\n",
            "batch id predict after epoch:  452\n",
            "batch id predict after epoch:  453\n",
            "batch id predict after epoch:  454\n",
            "batch id predict after epoch:  455\n",
            "batch id predict after epoch:  456\n",
            "batch id predict after epoch:  457\n",
            "batch id predict after epoch:  458\n",
            "batch id predict after epoch:  459\n",
            "batch id predict after epoch:  460\n",
            "batch id predict after epoch:  461\n",
            "batch id predict after epoch:  462\n",
            "batch id predict after epoch:  463\n",
            "batch id predict after epoch:  464\n",
            "batch id predict after epoch:  465\n",
            "batch id predict after epoch:  466\n",
            "batch id predict after epoch:  467\n",
            "batch id predict after epoch:  468\n",
            "batch id predict after epoch:  469\n",
            "batch id predict after epoch:  470\n",
            "batch id predict after epoch:  471\n",
            "batch id predict after epoch:  472\n",
            "batch id predict after epoch:  473\n",
            "batch id predict after epoch:  474\n",
            "batch id predict after epoch:  475\n",
            "batch id predict after epoch:  476\n",
            "batch id predict after epoch:  477\n",
            "batch id predict after epoch:  478\n",
            "batch id predict after epoch:  479\n",
            "batch id predict after epoch:  480\n",
            "batch id predict after epoch:  481\n",
            "batch id predict after epoch:  482\n",
            "batch id predict after epoch:  483\n",
            "batch id predict after epoch:  484\n",
            "batch id predict after epoch:  485\n",
            "batch id predict after epoch:  486\n",
            "batch id predict after epoch:  487\n",
            "batch id predict after epoch:  488\n",
            "batch id predict after epoch:  489\n",
            "batch id predict after epoch:  490\n",
            "batch id predict after epoch:  491\n",
            "batch id predict after epoch:  492\n",
            "batch id predict after epoch:  493\n",
            "batch id predict after epoch:  494\n",
            "batch id predict after epoch:  495\n",
            "batch id predict after epoch:  496\n",
            "batch id predict after epoch:  497\n",
            "batch id predict after epoch:  498\n",
            "batch id predict after epoch:  499\n",
            "batch id predict after epoch:  500\n",
            "batch id predict after epoch:  501\n",
            "batch id predict after epoch:  502\n",
            "batch id predict after epoch:  503\n",
            "batch id predict after epoch:  504\n",
            "batch id predict after epoch:  505\n",
            "batch id predict after epoch:  506\n",
            "batch id predict after epoch:  507\n",
            "batch id predict after epoch:  508\n",
            "batch id predict after epoch:  509\n",
            "batch id predict after epoch:  510\n",
            "batch id predict after epoch:  511\n",
            "batch id predict after epoch:  512\n",
            "batch id predict after epoch:  513\n",
            "batch id predict after epoch:  514\n",
            "batch id predict after epoch:  515\n",
            "batch id predict after epoch:  516\n",
            "batch id predict after epoch:  517\n",
            "batch id predict after epoch:  518\n",
            "batch id predict after epoch:  519\n",
            "batch id predict after epoch:  520\n",
            "batch id predict after epoch:  521\n",
            "batch id predict after epoch:  522\n",
            "batch id predict after epoch:  523\n",
            "batch id predict after epoch:  524\n",
            "batch id predict after epoch:  525\n",
            "batch id predict after epoch:  526\n",
            "batch id predict after epoch:  527\n",
            "batch id predict after epoch:  528\n",
            "batch id predict after epoch:  529\n",
            "batch id predict after epoch:  530\n",
            "batch id predict after epoch:  531\n",
            "batch id predict after epoch:  532\n",
            "batch id predict after epoch:  533\n",
            "batch id predict after epoch:  534\n",
            "batch id predict after epoch:  535\n",
            "batch id predict after epoch:  536\n",
            "batch id predict after epoch:  537\n",
            "batch id predict after epoch:  538\n",
            "batch id predict after epoch:  539\n",
            "batch id predict after epoch:  540\n",
            "batch id predict after epoch:  541\n",
            "batch id predict after epoch:  542\n",
            "batch id predict after epoch:  543\n",
            "batch id predict after epoch:  544\n",
            "batch id predict after epoch:  545\n",
            "batch id predict after epoch:  546\n",
            "batch id predict after epoch:  547\n",
            "batch id predict after epoch:  548\n",
            "batch id predict after epoch:  549\n",
            "batch id predict after epoch:  550\n",
            "batch id predict after epoch:  551\n",
            "batch id predict after epoch:  552\n",
            "batch id predict after epoch:  553\n",
            "batch id predict after epoch:  554\n",
            "batch id predict after epoch:  555\n",
            "batch id predict after epoch:  556\n",
            "batch id predict after epoch:  557\n",
            "batch id predict after epoch:  558\n",
            "batch id predict after epoch:  559\n",
            "batch id predict after epoch:  560\n",
            "batch id predict after epoch:  561\n",
            "batch id predict after epoch:  562\n",
            "batch id predict after epoch:  563\n",
            "batch id predict after epoch:  564\n",
            "batch id predict after epoch:  565\n",
            "batch id predict after epoch:  566\n",
            "batch id predict after epoch:  567\n",
            "batch id predict after epoch:  568\n",
            "batch id predict after epoch:  569\n",
            "batch id predict after epoch:  570\n",
            "batch id predict after epoch:  571\n",
            "batch id predict after epoch:  572\n",
            "batch id predict after epoch:  573\n",
            "batch id predict after epoch:  574\n",
            "batch id predict after epoch:  575\n",
            "batch id predict after epoch:  576\n",
            "batch id predict after epoch:  577\n",
            "batch id predict after epoch:  578\n",
            "batch id predict after epoch:  579\n",
            "batch id predict after epoch:  580\n",
            "batch id predict after epoch:  581\n",
            "batch id predict after epoch:  582\n",
            "batch id predict after epoch:  583\n",
            "batch id predict after epoch:  584\n",
            "batch id predict after epoch:  585\n",
            "batch id predict after epoch:  586\n",
            "batch id predict after epoch:  587\n",
            "batch id predict after epoch:  588\n",
            "batch id predict after epoch:  589\n",
            "batch id predict after epoch:  590\n",
            "batch id predict after epoch:  591\n",
            "batch id predict after epoch:  592\n",
            "batch id predict after epoch:  593\n",
            "batch id predict after epoch:  594\n",
            "batch id predict after epoch:  595\n",
            "batch id predict after epoch:  596\n",
            "batch id predict after epoch:  597\n",
            "batch id predict after epoch:  598\n",
            "batch id predict after epoch:  599\n",
            "batch id predict after epoch:  600\n",
            "batch id predict after epoch:  601\n",
            "batch id predict after epoch:  602\n",
            "batch id predict after epoch:  603\n",
            "batch id predict after epoch:  604\n",
            "batch id predict after epoch:  605\n",
            "batch id predict after epoch:  606\n",
            "batch id predict after epoch:  607\n",
            "batch id predict after epoch:  608\n",
            "batch id predict after epoch:  609\n",
            "batch id predict after epoch:  610\n",
            "batch id predict after epoch:  611\n",
            "batch id predict after epoch:  612\n",
            "batch id predict after epoch:  613\n",
            "batch id predict after epoch:  614\n",
            "batch id predict after epoch:  615\n",
            "batch id predict after epoch:  616\n",
            "batch id predict after epoch:  617\n",
            "batch id predict after epoch:  618\n",
            "batch id predict after epoch:  619\n",
            "batch id predict after epoch:  620\n",
            "batch id predict after epoch:  621\n",
            "batch id predict after epoch:  622\n",
            "batch id predict after epoch:  623\n",
            "batch id predict after epoch:  624\n",
            "batch id predict after epoch:  625\n",
            "batch id predict after epoch:  626\n",
            "batch id predict after epoch:  627\n",
            "batch id predict after epoch:  628\n",
            "batch id predict after epoch:  629\n",
            "batch id predict after epoch:  630\n",
            "batch id predict after epoch:  631\n",
            "batch id predict after epoch:  632\n",
            "batch id predict after epoch:  633\n",
            "batch id predict after epoch:  634\n",
            "batch id predict after epoch:  635\n",
            "batch id predict after epoch:  636\n",
            "batch id predict after epoch:  637\n",
            "batch id predict after epoch:  638\n",
            "batch id predict after epoch:  639\n",
            "batch id predict after epoch:  640\n",
            "batch id predict after epoch:  641\n",
            "batch id predict after epoch:  642\n",
            "batch id predict after epoch:  643\n",
            "batch id predict after epoch:  644\n",
            "batch id predict after epoch:  645\n",
            "batch id predict after epoch:  646\n",
            "batch id predict after epoch:  647\n",
            "batch id predict after epoch:  648\n",
            "batch id predict after epoch:  649\n",
            "batch id predict after epoch:  650\n",
            "batch id predict after epoch:  651\n",
            "batch id predict after epoch:  652\n",
            "batch id predict after epoch:  653\n",
            "batch id predict after epoch:  654\n",
            "batch id predict after epoch:  655\n",
            "batch id predict after epoch:  656\n",
            "batch id predict after epoch:  657\n",
            "batch id predict after epoch:  658\n",
            "batch id predict after epoch:  659\n",
            "batch id predict after epoch:  660\n",
            "batch id predict after epoch:  661\n",
            "batch id predict after epoch:  662\n",
            "batch id predict after epoch:  663\n",
            "batch id predict after epoch:  664\n",
            "batch id predict after epoch:  665\n",
            "batch id predict after epoch:  666\n",
            "batch id predict after epoch:  667\n",
            "batch id predict after epoch:  668\n",
            "batch id predict after epoch:  669\n",
            "batch id predict after epoch:  670\n",
            "batch id predict after epoch:  671\n",
            "batch id predict after epoch:  672\n",
            "batch id predict after epoch:  673\n",
            "batch id predict after epoch:  674\n",
            "batch id predict after epoch:  675\n",
            "batch id predict after epoch:  676\n",
            "batch id predict after epoch:  677\n",
            "batch id predict after epoch:  678\n",
            "batch id predict after epoch:  679\n",
            "batch id predict after epoch:  680\n",
            "batch id predict after epoch:  681\n",
            "batch id predict after epoch:  682\n",
            "batch id predict after epoch:  683\n",
            "batch id predict after epoch:  684\n",
            "batch id predict after epoch:  685\n",
            "batch id predict after epoch:  686\n",
            "batch id predict after epoch:  687\n",
            "batch id predict after epoch:  688\n",
            "batch id predict after epoch:  689\n",
            "batch id predict after epoch:  690\n",
            "batch id predict after epoch:  691\n",
            "batch id predict after epoch:  692\n",
            "batch id predict after epoch:  693\n",
            "batch id predict after epoch:  694\n",
            "batch id predict after epoch:  695\n",
            "batch id predict after epoch:  696\n",
            "batch id predict after epoch:  697\n",
            "batch id predict after epoch:  698\n",
            "batch id predict after epoch:  699\n",
            "batch id predict after epoch:  700\n",
            "batch id predict after epoch:  701\n",
            "batch id predict after epoch:  702\n",
            "batch id predict after epoch:  703\n",
            "batch id predict after epoch:  704\n",
            "batch id predict after epoch:  705\n",
            "batch id predict after epoch:  706\n",
            "batch id predict after epoch:  707\n",
            "batch id predict after epoch:  708\n",
            "batch id predict after epoch:  709\n",
            "batch id predict after epoch:  710\n",
            "batch id predict after epoch:  711\n",
            "batch id predict after epoch:  712\n",
            "batch id predict after epoch:  713\n",
            "batch id predict after epoch:  714\n",
            "batch id predict after epoch:  715\n",
            "batch id predict after epoch:  716\n",
            "batch id predict after epoch:  717\n",
            "batch id predict after epoch:  718\n",
            "batch id predict after epoch:  719\n",
            "batch id predict after epoch:  720\n",
            "batch id predict after epoch:  721\n",
            "batch id predict after epoch:  722\n",
            "batch id predict after epoch:  723\n",
            "batch id predict after epoch:  724\n",
            "batch id predict after epoch:  725\n",
            "batch id predict after epoch:  726\n",
            "batch id predict after epoch:  727\n",
            "batch id predict after epoch:  728\n",
            "batch id predict after epoch:  729\n",
            "batch id predict after epoch:  730\n",
            "batch id predict after epoch:  731\n",
            "batch id predict after epoch:  732\n",
            "batch id predict after epoch:  733\n",
            "batch id predict after epoch:  734\n",
            "batch id predict after epoch:  735\n",
            "batch id predict after epoch:  736\n",
            "batch id predict after epoch:  737\n",
            "batch id predict after epoch:  738\n",
            "batch id predict after epoch:  739\n",
            "batch id predict after epoch:  740\n",
            "batch id predict after epoch:  741\n",
            "batch id predict after epoch:  742\n",
            "batch id predict after epoch:  743\n",
            "batch id predict after epoch:  744\n",
            "batch id predict after epoch:  745\n",
            "batch id predict after epoch:  746\n",
            "batch id predict after epoch:  747\n",
            "batch id predict after epoch:  748\n",
            "batch id predict after epoch:  749\n",
            "batch id predict after epoch:  750\n",
            "batch id predict after epoch:  751\n",
            "batch id predict after epoch:  752\n",
            "batch id predict after epoch:  753\n",
            "batch id predict after epoch:  754\n",
            "batch id predict after epoch:  755\n",
            "batch id predict after epoch:  756\n",
            "batch id predict after epoch:  757\n",
            "batch id predict after epoch:  758\n",
            "batch id predict after epoch:  759\n",
            "batch id predict after epoch:  760\n",
            "batch id predict after epoch:  761\n",
            "batch id predict after epoch:  762\n",
            "batch id predict after epoch:  763\n",
            "batch id predict after epoch:  764\n",
            "batch id predict after epoch:  765\n",
            "batch id predict after epoch:  766\n",
            "batch id predict after epoch:  767\n",
            "batch id predict after epoch:  768\n",
            "batch id predict after epoch:  769\n",
            "batch id predict after epoch:  770\n",
            "batch id predict after epoch:  771\n",
            "batch id predict after epoch:  772\n",
            "batch id predict after epoch:  773\n",
            "batch id predict after epoch:  774\n",
            "batch id predict after epoch:  775\n",
            "batch id predict after epoch:  776\n",
            "batch id predict after epoch:  777\n",
            "batch id predict after epoch:  778\n",
            "batch id predict after epoch:  779\n",
            "batch id predict after epoch:  780\n",
            "batch id predict after epoch:  781\n",
            "batch id predict after epoch:  782\n",
            "batch id predict after epoch:  783\n",
            "batch id predict after epoch:  784\n",
            "batch id predict after epoch:  785\n",
            "batch id predict after epoch:  786\n",
            "batch id predict after epoch:  787\n",
            "batch id predict after epoch:  788\n",
            "batch id predict after epoch:  789\n",
            "batch id predict after epoch:  790\n",
            "batch id predict after epoch:  791\n",
            "batch id predict after epoch:  792\n",
            "batch id predict after epoch:  793\n",
            "batch id predict after epoch:  794\n",
            "batch id predict after epoch:  795\n",
            "batch id predict after epoch:  796\n",
            "batch id predict after epoch:  797\n",
            "batch id predict after epoch:  798\n",
            "batch id predict after epoch:  799\n",
            "batch id predict after epoch:  800\n",
            "batch id predict after epoch:  801\n",
            "batch id predict after epoch:  802\n",
            "batch id predict after epoch:  803\n",
            "batch id predict after epoch:  804\n",
            "batch id predict after epoch:  805\n",
            "batch id predict after epoch:  806\n",
            "batch id predict after epoch:  807\n",
            "batch id predict after epoch:  808\n",
            "batch id predict after epoch:  809\n",
            "batch id predict after epoch:  810\n",
            "batch id predict after epoch:  811\n",
            "batch id predict after epoch:  812\n",
            "batch id predict after epoch:  813\n",
            "batch id predict after epoch:  814\n",
            "batch id predict after epoch:  815\n",
            "batch id predict after epoch:  816\n",
            "batch id predict after epoch:  817\n",
            "batch id predict after epoch:  818\n",
            "batch id predict after epoch:  819\n",
            "batch id predict after epoch:  820\n",
            "batch id predict after epoch:  821\n",
            "batch id predict after epoch:  822\n",
            "batch id predict after epoch:  823\n",
            "batch id predict after epoch:  824\n",
            "batch id predict after epoch:  825\n",
            "batch id predict after epoch:  826\n",
            "batch id predict after epoch:  827\n",
            "batch id predict after epoch:  828\n",
            "batch id predict after epoch:  829\n",
            "batch id predict after epoch:  830\n",
            "batch id predict after epoch:  831\n",
            "batch id predict after epoch:  832\n",
            "batch id predict after epoch:  833\n",
            "batch id predict after epoch:  834\n",
            "batch id predict after epoch:  835\n",
            "batch id predict after epoch:  836\n",
            "batch id predict after epoch:  837\n",
            "batch id predict after epoch:  838\n",
            "batch id predict after epoch:  839\n",
            "batch id predict after epoch:  840\n",
            "batch id predict after epoch:  841\n",
            "batch id predict after epoch:  842\n",
            "batch id predict after epoch:  843\n",
            "batch id predict after epoch:  844\n",
            "batch id predict after epoch:  845\n",
            "batch id predict after epoch:  846\n",
            "batch id predict after epoch:  847\n",
            "batch id predict after epoch:  848\n",
            "batch id predict after epoch:  849\n",
            "batch id predict after epoch:  850\n",
            "batch id predict after epoch:  851\n",
            "batch id predict after epoch:  852\n",
            "batch id predict after epoch:  853\n",
            "batch id predict after epoch:  854\n",
            "batch id predict after epoch:  855\n",
            "batch id predict after epoch:  856\n",
            "batch id predict after epoch:  857\n",
            "batch id predict after epoch:  858\n",
            "batch id predict after epoch:  859\n",
            "batch id predict after epoch:  860\n",
            "batch id predict after epoch:  861\n",
            "batch id predict after epoch:  862\n",
            "batch id predict after epoch:  863\n",
            "batch id predict after epoch:  864\n",
            "batch id predict after epoch:  865\n",
            "batch id predict after epoch:  866\n",
            "batch id predict after epoch:  867\n",
            "batch id predict after epoch:  868\n",
            "batch id predict after epoch:  869\n",
            "batch id predict after epoch:  870\n",
            "batch id predict after epoch:  871\n",
            "batch id predict after epoch:  872\n",
            "batch id predict after epoch:  873\n",
            "batch id predict after epoch:  874\n",
            "batch id predict after epoch:  875\n",
            "batch id predict after epoch:  876\n",
            "batch id predict after epoch:  877\n",
            "batch id predict after epoch:  878\n",
            "batch id predict after epoch:  879\n",
            "batch id predict after epoch:  880\n",
            "batch id predict after epoch:  881\n",
            "batch id predict after epoch:  882\n",
            "batch id predict after epoch:  883\n",
            "batch id predict after epoch:  884\n",
            "batch id predict after epoch:  885\n",
            "batch id predict after epoch:  886\n",
            "batch id predict after epoch:  887\n",
            "batch id predict after epoch:  888\n",
            "batch id predict after epoch:  889\n",
            "batch id predict after epoch:  890\n",
            "batch id predict after epoch:  891\n",
            "batch id predict after epoch:  892\n",
            "batch id predict after epoch:  893\n",
            "batch id predict after epoch:  894\n",
            "batch id predict after epoch:  895\n",
            "batch id predict after epoch:  896\n",
            "batch id predict after epoch:  897\n",
            "batch id predict after epoch:  898\n",
            "batch id predict after epoch:  899\n",
            "batch id predict after epoch:  900\n",
            "batch id predict after epoch:  901\n",
            "batch id predict after epoch:  902\n",
            "batch id predict after epoch:  903\n",
            "batch id predict after epoch:  904\n",
            "batch id predict after epoch:  905\n",
            "batch id predict after epoch:  906\n",
            "batch id predict after epoch:  907\n",
            "batch id predict after epoch:  908\n",
            "batch id predict after epoch:  909\n",
            "batch id predict after epoch:  910\n",
            "batch id predict after epoch:  911\n",
            "batch id predict after epoch:  912\n",
            "batch id predict after epoch:  913\n",
            "batch id predict after epoch:  914\n",
            "batch id predict after epoch:  915\n",
            "batch id predict after epoch:  916\n",
            "batch id predict after epoch:  917\n",
            "batch id predict after epoch:  918\n",
            "batch id predict after epoch:  919\n",
            "batch id predict after epoch:  920\n",
            "batch id predict after epoch:  921\n",
            "batch id predict after epoch:  922\n",
            "batch id predict after epoch:  923\n",
            "batch id predict after epoch:  924\n",
            "batch id predict after epoch:  925\n",
            "batch id predict after epoch:  926\n",
            "batch id predict after epoch:  927\n",
            "batch id predict after epoch:  928\n",
            "batch id predict after epoch:  929\n",
            "batch id predict after epoch:  930\n",
            "batch id predict after epoch:  931\n",
            "[1 0 0 ... 1 0 1]\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "trained_net = train(net, criterion, optimizer, 3, train_loader, val_loader, test_loader)"
      ],
      "id": "8b7f8cdf"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_net.eval()"
      ],
      "metadata": {
        "id": "1LEvesrMqQac"
      },
      "id": "1LEvesrMqQac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# load model from checkpoint\n",
        "model_loaded = TripletNet(FeatureExtractNET())\n",
        "checkpoint = torch.load('/content/drive/MyDrive/test3/model_epoch_5.pt')\n",
        "model_loaded.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "model_loaded.to(device)\n",
        "model_loaded.eval()"
      ],
      "metadata": {
        "id": "8uUh7Yk99ecP"
      },
      "id": "8uUh7Yk99ecP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFZPwoPglJFH"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  valloader_iterator = iter(train_loader)\n",
        "  mean_accuracy = val_accuracy(model_loaded, train_loader, valloader_iterator)\n",
        "  print(mean_accuracy)"
      ],
      "id": "dFZPwoPglJFH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB9aCp_jT2OH"
      },
      "outputs": [],
      "source": [
        "print('================== START PREDICTION ==================')\n",
        "\n",
        "# Change to evaluation mode\n",
        "model_loaded.eval()\n",
        "\n",
        "redicted_labels = np.zeros(59544)\n",
        "pred_test=[]\n",
        "\n",
        "#Predict labels 1 or 0 for each test triplet\n",
        "for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
        "\n",
        "    data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "    # wrap in torch.autograd.Variable\n",
        "    data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # compute output and loss\n",
        "        embedded_x, embedded_y, embedded_z = trained_net(data1, data2, data3)\n",
        "\n",
        "    dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "    dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "    #print(np.squeeze(embedded_a.cpu().detach().numpy()).shape)\n",
        "    \n",
        "\n",
        "    pred_test.append(1*(dist_a <= dist_b))\n",
        "\n",
        "\n",
        "    print('batch: ', batch_idx)"
      ],
      "id": "iB9aCp_jT2OH"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_np = []\n",
        "for i in range(len(pred_test)):\n",
        "  pred_test_cpu = pred_test[i].cpu().detach().numpy()\n",
        "  pred_test_np += list(pred_test_cpu)\n",
        "len(pred_test_np)\n",
        "predicted_labels = np.hstack(pred_test_np)\n",
        "print(predicted_labels)\n",
        "\n",
        "#Write submisison file\n",
        "df = pd.DataFrame(predicted_labels)\n",
        "df.to_csv('/content/drive/MyDrive/test3/submission_1epoch.txt', index=False, header=None) #write CSV"
      ],
      "metadata": {
        "id": "eqMINu0rwiv2"
      },
      "id": "eqMINu0rwiv2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = len(open(\"/content/drive/MyDrive/test3/submission_1epoch.txt\",'rU').readlines())\n",
        "print(count)"
      ],
      "metadata": {
        "id": "rc5c6182Do0U"
      },
      "id": "rc5c6182Do0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nq4tku-jQRuh"
      },
      "id": "nq4tku-jQRuh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Task 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}