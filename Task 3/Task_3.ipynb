{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVAaWUZmJuEf",
        "outputId": "4004f92e-ec2e-4543-8abe-9b6e06ac9254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BVAaWUZmJuEf"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9cafface"
      },
      "outputs": [],
      "source": [
        "# Solve the imshow dead kernel problem\n",
        "import os    \n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "id": "9cafface"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc461e5c",
        "outputId": "44836613-cd08-4349-cc20-d4ff27081b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START LOADING DATA ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start loading the data\n",
        "'''\n",
        "print('================== START LOADING DATA ==================')"
      ],
      "id": "bc461e5c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aa85839b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "aa85839b"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "e8218f58"
      },
      "outputs": [],
      "source": [
        "# Split the training set into a 80% training set and 20% validation set\n",
        "import random\n",
        "\n",
        "def split_huge_file(file,out1,out2,percentage=0.75,seed=2022):\n",
        "    \"\"\"Splits a file in 2 given the approximate `percentage` to go in the large file.\"\"\"\n",
        "    random.seed(seed)\n",
        "    with open(file, 'r',encoding=\"utf-8\") as fin, \\\n",
        "         open(out1, 'w') as foutBig, \\\n",
        "         open(out2, 'w') as foutSmall:\n",
        "\n",
        "        for line in fin:\n",
        "            r = random.random() \n",
        "            if r < percentage:\n",
        "                foutBig.write(line)\n",
        "            else:\n",
        "                foutSmall.write(line)"
      ],
      "id": "e8218f58"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7a883b1c"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/My Drive/'\n",
        "split_huge_file(os.path.join(path, f'train_triplets.txt'), 'train_triplets_splits.txt', 'val_triplets_splits.txt', percentage=0.99, seed=2022)"
      ],
      "id": "7a883b1c"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RWybssqKbO2N"
      },
      "id": "RWybssqKbO2N",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhLDDexX2vpg",
        "outputId": "810c99b4-825f-41ff-db82-d21b1b4c9ed0"
      },
      "id": "WhLDDexX2vpg",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May  4 09:36:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    31W /  70W |   2116MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "8aea5632"
      },
      "outputs": [],
      "source": [
        "# Image loader helper function\n",
        "def default_image_loader(path):\n",
        "    return Image.open(path).convert('RGB')"
      ],
      "id": "8aea5632"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "0fd6fd88"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "im = Image.open(r\"/content/drive/My Drive/food/00003.jpg\")"
      ],
      "id": "0fd6fd88"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "cbb865c1"
      },
      "outputs": [],
      "source": [
        "# display(im)"
      ],
      "id": "cbb865c1"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9407678a",
        "outputId": "fca7f021-01c2-4367-abf4-ef9147dbad3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(329, 468, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "data = np.asarray(im)\n",
        "data.shape"
      ],
      "id": "9407678a"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7960455c"
      },
      "outputs": [],
      "source": [
        "# im.resize((354,242))"
      ],
      "id": "7960455c"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "33fe9cfd"
      },
      "outputs": [],
      "source": [
        "class TripletImageLoader(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_path, triplets_file_name, transform=None, loader=default_image_loader):\n",
        "        \"\"\" base_path: The path contains the text file of the training triplets\n",
        "            triplets_file_name: The text file with each line containing three integers, \n",
        "            where integer i refers to the i-th image in the filenames file.  \n",
        "            Each line contains three integers (a triplet).\n",
        "            For example, the triplet \"00723 00478 02630\" denotes that the dish in image \"00723.jpg\" is more similar in taste \n",
        "            to the dish in image \"00478.jpg\" than to the dish in image \"02630.jpg\" according to a human annotator.\n",
        "         \"\"\"\n",
        "        self.base_path = base_path  \n",
        "        triplets = []\n",
        "        for line in open(triplets_file_name):\n",
        "            triplets.append((line.split()[0], line.split()[1], line.split()[2])) # anchor, positive, negative\n",
        "        self.triplets = triplets\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path1, path2, path3 = self.triplets[index]\n",
        "        img1 = self.loader(os.path.join(self.base_path, f'{path1}.jpg'))\n",
        "        img2 = self.loader(os.path.join(self.base_path, f'{path2}.jpg'))\n",
        "        img3 = self.loader(os.path.join(self.base_path, f'{path3}.jpg'))\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "\n",
        "        return img1, img2, img3\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)"
      ],
      "id": "33fe9cfd"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "38209c7b"
      },
      "outputs": [],
      "source": [
        "# Initialization: importing the packages that we will use\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Google colab offers time limited use of GPU for free\n",
        "\n",
        "################# Configuration  ######################\n",
        "IMAGE_SIZE = (242, 354) # bigger image size improves performance but makes training slower.\n",
        "\n",
        "# Training parameters \n",
        "BATCH_SIZE = 64"
      ],
      "id": "38209c7b"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9974abbb",
        "outputId": "1112fbcf-c2e3-4287-a39e-0d4464d656db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ],
      "id": "9974abbb"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zS2MM05sNKp-",
        "outputId": "90e429f2-cc70-4d5a-e333-4faa052a6235"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "device"
      ],
      "id": "zS2MM05sNKp-"
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/test_triplets.txt /content"
      ],
      "metadata": {
        "id": "Ay-2AUvx-Hyu"
      },
      "id": "Ay-2AUvx-Hyu",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "3b63d3d1"
      },
      "outputs": [],
      "source": [
        "# Dataset and Trasformations\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "############# Datasets and Dataloaders ################\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(), # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # we want our network to be robust over geometrical transformations that leave the image semantically invariant\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #  We transform them to Tensors of normalized range [-1, 1].\n",
        "    # (mean, mean, mean) , (std, std, std): output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "path = '/content/drive/MyDrive/food'\n",
        "train_dataset = TripletImageLoader(path.rstrip('\\n'), 'train_triplets_splits.txt', transform=transform_train)\n",
        "val_dataset = TripletImageLoader(path.rstrip('\\n'), 'val_triplets_splits.txt', transform=transform_val)\n",
        "test_dataset = TripletImageLoader(path.rstrip('\\n'), 'test_triplets.txt', transform=transform_test)"
      ],
      "id": "3b63d3d1"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ff181f",
        "outputId": "0c9f540c-01af-42e6-f844-72c7faccf8cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58933, 582, 59544)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "id": "a7ff181f"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66669e53",
        "outputId": "a87744b6-9890-4274-d5cf-d046b95e75fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.6848,  0.6188,  0.6295,  ...,  0.6845,  0.6174,  0.5918],\n",
              "          [ 0.7634,  0.6081,  0.6502,  ...,  0.7868,  0.7373,  0.6763],\n",
              "          [ 0.7259,  0.7257,  0.6671,  ...,  0.7282,  0.7121,  0.6763],\n",
              "          ...,\n",
              "          [ 0.1368,  0.1779,  0.1858,  ...,  0.5491,  0.5174,  0.5192],\n",
              "          [ 0.1719,  0.1433,  0.1679,  ...,  0.5418,  0.5576,  0.5111],\n",
              "          [ 0.2298,  0.0134,  0.0636,  ...,  0.6480,  0.7295,  0.7176]],\n",
              " \n",
              "         [[ 0.3083,  0.2423,  0.2557,  ...,  0.2424,  0.3027,  0.3446],\n",
              "          [ 0.3778,  0.2235,  0.2622,  ...,  0.3537,  0.3888,  0.3711],\n",
              "          [ 0.3208,  0.3215,  0.2648,  ...,  0.3097,  0.3167,  0.2928],\n",
              "          ...,\n",
              "          [-0.2550, -0.2139, -0.2107,  ...,  0.0953,  0.1322,  0.1737],\n",
              "          [-0.2046, -0.2332, -0.2138,  ...,  0.0879,  0.1528,  0.1360],\n",
              "          [-0.1141, -0.3449, -0.3147,  ...,  0.1256,  0.0728, -0.0351]],\n",
              " \n",
              "         [[ 0.1512,  0.0762,  0.0626,  ..., -0.0076, -0.0635, -0.0872],\n",
              "          [ 0.2222,  0.0503,  0.0729,  ...,  0.0857,  0.1187,  0.0999],\n",
              "          [ 0.1609,  0.1532,  0.0806,  ...,  0.0070,  0.1151,  0.1430],\n",
              "          ...,\n",
              "          [-0.6279, -0.5769, -0.5614,  ..., -0.1400, -0.3230, -0.3943],\n",
              "          [-0.5889, -0.6165, -0.5872,  ..., -0.1474, -0.1789, -0.2451],\n",
              "          [-0.4484, -0.6471, -0.5584,  ..., -0.1159, -0.0343, -0.0555]]]),\n",
              " tensor([[[0.9716, 0.9817, 0.9216,  ..., 0.9213, 0.8747, 0.8964],\n",
              "          [0.8742, 0.9024, 0.9108,  ..., 0.9460, 0.9252, 0.9207],\n",
              "          [0.9315, 0.9169, 0.9928,  ..., 0.9602, 0.9514, 0.9227],\n",
              "          ...,\n",
              "          [0.9928, 0.8873, 0.8463,  ..., 0.9299, 0.9639, 0.9754],\n",
              "          [0.9918, 0.8900, 0.8946,  ..., 0.9904, 0.8834, 0.8569],\n",
              "          [0.9392, 0.9270, 0.9678,  ..., 1.0000, 0.9470, 0.9445]],\n",
              " \n",
              "         [[0.9498, 0.9751, 0.8980,  ..., 0.8986, 0.8418, 0.8492],\n",
              "          [0.8506, 0.8799, 0.8897,  ..., 0.9100, 0.8667, 0.8481],\n",
              "          [0.9057, 0.8961, 0.9808,  ..., 0.9032, 0.8778, 0.8328],\n",
              "          ...,\n",
              "          [0.9762, 0.8023, 0.7349,  ..., 0.9023, 0.9496, 0.9684],\n",
              "          [0.9093, 0.7772, 0.7547,  ..., 0.9391, 0.8346, 0.8207],\n",
              "          [0.8007, 0.7855, 0.8103,  ..., 0.9511, 0.8843, 0.8824]],\n",
              " \n",
              "         [[0.8159, 0.8326, 0.7496,  ..., 0.8866, 0.8447, 0.8637],\n",
              "          [0.7200, 0.7490, 0.7521,  ..., 0.8754, 0.8447, 0.8475],\n",
              "          [0.7949, 0.7840, 0.8595,  ..., 0.8347, 0.8253, 0.7896],\n",
              "          ...,\n",
              "          [0.4972, 0.3007, 0.2132,  ..., 0.4547, 0.4431, 0.3899],\n",
              "          [0.4153, 0.2669, 0.2286,  ..., 0.4663, 0.2925, 0.1991],\n",
              "          [0.2980, 0.2647, 0.2715,  ..., 0.4342, 0.3089, 0.2374]]]),\n",
              " tensor([[[ 0.0331,  0.0900,  0.1960,  ..., -0.1534, -0.2033, -0.2207],\n",
              "          [ 0.1129,  0.1367,  0.2146,  ..., -0.1964, -0.1716, -0.1799],\n",
              "          [ 0.2135,  0.2242,  0.2509,  ..., -0.1864, -0.2052, -0.1794],\n",
              "          ...,\n",
              "          [-0.8423, -0.7983, -0.8123,  ..., -0.9410, -0.9332, -0.9569],\n",
              "          [-0.8787, -0.7546, -0.7393,  ..., -0.9468, -0.9598, -0.9783],\n",
              "          [-0.8815, -0.8117, -0.6378,  ..., -0.9609, -0.9906, -0.9991]],\n",
              " \n",
              "         [[-0.6539, -0.6050, -0.5067,  ..., -0.8570, -0.9107, -0.9321],\n",
              "          [-0.6197, -0.6015, -0.5323,  ..., -0.8866, -0.8696, -0.8779],\n",
              "          [-0.5927, -0.5852, -0.5585,  ..., -0.8533, -0.8764, -0.8541],\n",
              "          ...,\n",
              "          [-0.8133, -0.7810, -0.8200,  ..., -0.9331, -0.9019, -0.9193],\n",
              "          [-0.8412, -0.7344, -0.7470,  ..., -0.9389, -0.9328, -0.9415],\n",
              "          [-0.8298, -0.7848, -0.6522,  ..., -0.9530, -0.9725, -0.9763]],\n",
              " \n",
              "         [[-0.9876, -0.9549, -0.8518,  ..., -0.9287, -0.9644, -0.9599],\n",
              "          [-0.9759, -0.9650, -0.8879,  ..., -0.9650, -0.9260, -0.9124],\n",
              "          [-0.9720, -0.9597, -0.9330,  ..., -0.9375, -0.9421, -0.8945],\n",
              "          ...,\n",
              "          [-0.7238, -0.6822, -0.7102,  ..., -0.9018, -0.8783, -0.8957],\n",
              "          [-0.7376, -0.6251, -0.6372,  ..., -0.9075, -0.9006, -0.9128],\n",
              "          [-0.7064, -0.6645, -0.5424,  ..., -0.9149, -0.9333, -0.9434]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "test_dataset[0]"
      ],
      "id": "66669e53"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "9804534d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
      ],
      "id": "9804534d"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a237b3",
        "outputId": "0d25596d-41b4-4c02-eb2b-c56305693fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(921, 10, 931)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ],
      "id": "f7a237b3"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "cdce85ca"
      },
      "outputs": [],
      "source": [
        "# Visualization of Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    plt.figure()\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images_anchor, images_positive, images_negative = dataiter.next()\n",
        "\n",
        "# # show images\n",
        "# imshow(torchvision.utils.make_grid(images_anchor))\n",
        "# imshow(torchvision.utils.make_grid(images_positive))\n",
        "# imshow(torchvision.utils.make_grid(images_negative))"
      ],
      "id": "cdce85ca"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40002326",
        "outputId": "b17e4b8f-e856-4443-8d89-a10249ee4683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== DATA LOADED ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Data loaded\n",
        "'''\n",
        "print('================== DATA LOADED ==================')"
      ],
      "id": "40002326"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87028c17",
        "outputId": "65b5c69b-a0f2-49c0-e6b3-b4ca4e707c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START CONSTRUCTING NETWORK ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start constructing the network\n",
        "'''\n",
        "print('================== START CONSTRUCTING NETWORK ==================')"
      ],
      "id": "87028c17"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fae256a1"
      },
      "outputs": [],
      "source": [
        "# import torch.optim\n",
        "# import torch.utils.data\n",
        "# import torch\n",
        "# import torchvision\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# import torch.utils.data\n",
        "# import torch.backends.cudnn as cudnn\n",
        "\n",
        "# #########################NET##############################\n",
        "\n",
        "# #The backbone for the CNNS with shared weights\n",
        "# def backbone(**kwargs):\n",
        "#     \"\"\"\n",
        "#     Construct a ResNet-101 model.\n",
        "#     Returns:\n",
        "#         Embeddingnet(model): The CNN with the specified model as its backbone is instantiated\n",
        "#     \"\"\"\n",
        "#     #model = torch.hub.load('pytorch/vision:v1.7.1', 'resnet101', pretrained=True)\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     #model = models.resnet34(pretrained=True)\n",
        "#     #model = models.vgg11_bn()\n",
        "#     #model = torch.hub.load('pytorch/vision:v0.8.2', 'alexnet', pretrained=True)\n",
        "#     #model = models.alexnet(pretrained=True)            #used in the paper\n",
        "#     #print('Layers',model.children)\n",
        "#     #model = models.resnet50(pretrained=True)\n",
        "#     #model = models.inception_v3(pretrained=True)\n",
        "#     #model = torchvision.models.resnet.ResNet(\n",
        "#         #torchvision.models.resnet.BasicBlock, [2, 1, 1, 1])\n",
        "\n",
        "#     return EmbeddingNet(model)\n",
        "\n",
        "# #The overall network consisting of three embedding nets with shared weights\n",
        "# class TripletNet(nn.Module):\n",
        "#     \"\"\"Triplet Network.\"\"\"\n",
        "\n",
        "#     def __init__(self, embeddingnet):\n",
        "#         \"\"\"Triplet Network Builder.\"\"\"\n",
        "#         super(TripletNet, self).__init__()\n",
        "#         self.embeddingnet = embeddingnet\n",
        "#         #print(self.embeddingnet.children())\n",
        "\n",
        "#     def forward(self, a, p, n):\n",
        "#         \"\"\"Forward pass.\"\"\"\n",
        "#         # anchor\n",
        "#         embedded_a = self.embeddingnet(a)\n",
        "\n",
        "#         # positive examples\n",
        "#         embedded_p = self.embeddingnet(p)\n",
        "\n",
        "#         # negative examples\n",
        "#         embedded_n = self.embeddingnet(n)\n",
        "\n",
        "#         return embedded_a, embedded_p, embedded_n\n",
        "\n",
        "# #The CNN used by Triplet Net with 'model' as its backbone and a final fully connected Layer\n",
        "# class EmbeddingNet(nn.Module):\n",
        "#     \"\"\"EmbeddingNet using the specified model in backbone().\"\"\"\n",
        "\n",
        "#     def __init__(self, resnet):\n",
        "#         \"\"\"Initialize EmbeddingNet model.\"\"\"\n",
        "#         super(EmbeddingNet, self).__init__()\n",
        "#         # Everything excluding the last linear layer\n",
        "#         self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "#         num_ftrs =  resnet.fc.in_features\n",
        "#         self.fc1 = nn.Linear(num_ftrs, 1024)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"Forward pass of EmbeddingNet.\"\"\"\n",
        "#         out = self.features(x)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc1(out)\n",
        "#         return out"
      ],
      "id": "fae256a1"
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "7605800f"
      },
      "outputs": [],
      "source": [
        "# Construct a triplet net\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "#########################NET##############################\n",
        "\n",
        "#The backbone for the CNNS with shared weights\n",
        "def FeatureExtractNET(**kwargs):\n",
        "    \"\"\"\n",
        "    Construct a ResNet-101 model.\n",
        "    Returns: The CNN for feature extraction with a fully connected layer\n",
        "    \"\"\"\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    return EmbeddingNet(model)\n",
        "\n",
        "#The CNN used by Triplet Net with 'model' as its backbone and a final fully connected Layer\n",
        "class EmbeddingNet(nn.Module):\n",
        "    \"\"\"EmbeddingNet using the specified model in backbone().\"\"\"\n",
        "\n",
        "    def __init__(self, resnet):\n",
        "        \"\"\"Initialize EmbeddingNet model.\"\"\"\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        # Everything excluding the last linear layer\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        num_ftrs =  resnet.fc.in_features\n",
        "        self.fc1 = nn.Linear(num_ftrs, 1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of EmbeddingNet.\"\"\"\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "    \n",
        "#The overall network consisting of three embedding nets with shared weights\n",
        "class TripletNet(nn.Module):\n",
        "    \"\"\"Triplet Network.\"\"\"\n",
        "\n",
        "    def __init__(self, embeddingnet):\n",
        "        \"\"\"Triplet Network Builder.\"\"\"\n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embeddingnet = embeddingnet\n",
        "\n",
        "    def forward(self, a, p, n):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # anchor\n",
        "        embedded_a = self.embeddingnet(a)\n",
        "\n",
        "        # positive examples\n",
        "        embedded_p = self.embeddingnet(p)\n",
        "\n",
        "        # negative examples\n",
        "        embedded_n = self.embeddingnet(n)\n",
        "\n",
        "        return embedded_a, embedded_p, embedded_n"
      ],
      "id": "7605800f"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7843631c",
        "outputId": "c417aeb6-267f-46ee-b46b-a24df18ab7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Initialize CUDA support for TripletNet model ...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): TripletNet(\n",
              "    (embeddingnet): EmbeddingNet(\n",
              "      (features): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "        (4): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      )\n",
              "      (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "net = TripletNet(FeatureExtractNET())\n",
        "\n",
        "#Move the net to GPU for training\n",
        "print(\"==> Initialize CUDA support for TripletNet model ...\")\n",
        "net = torch.nn.DataParallel(net).cuda()\n",
        "cudnn.benchmark = True\n",
        "net"
      ],
      "id": "7843631c"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "wO2TH6TjURu8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "wO2TH6TjURu8"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "3626cfdd"
      },
      "outputs": [],
      "source": [
        "# batch = next(iter(train_loader))\n",
        "# batch[0], batch[1], batch[2] = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()\n",
        "# net(batch[0],batch[1],batch[2])[0].size()"
      ],
      "id": "3626cfdd"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "b71be7bc"
      },
      "outputs": [],
      "source": [
        "# import hiddenlayer as hl\n",
        "\n",
        "# transforms = [hl.transforms.Prune('Constant')] # Removes Constant nodes from graph.\n",
        "\n",
        "# graph = hl.build_graph(net, (batch[0], batch[1], batch[2]), transforms=transforms)\n",
        "# graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "# graph.save('rnn_hiddenlayer_1', format='png')"
      ],
      "id": "b71be7bc"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "5fa4121e"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.TripletMarginLoss(margin=5.0, p=2)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(),\n",
        "                            lr=0.0005,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=2e-3,#The value used in the paper is 1e-3\n",
        "                            nesterov=True)"
      ],
      "id": "5fa4121e"
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "vQXSC0QUb2y9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "vQXSC0QUb2y9"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "239e50ac"
      },
      "outputs": [],
      "source": [
        "# from torch.autograd import Variable\n",
        "# for epoch in range(1):\n",
        "\n",
        "#         running_loss = 0.0\n",
        "#         loss_train = 0.0\n",
        "#         for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
        "\n",
        "# #             if is_gpu:\n",
        "# #                 data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "#             # wrap in torch.autograd.Variable\n",
        "#             data1, data2, data3 = Variable(\n",
        "#                 data1), Variable(data2), Variable(data3)\n",
        "#             print('anchor', data1.size())\n",
        "#             print('positive', data2.size())\n",
        "#             print('negative', data3.size())\n",
        "\n",
        "#             # compute output and loss\n",
        "#             embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
        "#             loss = criterion(embedded_a, embedded_p, embedded_n)\n",
        "#             print(loss)\n",
        "\n",
        "#             # compute gradient and do optimizer step\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # print the loss\n",
        "#             running_loss += loss.data\n",
        "\n",
        "# #             loss_train_cls = torch.sum(\n",
        "# #                 1 * (criterion_val(embedded_a, embedded_p,\n",
        "# #                                    embedded_n) > 0)) / train_batch_size  # CHANGED, MAY NEED TO REVERT BACK\n",
        "\n",
        "# #             loss_train += loss_train_cls.data\n",
        "\n",
        "#             if batch_idx % 30 == 0:\n",
        "#                 print(\"mini Batch Loss: {}\".format(loss.data))"
      ],
      "id": "239e50ac"
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3dca38",
        "outputId": "d59e89c6-3242-4fb8-81ba-9a78749c978c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== NETWORK CONSTRUCTED ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Network constructed\n",
        "'''\n",
        "print('================== NETWORK CONSTRUCTED ==================')"
      ],
      "id": "1c3dca38"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "5vdmS_syHlgh"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# pdist = nn.PairwiseDistance(p=2)\n",
        "# input1 = torch.randn(64, 1024)\n",
        "# input2 = torch.randn(64, 1024)\n",
        "# input3 = torch.randn(64, 1024)\n",
        "# print(input1.size())\n",
        "# print(input2.size())\n",
        "# print(input3.size())\n",
        "# dist1 = pdist(input1, input2)\n",
        "# dist2 = pdist(input1, input3)\n",
        "# print(dist1.size())\n",
        "# print(dist2.size())\n",
        "# pred = dist1 - dist2\n",
        "# print(pred.size())\n",
        "# sum = 0\n",
        "# for i in range(pred.size()[0]):\n",
        "#   if pred[i] < 0:\n",
        "#     sum+=1\n",
        "# print(sum/pred.size()[0])\n",
        "# print((pred < 0).sum()*1.0/pred.size()[0])"
      ],
      "id": "5vdmS_syHlgh"
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "49lQY_gyLSqM"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# for i in range(10):\n",
        "#   a = random.randint(0,10)\n",
        "#   print(a)"
      ],
      "id": "49lQY_gyLSqM"
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "M5uSm08ngNj7"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def accuracy(dista, distb):\n",
        "    margin = 0\n",
        "    pred = (dista - distb).cpu().data\n",
        "    return (pred < 0).sum()*1.0/dista.size()[0]\n",
        "\n",
        "def val_accuracy(trainednet, valloader, valloader_iter):\n",
        "  # sum_accuracy = 0\n",
        "  # num_batch_evaluated = 0\n",
        "  \n",
        "  # Pick one batches for evaluation\n",
        "  # selected_batch = random.randint(0, len(valloader))\n",
        "\n",
        "  # print('Batch selected for evaluation: ', selected_batch)\n",
        "  try:\n",
        "    data1, data2, data3 = next(valloader_iter)\n",
        "  except StopIteration:\n",
        "    valloader_iterator = iter(valloader)\n",
        "    data1, data2, data3 = next(valloader_iterator)\n",
        "\n",
        "  data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "  # wrap in torch.autograd.Variable\n",
        "  data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # compute output and loss\n",
        "    embedded_x, embedded_y, embedded_z = trainednet(data1, data2, data3)\n",
        "    dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "    dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "    print('dist a: {0}, dist b: {1}'.format(dist_a, dist_b))\n",
        "    batch_accuracy = accuracy(dist_a, dist_b)\n",
        "    print('random batch accuracy: {0} '.format(batch_accuracy))\n",
        "\n",
        "  # mean_accuracy = sum_accuracy / num_batch_evaluated\n",
        "  return batch_accuracy"
      ],
      "id": "M5uSm08ngNj7"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "6bw80HHgiuxo"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# batch = next(iter(train_loader))\n",
        "# batch[0], batch[1], batch[2] = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()\n",
        "# embedded_x, embedded_y, embedded_z = net(batch[0],batch[1],batch[2])\n",
        "# dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "# dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "# accuracy = accuracy(dist_a, dist_b)\n",
        "# print(accuracy)"
      ],
      "id": "6bw80HHgiuxo"
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "d1e25fe5"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, epochs, trainloader, valloader):\n",
        "\n",
        "  # Create an iterator object for valloader, for selecting a random batch from val set for validation\n",
        "  valloader_iterator = iter(valloader)\n",
        "\n",
        "  # Empty the cache of CUDA  \n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  print('================== START TRAINING ==================')\n",
        "  # Change to train mode\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "      running_loss = 0\n",
        "      for batch_idx, (data0, data1, data2) in enumerate(trainloader):\n",
        "          anchor, positive, negative = data0, data1, data2\n",
        "          anchor = Variable(anchor)\n",
        "          positive = Variable(positive)\n",
        "          negative = Variable(negative)\n",
        "          # print('anchor', anchor.size())\n",
        "          # print('positive', positive.size())\n",
        "          # print('negative', negative.size())\n",
        "          \n",
        "          # Calculate the output of three networks\n",
        "          embedded_a, embedded_p, embedded_n = model(anchor, positive, negative)\n",
        "          \n",
        "          # Calculate the loss\n",
        "          loss = criterion(embedded_a, embedded_p, embedded_n)\n",
        "          print(\"mini Batch Loss: {}\".format(loss.data))\n",
        "          \n",
        "          # Zero the gradient\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          # Back prop and update\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if batch_idx % 30 == 0:\n",
        "            print(\"Training Batch: {0} | Training Loss: {1}\".format(batch_idx+1, loss.data))\n",
        "            save_path = f'/content/drive/My Drive/test2/model_epoch_{epoch+1}_batch_{batch_idx+1}.pt'\n",
        "            torch.save({'Batch': batch_idx, 'model_state_dict': model.state_dict()}, save_path)\n",
        "            print(\"Training Batch: {0} | Model saved to: {1}\".format(batch_idx+1, save_path))\n",
        "\n",
        "            ''' For Validation'''\n",
        "            # Change to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            mean_accuracy = val_accuracy(model, valloader, valloader_iterator)\n",
        "            print(mean_accuracy)\n",
        "\n",
        "            # Change back to train mode\n",
        "            model.train()\n",
        "\n",
        "          # Empty the cache of CUDA  \n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "          \n",
        "      print(f'[{epoch + 1}] average loss per epoch: {running_loss / len(train_loader):.3f}')\n",
        "      # # save checkpoint of model\n",
        "      # if epoch % 5 == 0 and epoch > 0:\n",
        "\n",
        "      save_path = f'/content/drive/My Drive/test2/model_epoch_{epoch}.pt'\n",
        "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict()}, save_path)\n",
        "      print(f'Saved model checkpoint to {save_path}')\n",
        "\n",
        "      ''' For Validation'''\n",
        "      # Change to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      mean_accuracy = val_accuracy(model, valloader, valloader_iterator)\n",
        "      print(mean_accuracy)\n",
        "\n",
        "      # Change back to train mode\n",
        "      model.train()\n",
        "\n",
        "      \n",
        "  print('Finished Training')"
      ],
      "id": "d1e25fe5"
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7f8cdf",
        "outputId": "d84a3e2a-c104-4a6c-ffca-d1ea6fcc4a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START TRAINING ==================\n",
            "mini Batch Loss: 4.412944793701172\n",
            "Training Batch: 1 | Training Loss: 4.412944793701172\n",
            "Training Batch: 1 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_1.pt\n",
            "dist a: tensor([18.4125, 14.7937, 12.6985, 14.4316, 12.9494, 14.3909, 13.5844, 12.5614,\n",
            "        13.7599, 17.7909, 13.2882, 19.5298, 14.3636, 13.2031, 15.4984, 16.8897,\n",
            "        16.1553, 15.2223, 13.0401, 17.1888, 14.8710, 14.5714, 12.3360, 16.0529,\n",
            "        13.5545, 14.7240, 14.7704, 17.2614, 14.5392, 10.7585, 13.2949, 14.2915,\n",
            "        12.3717, 11.3969, 16.5193, 15.9918, 14.4647, 15.5827, 13.5296, 13.6141,\n",
            "        14.3182, 14.7529, 16.3411, 14.9880, 20.2623, 15.0018, 15.0246, 15.1213,\n",
            "        14.2975, 10.4766, 15.4648, 14.6889, 14.0904, 13.5552, 14.4220, 12.6184,\n",
            "        14.4068, 16.6642, 16.8570, 13.0066, 16.0955, 15.2479, 15.8116, 14.5757],\n",
            "       device='cuda:0'), dist b: tensor([17.5949, 17.3178, 15.9329, 15.1129, 13.9413, 16.6605, 17.4067, 15.9361,\n",
            "        14.5841, 16.2026, 12.7968, 18.3549, 17.4304, 15.8927, 14.1591, 14.2030,\n",
            "        17.2526, 20.1339, 15.8397, 15.1371, 12.7316, 15.2612, 14.3645, 15.9560,\n",
            "        16.5286, 15.5810, 12.6845, 17.8616, 14.9124, 12.9622, 16.0354, 11.6616,\n",
            "        16.7066, 13.0389, 16.9168, 14.3351, 16.2513, 14.5842, 17.5479, 13.7125,\n",
            "        15.0262, 13.2461, 13.7047, 16.0809, 16.0920, 15.4865, 15.1304, 13.9844,\n",
            "        13.1223, 14.7068, 18.1354, 15.6954, 15.6440, 13.1797, 14.3466, 11.7019,\n",
            "        14.9511, 16.3906, 13.6227, 14.3601, 16.3608, 13.7265, 16.5104, 16.9625],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.625 \n",
            "tensor(0.6250)\n",
            "mini Batch Loss: 4.6365461349487305\n",
            "mini Batch Loss: 4.608180999755859\n",
            "mini Batch Loss: 4.797224998474121\n",
            "mini Batch Loss: 4.563261985778809\n",
            "mini Batch Loss: 4.458860874176025\n",
            "mini Batch Loss: 4.8855390548706055\n",
            "mini Batch Loss: 4.270105361938477\n",
            "mini Batch Loss: 4.276784896850586\n",
            "mini Batch Loss: 4.653641700744629\n",
            "mini Batch Loss: 4.380197525024414\n",
            "mini Batch Loss: 4.972185134887695\n",
            "mini Batch Loss: 4.678810119628906\n",
            "mini Batch Loss: 5.105303764343262\n",
            "mini Batch Loss: 4.70206356048584\n",
            "mini Batch Loss: 4.314232349395752\n",
            "mini Batch Loss: 4.487246036529541\n",
            "mini Batch Loss: 4.6863555908203125\n",
            "mini Batch Loss: 4.948626518249512\n",
            "mini Batch Loss: 4.2499237060546875\n",
            "mini Batch Loss: 4.703093528747559\n",
            "mini Batch Loss: 4.719814300537109\n",
            "mini Batch Loss: 4.786975860595703\n",
            "mini Batch Loss: 4.532729148864746\n",
            "mini Batch Loss: 4.459679126739502\n",
            "mini Batch Loss: 4.36851692199707\n",
            "mini Batch Loss: 3.9574170112609863\n",
            "mini Batch Loss: 4.443903923034668\n",
            "mini Batch Loss: 4.411330223083496\n",
            "mini Batch Loss: 4.119119644165039\n",
            "mini Batch Loss: 4.603891372680664\n",
            "Training Batch: 31 | Training Loss: 4.603891372680664\n",
            "Training Batch: 31 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_31.pt\n",
            "dist a: tensor([22.3919, 16.7742, 17.3394, 17.8609, 15.6277, 15.7004, 15.5880, 16.3939,\n",
            "        17.5605, 19.5789, 17.7572, 17.2769, 18.2772, 16.8852, 18.8858, 14.8135,\n",
            "        18.3795, 14.6282, 15.9606, 13.8622, 14.7788, 17.7635, 12.3507, 15.6608,\n",
            "        16.1495, 17.9630, 18.6005, 15.3782, 16.6255, 16.0431, 15.7502, 13.9568,\n",
            "        14.6511, 20.4243, 14.4686, 14.3404, 16.2894, 17.7280, 18.3563, 16.7963,\n",
            "        13.4671, 15.4321, 18.5766, 15.1335, 18.6601, 17.0184, 16.8654, 19.7187,\n",
            "        18.0148, 17.2918, 16.2091, 17.2429, 14.5429, 15.1271, 12.1973, 14.0330,\n",
            "        14.7231, 15.9941, 19.4454, 13.8105, 16.7859, 17.0545, 16.1795, 17.8425],\n",
            "       device='cuda:0'), dist b: tensor([16.9331, 17.4622, 19.4855, 16.7991, 16.1832, 18.5434, 14.1922, 19.1392,\n",
            "        16.3790, 15.9664, 15.9114, 15.9545, 19.2191, 17.7089, 17.8331, 16.9910,\n",
            "        21.0305, 15.4875, 16.5440, 21.0746, 19.2169, 18.5655, 15.1190, 14.8854,\n",
            "        19.5462, 15.8118, 17.1634, 14.1687, 15.6699, 15.9482, 19.5778, 11.0669,\n",
            "        14.6019, 19.1429, 13.1744, 17.2167, 13.1633, 15.9995, 19.8176, 14.9956,\n",
            "        13.3048, 17.5981, 20.1203, 17.6494, 17.3027, 18.0047, 18.0080, 14.7346,\n",
            "        15.1833, 16.6949, 15.8477, 16.3692, 13.6950, 15.2622, 18.0107, 16.2244,\n",
            "        15.5542, 17.6500, 18.9280, 15.6765, 19.2760, 16.0401, 15.6475, 15.9502],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.484375 \n",
            "tensor(0.4844)\n",
            "mini Batch Loss: 4.184832572937012\n",
            "mini Batch Loss: 4.3088226318359375\n",
            "mini Batch Loss: 4.194525718688965\n",
            "mini Batch Loss: 4.673562049865723\n",
            "mini Batch Loss: 4.085546016693115\n",
            "mini Batch Loss: 4.874399185180664\n",
            "mini Batch Loss: 4.133363246917725\n",
            "mini Batch Loss: 4.7782464027404785\n",
            "mini Batch Loss: 4.243902683258057\n",
            "mini Batch Loss: 4.226424217224121\n",
            "mini Batch Loss: 4.521239280700684\n",
            "mini Batch Loss: 4.4726362228393555\n",
            "mini Batch Loss: 3.97289776802063\n",
            "mini Batch Loss: 3.8404088020324707\n",
            "mini Batch Loss: 4.1309494972229\n",
            "mini Batch Loss: 4.140923500061035\n",
            "mini Batch Loss: 3.8029944896698\n",
            "mini Batch Loss: 4.657375335693359\n",
            "mini Batch Loss: 3.938804864883423\n",
            "mini Batch Loss: 3.7577099800109863\n",
            "mini Batch Loss: 4.388097763061523\n",
            "mini Batch Loss: 3.7048935890197754\n",
            "mini Batch Loss: 3.8088455200195312\n",
            "mini Batch Loss: 4.734986305236816\n",
            "mini Batch Loss: 3.7848472595214844\n",
            "mini Batch Loss: 3.8170461654663086\n",
            "mini Batch Loss: 3.5825247764587402\n",
            "mini Batch Loss: 4.245203971862793\n",
            "mini Batch Loss: 3.604614734649658\n",
            "mini Batch Loss: 3.7640790939331055\n",
            "Training Batch: 61 | Training Loss: 3.7640790939331055\n",
            "Training Batch: 61 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_61.pt\n",
            "dist a: tensor([14.3923, 18.5454, 14.2552, 15.4375, 17.0542, 17.8678, 15.1521, 15.9192,\n",
            "        20.2616, 13.6917, 21.6646, 16.6029, 14.8691, 17.8025, 17.8400, 22.1279,\n",
            "        15.4269, 15.9495, 17.9524, 17.3455, 16.6586, 18.8552, 17.6863, 14.9991,\n",
            "        19.6398, 19.1574, 20.3962, 19.5301, 13.9480, 15.0408, 17.2208, 15.4122,\n",
            "        16.3549, 17.1821, 12.9404, 13.8343, 14.3440, 20.3364, 13.8805, 19.6290,\n",
            "        15.3522, 14.9425, 20.6996, 19.8462, 21.9524, 16.5677, 12.6446, 14.1443,\n",
            "        16.8327, 17.0554, 21.1904, 19.2273, 13.9227, 16.9252, 16.6069, 18.0598,\n",
            "        18.9976, 24.8125, 17.3630, 18.3379, 11.9680, 20.8247, 16.3464, 17.6419],\n",
            "       device='cuda:0'), dist b: tensor([22.7526, 18.7615, 16.1714, 17.8204, 18.5379, 22.0440, 16.4793, 16.0260,\n",
            "        17.2388, 15.6221, 18.2617, 20.4671, 21.0085, 20.4820, 14.8041, 16.6992,\n",
            "        23.2588, 19.4423, 19.7763, 21.3174, 24.6016, 19.6712, 18.9168, 14.8598,\n",
            "        17.1180, 15.0884, 22.1779, 20.7428, 22.5572, 24.4361, 24.9821, 15.0945,\n",
            "        20.9135, 17.8705, 18.2285, 17.1981, 17.9632, 13.8790, 23.0095, 19.3874,\n",
            "        18.3677, 16.0664, 16.1826, 15.2286, 21.0020, 15.2529, 17.2775, 22.5605,\n",
            "        19.0990, 17.5522, 26.5124, 16.1741, 16.1660, 18.2314, 16.6603, 16.5049,\n",
            "        18.2579, 15.7914, 21.6465, 19.8109, 16.4476, 13.5292, 17.9426, 15.4693],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.6875 \n",
            "tensor(0.6875)\n",
            "mini Batch Loss: 4.163460731506348\n",
            "mini Batch Loss: 3.5942270755767822\n",
            "mini Batch Loss: 3.7485780715942383\n",
            "mini Batch Loss: 3.821420431137085\n",
            "mini Batch Loss: 3.7108805179595947\n",
            "mini Batch Loss: 3.610060930252075\n",
            "mini Batch Loss: 3.889937400817871\n",
            "mini Batch Loss: 4.270237445831299\n",
            "mini Batch Loss: 3.5917701721191406\n",
            "mini Batch Loss: 4.152673244476318\n",
            "mini Batch Loss: 3.588104248046875\n",
            "mini Batch Loss: 3.8727056980133057\n",
            "mini Batch Loss: 3.318948268890381\n",
            "mini Batch Loss: 3.758040428161621\n",
            "mini Batch Loss: 3.2613961696624756\n",
            "mini Batch Loss: 3.8033101558685303\n",
            "mini Batch Loss: 4.078432083129883\n",
            "mini Batch Loss: 3.48130464553833\n",
            "mini Batch Loss: 3.71335768699646\n",
            "mini Batch Loss: 3.9348349571228027\n",
            "mini Batch Loss: 3.418076276779175\n",
            "mini Batch Loss: 3.7466623783111572\n",
            "mini Batch Loss: 3.016197443008423\n",
            "mini Batch Loss: 3.342299461364746\n",
            "mini Batch Loss: 4.241156101226807\n",
            "mini Batch Loss: 3.280536651611328\n",
            "mini Batch Loss: 3.7435994148254395\n",
            "mini Batch Loss: 3.143606424331665\n",
            "mini Batch Loss: 3.202789306640625\n",
            "mini Batch Loss: 4.078507423400879\n",
            "Training Batch: 91 | Training Loss: 4.078507423400879\n",
            "Training Batch: 91 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_91.pt\n",
            "dist a: tensor([15.7610, 17.1987, 24.2637, 13.0014, 18.1098, 17.4994, 15.6519, 13.3938,\n",
            "        13.5476, 23.6970, 15.7063, 18.1352, 24.2804, 21.2211, 22.9368, 16.1688,\n",
            "        20.5929, 17.5054, 19.1844, 17.0293, 25.4379, 13.8681, 14.2816, 18.7208,\n",
            "        27.5358, 12.7380, 13.0620, 18.5788, 21.1914, 19.7169, 17.2994, 20.3661,\n",
            "        17.2562, 23.2064, 16.0590, 17.2378, 19.7308, 16.1989, 21.4585, 15.4590,\n",
            "        17.6968, 17.2037, 13.1487, 18.1385, 18.0835, 21.1689, 15.4190, 18.4053,\n",
            "        12.3091, 19.6274, 18.5325, 23.8344, 17.8112, 13.3003, 16.8437, 21.7665,\n",
            "        14.0256, 14.2326, 15.3523, 18.8918, 19.7198, 17.1736, 18.7253, 21.9853],\n",
            "       device='cuda:0'), dist b: tensor([16.5159, 20.5511, 25.1276, 19.6692, 22.2838, 16.5497, 20.7420, 15.2601,\n",
            "        22.7987, 16.0293, 20.6635, 15.4819, 16.8248, 21.9226, 16.3754, 16.8288,\n",
            "        20.1679, 18.7631, 19.6105, 17.5646, 25.1922, 13.2780, 12.2802, 16.9749,\n",
            "        30.1497, 24.7959, 21.1904, 20.4044, 15.9498, 21.8078, 17.7433, 19.4798,\n",
            "        24.3511, 26.4578, 17.2613, 18.2520, 16.6089, 18.6055, 16.0482, 17.7540,\n",
            "        23.3054, 19.8832, 16.9045, 18.0457, 18.6006, 20.7563, 13.8861, 16.0376,\n",
            "        27.3396, 24.7930, 19.3748, 19.1241, 21.6049, 15.8515, 14.5551, 21.2567,\n",
            "        20.1752, 16.3237, 16.7643, 21.6229, 26.8732, 19.7688, 26.2699, 24.6576],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.671875 \n",
            "tensor(0.6719)\n",
            "mini Batch Loss: 3.3390395641326904\n",
            "mini Batch Loss: 3.976857900619507\n",
            "mini Batch Loss: 3.2600412368774414\n",
            "mini Batch Loss: 3.8122634887695312\n",
            "mini Batch Loss: 3.218477725982666\n",
            "mini Batch Loss: 3.3273677825927734\n",
            "mini Batch Loss: 3.7363076210021973\n",
            "mini Batch Loss: 3.723024845123291\n",
            "mini Batch Loss: 3.564185380935669\n",
            "mini Batch Loss: 2.786552906036377\n",
            "mini Batch Loss: 3.7153210639953613\n",
            "mini Batch Loss: 3.265787363052368\n",
            "mini Batch Loss: 3.379662036895752\n",
            "mini Batch Loss: 3.1416313648223877\n",
            "mini Batch Loss: 3.032357931137085\n",
            "mini Batch Loss: 3.016976833343506\n",
            "mini Batch Loss: 3.0044493675231934\n",
            "mini Batch Loss: 3.374495029449463\n",
            "mini Batch Loss: 3.4637234210968018\n",
            "mini Batch Loss: 3.561558485031128\n",
            "mini Batch Loss: 3.5001611709594727\n",
            "mini Batch Loss: 3.8901526927948\n",
            "mini Batch Loss: 3.7608044147491455\n",
            "mini Batch Loss: 3.516803741455078\n",
            "mini Batch Loss: 2.674370765686035\n",
            "mini Batch Loss: 3.098781108856201\n",
            "mini Batch Loss: 3.3990039825439453\n",
            "mini Batch Loss: 3.0913586616516113\n",
            "mini Batch Loss: 3.8541271686553955\n",
            "mini Batch Loss: 2.7018256187438965\n",
            "Training Batch: 121 | Training Loss: 2.7018256187438965\n",
            "Training Batch: 121 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_121.pt\n",
            "dist a: tensor([17.8556, 14.5203, 20.2818, 17.4063, 15.8763, 13.1675, 20.6521, 17.2524,\n",
            "        18.5781, 19.7981, 16.4991, 15.5598, 15.2478, 14.7515, 16.2021, 20.3391,\n",
            "        22.6965, 15.4379, 15.9100, 17.3769, 20.0410, 21.7839, 16.5730, 17.2484,\n",
            "        13.3267, 19.5848, 13.6663, 17.4969, 15.9654, 20.7688, 18.9156, 15.5085,\n",
            "        20.0701, 18.9432, 18.2004, 12.4129, 17.3230, 13.6569, 17.0932, 12.7474,\n",
            "        21.5122, 15.8254, 15.8836, 20.7884, 16.6766, 12.7761, 13.7568, 18.7269,\n",
            "        24.3462, 14.3533, 13.2735, 17.8451, 17.0670, 14.2750, 19.2350, 32.4058,\n",
            "        18.5700, 16.5836, 16.5836, 16.4359, 15.4837, 15.5736, 21.6418, 15.0867],\n",
            "       device='cuda:0'), dist b: tensor([20.9454, 16.9006, 19.8914, 25.0275, 18.8734, 18.0517, 16.0129, 23.7110,\n",
            "        28.7361, 22.8315, 23.9416, 17.5531, 22.1851, 18.5423, 21.9458, 18.2131,\n",
            "        17.3170, 13.2010, 14.9480, 20.6192, 22.9023, 15.8369, 20.6942, 22.6133,\n",
            "        19.3231, 27.6264, 21.2869, 22.0955, 18.8902, 20.6592, 19.5053, 17.2977,\n",
            "        17.1411, 20.5281, 17.7894, 22.9507, 23.3254, 19.6303, 18.8467, 26.7605,\n",
            "        19.1112, 28.2450, 21.2799, 19.8414, 20.5548, 21.3527, 15.5104, 19.3723,\n",
            "        21.3951, 17.9437, 20.1670, 18.5007, 19.9613, 23.7620, 22.2052, 24.7519,\n",
            "        23.1892, 18.8885, 19.1393, 15.6155, 26.8652, 17.8082, 22.1763, 20.6441],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.765625 \n",
            "tensor(0.7656)\n",
            "mini Batch Loss: 2.625983715057373\n",
            "mini Batch Loss: 4.01753044128418\n",
            "mini Batch Loss: 3.5207836627960205\n",
            "mini Batch Loss: 3.1383872032165527\n",
            "mini Batch Loss: 3.4565789699554443\n",
            "mini Batch Loss: 3.1148359775543213\n",
            "mini Batch Loss: 2.8784375190734863\n",
            "mini Batch Loss: 3.418483257293701\n",
            "mini Batch Loss: 3.876476287841797\n",
            "mini Batch Loss: 3.3322553634643555\n",
            "mini Batch Loss: 4.297336101531982\n",
            "mini Batch Loss: 3.114668607711792\n",
            "mini Batch Loss: 3.2124338150024414\n",
            "mini Batch Loss: 3.761946678161621\n",
            "mini Batch Loss: 2.7972917556762695\n",
            "mini Batch Loss: 3.3595986366271973\n",
            "mini Batch Loss: 2.992386817932129\n",
            "mini Batch Loss: 3.033517837524414\n",
            "mini Batch Loss: 3.7901132106781006\n",
            "mini Batch Loss: 3.180316686630249\n",
            "mini Batch Loss: 3.145068645477295\n",
            "mini Batch Loss: 3.447606325149536\n",
            "mini Batch Loss: 2.8618340492248535\n",
            "mini Batch Loss: 3.8106579780578613\n",
            "mini Batch Loss: 2.8741555213928223\n",
            "mini Batch Loss: 3.1706109046936035\n",
            "mini Batch Loss: 2.9441118240356445\n",
            "mini Batch Loss: 2.691606044769287\n",
            "mini Batch Loss: 3.9851646423339844\n",
            "mini Batch Loss: 3.513777256011963\n",
            "Training Batch: 151 | Training Loss: 3.513777256011963\n",
            "Training Batch: 151 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_151.pt\n",
            "dist a: tensor([23.7978, 17.2327, 17.8842, 18.2553, 20.3375, 19.5214, 22.2096, 16.3328,\n",
            "        18.7348, 14.6375, 17.7006, 17.3142, 28.0441, 18.2970, 18.3132, 18.9624,\n",
            "        15.8655, 19.0327, 21.8238, 12.4713, 16.0775, 20.0701, 13.7289, 17.2411,\n",
            "        14.9438, 22.0879, 18.8029, 18.8171, 26.5692, 24.4528, 20.2472, 15.7049,\n",
            "         7.6326, 17.3874, 19.6807, 21.2088, 13.4452, 19.9329, 20.1907, 24.4073,\n",
            "        19.1680, 15.8227, 17.2024, 20.1102, 19.1265, 17.3401, 21.2207, 20.6885,\n",
            "        15.2191, 17.0148, 16.5837, 14.7380, 15.7374, 22.3627, 15.1789, 16.3879,\n",
            "        14.6741, 14.8741, 21.4943, 11.9643, 16.5280, 12.9788, 21.8660, 20.4683],\n",
            "       device='cuda:0'), dist b: tensor([23.7447, 20.7063, 26.2148, 14.6936, 18.9561, 19.7519, 22.3908, 20.0910,\n",
            "        24.6319, 17.1264, 19.8451, 20.1894, 16.3345, 23.1731, 21.3129, 13.2687,\n",
            "        16.3291, 14.5764, 19.5922, 13.9242, 15.0381, 17.3490, 24.8336, 16.4897,\n",
            "        24.0891, 23.2810, 22.3407, 18.0068, 23.3764, 17.2175, 25.3227, 21.7210,\n",
            "        29.8544, 20.3986, 18.4749, 24.7068, 13.6013, 22.8488, 19.7166, 24.6854,\n",
            "        18.4360, 23.7528, 30.5664, 21.1296, 24.9430, 17.0249, 11.8469, 21.7495,\n",
            "        21.7928, 19.7319, 28.2613, 16.6102, 14.5572, 13.6595, 14.4820, 17.5107,\n",
            "        20.1433, 14.2901, 22.0380, 22.9628, 20.6175, 16.1094, 17.0297, 26.7985],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.640625 \n",
            "tensor(0.6406)\n",
            "mini Batch Loss: 3.47182035446167\n",
            "mini Batch Loss: 2.937873125076294\n",
            "mini Batch Loss: 3.592459201812744\n",
            "mini Batch Loss: 3.2906012535095215\n",
            "mini Batch Loss: 3.3916730880737305\n",
            "mini Batch Loss: 2.168318748474121\n",
            "mini Batch Loss: 3.1859774589538574\n",
            "mini Batch Loss: 3.7474045753479004\n",
            "mini Batch Loss: 2.755019187927246\n",
            "mini Batch Loss: 3.6777725219726562\n",
            "mini Batch Loss: 3.492318868637085\n",
            "mini Batch Loss: 3.0559725761413574\n",
            "mini Batch Loss: 3.423722505569458\n",
            "mini Batch Loss: 3.8384015560150146\n",
            "mini Batch Loss: 3.3114662170410156\n",
            "mini Batch Loss: 3.067124605178833\n",
            "mini Batch Loss: 3.370877742767334\n",
            "mini Batch Loss: 4.258808612823486\n",
            "mini Batch Loss: 2.7674129009246826\n",
            "mini Batch Loss: 3.075369119644165\n",
            "mini Batch Loss: 3.5406806468963623\n",
            "mini Batch Loss: 3.3348052501678467\n",
            "mini Batch Loss: 3.598752975463867\n",
            "mini Batch Loss: 3.1346256732940674\n",
            "mini Batch Loss: 3.194840908050537\n",
            "mini Batch Loss: 2.837193250656128\n",
            "mini Batch Loss: 3.4069998264312744\n",
            "mini Batch Loss: 2.791992664337158\n",
            "mini Batch Loss: 2.9899044036865234\n",
            "mini Batch Loss: 2.7976629734039307\n",
            "Training Batch: 181 | Training Loss: 2.7976629734039307\n",
            "Training Batch: 181 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_181.pt\n",
            "dist a: tensor([27.6470, 23.2243, 16.8865, 17.5215, 16.4814, 16.8906, 18.7964, 12.4866,\n",
            "        21.2443, 25.3062, 21.2111, 17.7248, 18.1626, 16.7679, 21.6160, 22.0554,\n",
            "        17.2482, 12.0855, 15.2848, 20.0202, 18.3902, 12.7741, 16.5762, 23.2582,\n",
            "        10.8829, 13.4950, 16.5611, 16.6578, 17.2529, 11.7791, 17.6095, 13.2676,\n",
            "        22.6311, 14.2790, 11.9825, 16.6122, 18.5348, 17.0823, 23.2858, 16.9350,\n",
            "        19.9825, 18.6053, 11.9828, 16.0099, 18.4059, 17.4390, 15.1002, 24.2730,\n",
            "        11.6857, 20.7670, 12.1775, 18.8702, 17.0731, 31.5853, 16.6577, 21.9096,\n",
            "        22.9865, 20.8347, 13.3404, 24.2265, 14.0543, 21.2174, 23.8708, 22.8979],\n",
            "       device='cuda:0'), dist b: tensor([30.1320, 28.3129, 18.7299, 20.8002, 20.2042, 15.9823, 16.3123, 19.0705,\n",
            "        13.8690, 23.5155, 16.2976, 24.5371, 26.2991, 26.0979, 16.9055, 20.1471,\n",
            "        14.9425, 16.6663, 15.2743, 24.6047, 14.2247, 27.0578, 22.5623, 27.6699,\n",
            "        22.4062, 23.4653, 21.8358, 19.9562, 23.3641, 23.8750, 19.6669, 23.3206,\n",
            "        30.7125, 17.8561, 17.7780, 25.3048, 28.7338, 17.5179, 17.8180, 15.0145,\n",
            "        19.9683, 19.4770, 22.8226, 20.1009, 28.2380, 22.7283, 18.0577, 27.0290,\n",
            "        24.7988, 19.0476, 15.4562, 16.4821, 16.9269, 28.5380, 16.0467, 30.3552,\n",
            "        27.0501, 27.1272, 16.5770, 16.5468, 15.5796, 20.3191, 16.8300, 20.2719],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.65625 \n",
            "tensor(0.6562)\n",
            "mini Batch Loss: 3.2684988975524902\n",
            "mini Batch Loss: 4.147448539733887\n",
            "mini Batch Loss: 2.635190963745117\n",
            "mini Batch Loss: 3.3820595741271973\n",
            "mini Batch Loss: 3.3472821712493896\n",
            "mini Batch Loss: 3.3999204635620117\n",
            "mini Batch Loss: 3.16408634185791\n",
            "mini Batch Loss: 3.488318920135498\n",
            "mini Batch Loss: 4.070218563079834\n",
            "mini Batch Loss: 2.6333839893341064\n",
            "mini Batch Loss: 3.490337610244751\n",
            "mini Batch Loss: 3.3498947620391846\n",
            "mini Batch Loss: 3.4044270515441895\n",
            "mini Batch Loss: 3.8394041061401367\n",
            "mini Batch Loss: 3.3470962047576904\n",
            "mini Batch Loss: 2.66202449798584\n",
            "mini Batch Loss: 2.651671886444092\n",
            "mini Batch Loss: 4.302685737609863\n",
            "mini Batch Loss: 2.8744726181030273\n",
            "mini Batch Loss: 2.309248685836792\n",
            "mini Batch Loss: 2.668116807937622\n",
            "mini Batch Loss: 3.9567649364471436\n",
            "mini Batch Loss: 3.3099470138549805\n",
            "mini Batch Loss: 2.6276040077209473\n",
            "mini Batch Loss: 3.6604630947113037\n",
            "mini Batch Loss: 3.200237274169922\n",
            "mini Batch Loss: 3.692340850830078\n",
            "mini Batch Loss: 4.10975980758667\n",
            "mini Batch Loss: 2.776733636856079\n",
            "mini Batch Loss: 3.5511155128479004\n",
            "Training Batch: 211 | Training Loss: 3.5511155128479004\n",
            "Training Batch: 211 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_211.pt\n",
            "dist a: tensor([17.1961, 17.6753, 16.4186, 14.3204, 15.5943, 13.2779, 16.3964, 15.0966,\n",
            "        15.3592, 19.2282, 12.5329, 18.5020, 17.0448, 16.4407, 16.4745, 20.5411,\n",
            "        21.7835, 19.5208, 17.4167, 14.9861, 22.0482, 23.6433, 16.4984, 22.0698,\n",
            "        24.2570, 18.2760, 19.2876, 16.4317, 15.2554, 22.9987, 19.1156, 18.2152,\n",
            "        19.2686, 23.9994, 17.7414, 24.1590, 22.8187, 14.2700, 24.4384, 18.3138,\n",
            "        16.3887, 16.2265, 15.0226, 16.2324, 17.0747, 17.9012, 15.4053, 13.4304,\n",
            "        20.6175, 18.2850, 15.0304, 14.7157, 17.0885, 17.0027, 16.7819, 19.1524,\n",
            "        19.2971, 14.4669, 19.3081, 17.3201, 16.6585, 17.4079, 14.5966, 18.9356],\n",
            "       device='cuda:0'), dist b: tensor([20.7905, 21.1904, 16.4086, 23.2384, 20.4228, 22.4324, 18.4226, 16.2629,\n",
            "        16.1870, 26.2983, 22.8914, 19.9808, 26.1283, 25.6346, 15.6298, 23.7292,\n",
            "        17.3720, 26.5303, 17.3160, 26.1908, 17.7873, 21.4113, 18.8524, 19.6788,\n",
            "        17.6398, 15.8112, 23.5936, 17.7276, 18.4294, 22.0950, 16.1929, 31.7000,\n",
            "        22.5203, 22.4202, 18.5660, 20.7422, 18.7540, 22.0882, 16.1636, 23.4249,\n",
            "        19.7133, 17.6937, 18.9160, 20.1004, 22.1753, 22.0622, 28.5552, 21.7025,\n",
            "        24.5745, 20.8156, 15.6294, 21.8327, 17.3940, 18.9385, 14.4470, 15.9818,\n",
            "        30.0223, 31.0999, 14.9260, 18.4357, 17.0073, 22.0019, 18.1120, 23.7012],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.71875 \n",
            "tensor(0.7188)\n",
            "mini Batch Loss: 3.3209359645843506\n",
            "mini Batch Loss: 3.380535840988159\n",
            "mini Batch Loss: 3.006814479827881\n",
            "mini Batch Loss: 3.0788497924804688\n",
            "mini Batch Loss: 3.1392030715942383\n",
            "mini Batch Loss: 3.2028608322143555\n",
            "mini Batch Loss: 3.368854522705078\n",
            "mini Batch Loss: 2.743793487548828\n",
            "mini Batch Loss: 3.523308277130127\n",
            "mini Batch Loss: 3.1985764503479004\n",
            "mini Batch Loss: 3.5542373657226562\n",
            "mini Batch Loss: 3.80907940864563\n",
            "mini Batch Loss: 3.0283193588256836\n",
            "mini Batch Loss: 2.60142183303833\n",
            "mini Batch Loss: 3.2948336601257324\n",
            "mini Batch Loss: 3.7766733169555664\n",
            "mini Batch Loss: 2.926555633544922\n",
            "mini Batch Loss: 3.3422460556030273\n",
            "mini Batch Loss: 3.717318058013916\n",
            "mini Batch Loss: 3.603302001953125\n",
            "mini Batch Loss: 3.513967990875244\n",
            "mini Batch Loss: 2.577444076538086\n",
            "mini Batch Loss: 4.209197998046875\n",
            "mini Batch Loss: 3.2240939140319824\n",
            "mini Batch Loss: 3.2354249954223633\n",
            "mini Batch Loss: 3.218508243560791\n",
            "mini Batch Loss: 4.055874824523926\n",
            "mini Batch Loss: 3.4219133853912354\n",
            "mini Batch Loss: 3.3502511978149414\n",
            "mini Batch Loss: 2.7747654914855957\n",
            "Training Batch: 241 | Training Loss: 2.7747654914855957\n",
            "Training Batch: 241 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_241.pt\n",
            "dist a: tensor([24.5119, 14.8356, 20.8461, 18.7005, 25.2794, 18.7911, 15.0827, 14.1875,\n",
            "        14.7034, 16.9322, 15.6791, 19.8850, 21.6414, 17.5060, 17.5472, 19.9233,\n",
            "        20.5496, 18.5765, 18.1851, 17.8318, 15.0846, 22.2131, 13.6334, 15.2816,\n",
            "        13.9802, 20.2294, 14.5500, 14.3442, 18.1955, 14.6554, 19.5189, 12.5459,\n",
            "        15.7577, 17.6716, 19.0329, 17.1912, 24.5857, 23.1113, 20.9044, 19.7597,\n",
            "        15.5489, 19.4252, 17.5952, 12.3290, 20.1777, 11.6289, 20.1748, 12.8770,\n",
            "        16.4113, 14.9734, 18.5694, 20.8942, 16.9724, 17.0058, 25.2778, 19.3379,\n",
            "        16.7126, 16.1286, 17.7072, 16.0608, 14.9909, 17.9282, 17.7593, 16.0130],\n",
            "       device='cuda:0'), dist b: tensor([23.7489, 17.6057, 23.1477, 20.6457, 22.6999, 26.4960, 24.5134, 19.2553,\n",
            "        20.9696, 16.6137, 18.8499, 16.5208, 25.3766, 27.5092, 25.3837, 20.9195,\n",
            "        24.3717, 13.3359, 17.7380, 13.6514, 22.1015, 15.2356, 20.7486, 22.3511,\n",
            "        14.7091, 24.8278, 17.7884, 21.6392, 23.1342, 19.5379, 13.6168, 28.0159,\n",
            "        23.4416, 19.3976, 24.0321, 16.8129, 21.7363, 18.7841, 30.4827, 16.2380,\n",
            "        23.9665, 16.3335, 15.7550, 25.3855, 31.2986, 20.3848, 26.0534, 21.0393,\n",
            "        20.1275, 14.2980, 18.9430, 22.0539, 16.7472, 27.0412, 19.2065, 20.6853,\n",
            "        22.6466, 27.1015, 16.5620, 21.3935, 21.3043, 21.0455, 18.4511, 23.3120],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.703125 \n",
            "tensor(0.7031)\n",
            "mini Batch Loss: 2.546863555908203\n",
            "mini Batch Loss: 2.645625114440918\n",
            "mini Batch Loss: 2.6971170902252197\n",
            "mini Batch Loss: 3.2663915157318115\n",
            "mini Batch Loss: 2.819340229034424\n",
            "mini Batch Loss: 3.0887818336486816\n",
            "mini Batch Loss: 2.6813735961914062\n",
            "mini Batch Loss: 2.8772168159484863\n",
            "mini Batch Loss: 3.5963294506073\n",
            "mini Batch Loss: 3.0192666053771973\n",
            "mini Batch Loss: 2.923142910003662\n",
            "mini Batch Loss: 3.2639825344085693\n",
            "mini Batch Loss: 2.6109843254089355\n",
            "mini Batch Loss: 3.2727177143096924\n",
            "mini Batch Loss: 3.319460391998291\n",
            "mini Batch Loss: 3.318230152130127\n",
            "mini Batch Loss: 3.5134963989257812\n",
            "mini Batch Loss: 2.5115065574645996\n",
            "mini Batch Loss: 3.7412495613098145\n",
            "mini Batch Loss: 2.976841688156128\n",
            "mini Batch Loss: 3.026165723800659\n",
            "mini Batch Loss: 3.1056337356567383\n",
            "mini Batch Loss: 3.5113682746887207\n",
            "mini Batch Loss: 2.761073112487793\n",
            "mini Batch Loss: 2.860718011856079\n",
            "mini Batch Loss: 2.7504889965057373\n",
            "mini Batch Loss: 4.024462699890137\n",
            "mini Batch Loss: 2.511478900909424\n",
            "mini Batch Loss: 3.1375861167907715\n",
            "mini Batch Loss: 3.172393321990967\n",
            "Training Batch: 271 | Training Loss: 3.172393321990967\n",
            "Training Batch: 271 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_271.pt\n",
            "dist a: tensor([18.7153, 14.1489, 12.9524, 17.1352, 17.6344, 18.1021], device='cuda:0'), dist b: tensor([20.8627, 23.3841, 11.1455, 28.8025, 17.9655, 18.2223], device='cuda:0')\n",
            "random batch accuracy: 0.8333333134651184 \n",
            "tensor(0.8333)\n",
            "mini Batch Loss: 3.0618839263916016\n",
            "mini Batch Loss: 3.481269359588623\n",
            "mini Batch Loss: 3.2356414794921875\n",
            "mini Batch Loss: 3.17092227935791\n",
            "mini Batch Loss: 2.7405898571014404\n",
            "mini Batch Loss: 2.9480433464050293\n",
            "mini Batch Loss: 3.6183791160583496\n",
            "mini Batch Loss: 3.235553741455078\n",
            "mini Batch Loss: 3.330543279647827\n",
            "mini Batch Loss: 2.927913188934326\n",
            "mini Batch Loss: 3.1815133094787598\n",
            "mini Batch Loss: 3.052686929702759\n",
            "mini Batch Loss: 2.9985084533691406\n",
            "mini Batch Loss: 3.03237247467041\n",
            "mini Batch Loss: 3.2438879013061523\n",
            "mini Batch Loss: 2.8293628692626953\n",
            "mini Batch Loss: 3.5900487899780273\n",
            "mini Batch Loss: 3.0324878692626953\n",
            "mini Batch Loss: 3.203235149383545\n",
            "mini Batch Loss: 3.9262008666992188\n",
            "mini Batch Loss: 2.8464128971099854\n",
            "mini Batch Loss: 2.816422462463379\n",
            "mini Batch Loss: 3.3900089263916016\n",
            "mini Batch Loss: 3.072021007537842\n",
            "mini Batch Loss: 2.768442153930664\n",
            "mini Batch Loss: 2.3401570320129395\n",
            "mini Batch Loss: 3.643934488296509\n",
            "mini Batch Loss: 3.2462000846862793\n",
            "mini Batch Loss: 3.777763843536377\n",
            "mini Batch Loss: 3.311979055404663\n",
            "Training Batch: 301 | Training Loss: 3.311979055404663\n",
            "Training Batch: 301 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_301.pt\n",
            "dist a: tensor([14.1359, 15.5098, 14.0955, 14.2530, 19.0538, 18.4391, 16.0175, 12.4456,\n",
            "        18.0639, 19.4880, 16.6504, 18.5567, 20.7881, 17.5329, 15.7965, 21.2658,\n",
            "        19.5799, 13.8059, 16.0376, 17.6971, 19.3685, 14.5346, 12.4322, 17.8106,\n",
            "        15.7349, 19.6636, 12.3261, 17.6422, 15.7700, 14.6194, 14.9571, 14.1642,\n",
            "        14.7736, 12.9984, 22.6006, 16.2720, 12.1398, 14.4150, 15.1741, 15.7539,\n",
            "        19.6845, 17.7338, 18.1843, 15.8384, 22.0113, 13.0867, 21.9856, 23.3639,\n",
            "        14.8534,  9.9022, 24.0545, 15.3754, 24.5034, 13.0357, 18.1285, 13.9189,\n",
            "        15.6656, 20.4887, 20.7322, 13.1908, 15.1188, 18.4380, 18.1638, 22.7793],\n",
            "       device='cuda:0'), dist b: tensor([34.5616, 21.6377, 26.3301, 13.8206, 18.3609, 26.8095, 24.6754, 18.9709,\n",
            "        21.6138, 27.0124, 14.1645, 19.8621, 23.3561, 19.9099, 21.7282, 19.5791,\n",
            "        22.4091, 30.4693, 19.5155, 19.1720, 24.8911, 24.2760, 24.0464, 16.3849,\n",
            "        18.3567, 20.3338, 20.9580, 24.2623, 16.5703, 18.8241, 17.1379, 14.3272,\n",
            "        29.5711, 26.8113, 25.3431, 18.7457, 25.1432, 13.2204, 19.6000, 25.8103,\n",
            "        15.0515, 20.4832, 15.7399, 18.4317, 18.6641, 24.7497, 20.8664, 23.2333,\n",
            "        19.5535, 26.1829, 27.5305, 15.6327, 17.7971, 14.6332, 14.5568, 16.3048,\n",
            "        22.5803, 18.3981, 16.9012, 24.2228, 16.1066, 19.6039, 23.9869, 29.4449],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.765625 \n",
            "tensor(0.7656)\n",
            "mini Batch Loss: 3.1382899284362793\n",
            "mini Batch Loss: 3.0615317821502686\n",
            "mini Batch Loss: 3.450589179992676\n",
            "mini Batch Loss: 2.9373340606689453\n",
            "mini Batch Loss: 3.727377414703369\n",
            "mini Batch Loss: 3.9349966049194336\n",
            "mini Batch Loss: 3.940586566925049\n",
            "mini Batch Loss: 3.19746994972229\n",
            "mini Batch Loss: 3.4423747062683105\n",
            "mini Batch Loss: 3.137788772583008\n",
            "mini Batch Loss: 2.53155255317688\n",
            "mini Batch Loss: 3.5182509422302246\n",
            "mini Batch Loss: 2.3942432403564453\n",
            "mini Batch Loss: 3.639223337173462\n",
            "mini Batch Loss: 3.4235706329345703\n",
            "mini Batch Loss: 2.847215414047241\n",
            "mini Batch Loss: 2.7361104488372803\n",
            "mini Batch Loss: 2.9763684272766113\n",
            "mini Batch Loss: 2.823206663131714\n",
            "mini Batch Loss: 2.8042752742767334\n",
            "mini Batch Loss: 2.8895394802093506\n",
            "mini Batch Loss: 2.6300230026245117\n",
            "mini Batch Loss: 3.4194016456604004\n",
            "mini Batch Loss: 3.1416168212890625\n",
            "mini Batch Loss: 3.007746696472168\n",
            "mini Batch Loss: 3.106443166732788\n",
            "mini Batch Loss: 3.02260160446167\n",
            "mini Batch Loss: 3.0833826065063477\n",
            "mini Batch Loss: 2.8898913860321045\n",
            "mini Batch Loss: 2.9196970462799072\n",
            "Training Batch: 331 | Training Loss: 2.9196970462799072\n",
            "Training Batch: 331 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_331.pt\n",
            "dist a: tensor([14.2465, 14.9728, 14.1170, 13.7504, 16.3003, 18.6545, 16.3396, 12.1202,\n",
            "        18.0075, 19.2663, 14.2673, 18.7391, 22.3975, 16.8599, 15.5523, 20.9895,\n",
            "        22.4070, 14.0892, 16.1043, 18.4834, 21.2035, 14.3210, 12.3834, 17.8819,\n",
            "        15.9984, 19.2816, 11.8338, 18.4958, 15.4608, 14.7519, 13.3823, 13.9957,\n",
            "        15.4580, 14.5349, 23.8106, 17.7924, 11.7925, 15.4744, 16.4793, 16.2871,\n",
            "        20.4896, 17.3808, 18.0392, 16.2081, 21.9133, 12.4825, 21.7747, 21.8272,\n",
            "        16.2545,  9.7774, 25.3452, 16.2437, 24.3084, 13.9951, 19.2678, 13.9282,\n",
            "        16.2528, 20.2936, 20.5448, 14.1281, 16.3146, 19.0138, 18.6576, 20.3237],\n",
            "       device='cuda:0'), dist b: tensor([33.8343, 20.2627, 26.8232, 13.7150, 19.4452, 27.0435, 25.5768, 19.3938,\n",
            "        19.2120, 26.9438, 12.4105, 20.2047, 24.2659, 19.4420, 20.4740, 21.1713,\n",
            "        25.3011, 29.8996, 18.4584, 19.3960, 20.3571, 25.1843, 24.4021, 17.7456,\n",
            "        18.2804, 19.0005, 18.3031, 25.9299, 16.4403, 19.4446, 19.5034, 14.2394,\n",
            "        28.7110, 28.8628, 27.8202, 19.0196, 25.9131, 13.7683, 16.8030, 24.9334,\n",
            "        14.4364, 21.0717, 15.3167, 20.3958, 19.1542, 22.9950, 21.4558, 21.6770,\n",
            "        21.3654, 25.9937, 27.7289, 16.5237, 17.8343, 15.6307, 14.3844, 16.3051,\n",
            "        22.5490, 18.4806, 16.6834, 23.8865, 15.7447, 17.4671, 22.7436, 28.8596],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "mini Batch Loss: 3.1453616619110107\n",
            "mini Batch Loss: 2.5375194549560547\n",
            "mini Batch Loss: 3.3061916828155518\n",
            "mini Batch Loss: 3.4725570678710938\n",
            "mini Batch Loss: 2.9667720794677734\n",
            "mini Batch Loss: 3.101088285446167\n",
            "mini Batch Loss: 3.9449243545532227\n",
            "mini Batch Loss: 3.386054515838623\n",
            "mini Batch Loss: 2.6985106468200684\n",
            "mini Batch Loss: 3.4318485260009766\n",
            "mini Batch Loss: 2.9288582801818848\n",
            "mini Batch Loss: 2.6445107460021973\n",
            "mini Batch Loss: 2.4022440910339355\n",
            "mini Batch Loss: 3.479278326034546\n",
            "mini Batch Loss: 2.93070912361145\n",
            "mini Batch Loss: 2.803941249847412\n",
            "mini Batch Loss: 3.3030037879943848\n",
            "mini Batch Loss: 1.9015353918075562\n",
            "mini Batch Loss: 2.8528482913970947\n",
            "mini Batch Loss: 3.574237585067749\n",
            "mini Batch Loss: 3.517219305038452\n",
            "mini Batch Loss: 3.6681532859802246\n",
            "mini Batch Loss: 2.7983789443969727\n",
            "mini Batch Loss: 3.1498398780822754\n",
            "mini Batch Loss: 3.2032785415649414\n",
            "mini Batch Loss: 3.153993606567383\n",
            "mini Batch Loss: 2.071629047393799\n",
            "mini Batch Loss: 2.9574975967407227\n",
            "mini Batch Loss: 2.61175799369812\n",
            "mini Batch Loss: 2.525261402130127\n",
            "Training Batch: 361 | Training Loss: 2.525261402130127\n",
            "Training Batch: 361 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_361.pt\n",
            "dist a: tensor([14.3440, 15.6598, 14.3227, 14.0764, 15.2922, 17.9816, 16.1094, 11.7600,\n",
            "        17.0363, 19.6198, 14.7344, 18.8908, 21.0320, 15.7899, 15.9413, 20.2540,\n",
            "        21.5341, 13.8558, 15.6270, 17.4041, 22.0488, 14.1245, 11.7986, 17.7818,\n",
            "        16.7462, 22.2898, 11.6570, 17.7775, 15.1934, 13.5617, 12.6124, 14.0959,\n",
            "        16.2166, 13.5323, 26.0131, 18.0737, 12.7073, 14.3736, 15.7410, 15.9561,\n",
            "        19.5484, 17.3277, 17.4266, 15.4047, 22.5355, 14.6993, 21.1094, 23.5563,\n",
            "        17.6856,  9.5811, 25.2643, 17.0064, 25.8445, 14.2322, 21.9382, 13.5440,\n",
            "        16.2327, 20.3881, 19.2883, 14.7318, 15.0629, 22.6337, 17.8547, 21.3201],\n",
            "       device='cuda:0'), dist b: tensor([33.5973, 21.2741, 29.3181, 13.2399, 18.4443, 26.4092, 24.2523, 19.1219,\n",
            "        19.5121, 28.9016, 11.1797, 20.0765, 23.0594, 17.0323, 21.1297, 21.2861,\n",
            "        25.1326, 31.0480, 17.6900, 19.2901, 20.4950, 26.3407, 24.3138, 15.5569,\n",
            "        17.6609, 20.3010, 19.3362, 24.6908, 16.8032, 17.8769, 19.5369, 14.4247,\n",
            "        29.3399, 27.5033, 30.6185, 20.3929, 27.9847, 12.4876, 16.9530, 24.4374,\n",
            "        14.6693, 21.3119, 15.0376, 22.2276, 18.9883, 24.4906, 22.5821, 22.3588,\n",
            "        21.1290, 26.9628, 30.0579, 16.8225, 18.7496, 15.5295, 14.0657, 16.6185,\n",
            "        23.2124, 18.4411, 15.6549, 24.1421, 15.1145, 12.6820, 22.3510, 29.9108],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "mini Batch Loss: 3.104039192199707\n",
            "mini Batch Loss: 3.3186492919921875\n",
            "mini Batch Loss: 2.603203296661377\n",
            "mini Batch Loss: 2.904191493988037\n",
            "mini Batch Loss: 2.119506597518921\n",
            "mini Batch Loss: 2.9988527297973633\n",
            "mini Batch Loss: 2.639403820037842\n",
            "mini Batch Loss: 3.0598883628845215\n",
            "mini Batch Loss: 2.264763832092285\n",
            "mini Batch Loss: 2.970510721206665\n",
            "mini Batch Loss: 2.9138224124908447\n",
            "mini Batch Loss: 2.688173294067383\n",
            "mini Batch Loss: 2.806551456451416\n",
            "mini Batch Loss: 3.4022326469421387\n",
            "mini Batch Loss: 2.706251621246338\n",
            "mini Batch Loss: 3.308694839477539\n",
            "mini Batch Loss: 2.7385683059692383\n",
            "mini Batch Loss: 2.1682777404785156\n",
            "mini Batch Loss: 3.113227367401123\n",
            "mini Batch Loss: 3.54001784324646\n",
            "mini Batch Loss: 3.3993749618530273\n",
            "mini Batch Loss: 3.616218090057373\n",
            "mini Batch Loss: 3.3195958137512207\n",
            "mini Batch Loss: 3.2062325477600098\n",
            "mini Batch Loss: 2.686432361602783\n",
            "mini Batch Loss: 2.7042369842529297\n",
            "mini Batch Loss: 3.510803699493408\n",
            "mini Batch Loss: 2.470081090927124\n",
            "mini Batch Loss: 3.4066433906555176\n",
            "mini Batch Loss: 3.1432833671569824\n",
            "Training Batch: 391 | Training Loss: 3.1432833671569824\n",
            "Training Batch: 391 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_391.pt\n",
            "dist a: tensor([14.2112, 14.6279, 14.5035, 14.6196, 15.5707, 16.7532, 16.7922, 12.2881,\n",
            "        17.5077, 18.6389, 14.0834, 19.9254, 21.7278, 15.8259, 14.9822, 20.3107,\n",
            "        21.1906, 14.1731, 15.5955, 17.5360, 22.1220, 14.9351, 11.4242, 18.3309,\n",
            "        17.2914, 23.7042, 11.9779, 18.8491, 16.3812, 13.3966, 11.8839, 14.6409,\n",
            "        17.0396, 13.8683, 25.7623, 16.9012, 13.4935, 13.8437, 15.9794, 16.3854,\n",
            "        19.7624, 16.6601, 15.5943, 14.8645, 22.9749, 13.6553, 21.3424, 22.8967,\n",
            "        17.8076,  9.5287, 26.3978, 18.0130, 23.8919, 14.0249, 21.9618, 12.7833,\n",
            "        15.6204, 22.4283, 18.8411, 13.7112, 15.5617, 22.4155, 17.7716, 23.4267],\n",
            "       device='cuda:0'), dist b: tensor([35.0398, 19.9900, 28.7566, 13.4415, 21.6444, 26.6514, 25.1029, 18.7803,\n",
            "        19.2161, 26.3017, 11.8138, 22.3395, 22.9669, 17.6988, 20.4144, 21.4173,\n",
            "        23.3700, 32.1376, 16.5691, 18.8750, 20.9949, 24.4083, 25.1248, 17.1994,\n",
            "        18.3215, 23.2188, 19.8765, 25.1996, 16.0565, 17.7101, 18.0690, 13.5848,\n",
            "        29.3361, 27.5884, 28.7312, 20.8774, 28.1922, 12.3400, 17.3122, 23.9977,\n",
            "        14.5938, 22.4647, 13.0949, 22.0800, 18.3456, 26.7126, 21.4466, 23.2521,\n",
            "        19.9837, 27.3656, 29.8535, 17.8633, 18.0634, 15.2824, 14.1752, 18.3858,\n",
            "        24.1967, 17.6295, 15.4571, 27.5946, 15.7798, 13.2182, 22.6331, 29.3870],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "mini Batch Loss: 3.1495261192321777\n",
            "mini Batch Loss: 3.2673141956329346\n",
            "mini Batch Loss: 3.3133864402770996\n",
            "mini Batch Loss: 2.5488853454589844\n",
            "mini Batch Loss: 2.8356685638427734\n",
            "mini Batch Loss: 3.2863070964813232\n",
            "mini Batch Loss: 3.8727312088012695\n",
            "mini Batch Loss: 2.728686809539795\n",
            "mini Batch Loss: 3.528639316558838\n",
            "mini Batch Loss: 2.423462152481079\n",
            "mini Batch Loss: 3.3847498893737793\n",
            "mini Batch Loss: 2.4684624671936035\n",
            "mini Batch Loss: 3.7010231018066406\n",
            "mini Batch Loss: 4.0012102127075195\n",
            "mini Batch Loss: 2.5349292755126953\n",
            "mini Batch Loss: 3.079815626144409\n",
            "mini Batch Loss: 3.2131285667419434\n",
            "mini Batch Loss: 2.640162944793701\n",
            "mini Batch Loss: 2.465827465057373\n",
            "mini Batch Loss: 3.038773536682129\n",
            "mini Batch Loss: 2.9600982666015625\n",
            "mini Batch Loss: 3.2504239082336426\n",
            "mini Batch Loss: 3.409318208694458\n",
            "mini Batch Loss: 3.025509834289551\n",
            "mini Batch Loss: 3.0686559677124023\n",
            "mini Batch Loss: 3.6506075859069824\n",
            "mini Batch Loss: 2.144228458404541\n",
            "mini Batch Loss: 2.698592185974121\n",
            "mini Batch Loss: 3.3685665130615234\n",
            "mini Batch Loss: 2.869741678237915\n",
            "Training Batch: 421 | Training Loss: 2.869741678237915\n",
            "Training Batch: 421 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_421.pt\n",
            "dist a: tensor([14.2604, 15.2300, 14.3678, 14.5383, 15.6626, 16.9001, 15.9368, 12.8157,\n",
            "        18.0148, 18.7168, 12.8343, 19.6394, 20.9320, 15.4910, 15.3295, 19.8291,\n",
            "        23.0858, 14.0479, 16.6286, 17.3696, 20.3242, 14.0634, 11.9757, 18.3116,\n",
            "        16.3011, 21.4088, 12.7236, 18.0885, 16.1853, 13.1812, 13.0867, 14.3020,\n",
            "        16.2872, 12.9367, 24.4036, 15.5218, 13.1259, 13.0891, 15.1264, 16.9988,\n",
            "        19.6662, 17.5790, 16.2862, 14.8866, 24.8396, 13.6798, 20.2109, 22.9625,\n",
            "        16.9907,  9.6483, 23.9787, 15.6671, 24.0441, 14.3227, 21.9646, 13.2653,\n",
            "        15.5523, 22.3138, 19.3924, 13.4778, 15.0087, 23.6565, 17.3247, 23.4081],\n",
            "       device='cuda:0'), dist b: tensor([36.1601, 19.5651, 27.3992, 13.6436, 19.3694, 29.2305, 23.6240, 18.8779,\n",
            "        20.0237, 28.0559, 12.1855, 21.8916, 23.5601, 18.1362, 20.8868, 20.3865,\n",
            "        26.2839, 32.5607, 18.9982, 18.7963, 22.3776, 26.4919, 23.6078, 16.8572,\n",
            "        17.9643, 22.7610, 19.1434, 24.4250, 15.4270, 17.5845, 17.8189, 13.7550,\n",
            "        29.8543, 26.7458, 27.9819, 20.8576, 25.9174, 11.7354, 17.9651, 24.5811,\n",
            "        14.0340, 20.6547, 13.2814, 19.2477, 19.6907, 26.4377, 23.8627, 22.5997,\n",
            "        22.6453, 27.8035, 30.0313, 16.0134, 18.9896, 14.3834, 14.1913, 17.1667,\n",
            "        22.1869, 17.5784, 15.2904, 25.0340, 15.6736, 14.4535, 23.7999, 29.3507],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.765625 \n",
            "tensor(0.7656)\n",
            "mini Batch Loss: 3.3838112354278564\n",
            "mini Batch Loss: 3.179196834564209\n",
            "mini Batch Loss: 2.4537296295166016\n",
            "mini Batch Loss: 2.9385275840759277\n",
            "mini Batch Loss: 3.2822484970092773\n",
            "mini Batch Loss: 3.586188554763794\n",
            "mini Batch Loss: 2.488008499145508\n",
            "mini Batch Loss: 3.596057415008545\n",
            "mini Batch Loss: 2.563737630844116\n",
            "mini Batch Loss: 2.8933815956115723\n",
            "mini Batch Loss: 2.8851211071014404\n",
            "mini Batch Loss: 2.6205830574035645\n",
            "mini Batch Loss: 2.614218235015869\n",
            "mini Batch Loss: 3.4746198654174805\n",
            "mini Batch Loss: 3.1066274642944336\n",
            "mini Batch Loss: 2.6895203590393066\n",
            "mini Batch Loss: 3.0225539207458496\n",
            "mini Batch Loss: 2.666713237762451\n",
            "mini Batch Loss: 2.916044235229492\n",
            "mini Batch Loss: 2.8810253143310547\n",
            "mini Batch Loss: 2.833482503890991\n",
            "mini Batch Loss: 2.982783794403076\n",
            "mini Batch Loss: 3.5710883140563965\n",
            "mini Batch Loss: 3.4488797187805176\n",
            "mini Batch Loss: 3.4957215785980225\n",
            "mini Batch Loss: 2.723662853240967\n",
            "mini Batch Loss: 2.0732364654541016\n",
            "mini Batch Loss: 2.463441848754883\n",
            "mini Batch Loss: 2.3724517822265625\n",
            "mini Batch Loss: 3.338932991027832\n",
            "Training Batch: 451 | Training Loss: 3.338932991027832\n",
            "Training Batch: 451 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_451.pt\n",
            "dist a: tensor([13.6105, 15.2200, 15.4413, 14.4123, 16.3778, 18.4192, 16.3995, 12.8473,\n",
            "        17.2342, 18.1934, 14.0295, 18.2499, 23.4169, 15.9827, 15.0173, 18.7830,\n",
            "        29.3946, 14.3787, 16.5635, 16.8741, 19.7613, 13.0430, 11.4371, 19.9326,\n",
            "        15.9790, 21.9595, 13.1291, 17.7375, 16.5061, 13.9734, 10.7072, 13.7680,\n",
            "        16.7949, 14.4592, 23.1854, 15.0266, 12.0621, 12.6022, 15.4772, 17.2612,\n",
            "        19.2410, 16.3472, 15.6307, 14.6734, 26.7115, 11.8378, 20.5338, 22.0136,\n",
            "        16.7652, 10.4592, 24.5612, 16.0230, 24.0725, 14.0306, 23.4657, 13.0659,\n",
            "        15.4374, 23.6981, 20.3786, 15.8177, 14.6429, 22.6047, 18.6505, 22.1394],\n",
            "       device='cuda:0'), dist b: tensor([32.9203, 21.0965, 26.6119, 13.5364, 19.4941, 27.8968, 22.3299, 19.8301,\n",
            "        21.3320, 28.5702, 13.1804, 19.9962, 27.0500, 19.7154, 20.4989, 20.3862,\n",
            "        31.3867, 31.7338, 19.8461, 21.0981, 21.7005, 23.9654, 23.2533, 17.0348,\n",
            "        16.0745, 21.6127, 19.0735, 24.2545, 17.1705, 19.3340, 22.8098, 14.1310,\n",
            "        29.2314, 29.5261, 29.2842, 19.8567, 23.7305, 11.7915, 18.7706, 23.0461,\n",
            "        13.8072, 18.5266, 14.2671, 20.1405, 20.9784, 25.5029, 23.7209, 20.8998,\n",
            "        23.8720, 27.7706, 29.7262, 16.3416, 18.4146, 15.8939, 13.9462, 18.5947,\n",
            "        22.3868, 17.2392, 16.0463, 23.4681, 17.9522, 15.8519, 23.7161, 26.8300],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.78125 \n",
            "tensor(0.7812)\n",
            "mini Batch Loss: 2.7809994220733643\n",
            "mini Batch Loss: 2.730355739593506\n",
            "mini Batch Loss: 2.505605697631836\n",
            "mini Batch Loss: 3.6244640350341797\n",
            "mini Batch Loss: 3.24584698677063\n",
            "mini Batch Loss: 3.2385435104370117\n",
            "mini Batch Loss: 3.2501864433288574\n",
            "mini Batch Loss: 3.1704301834106445\n",
            "mini Batch Loss: 2.8674118518829346\n",
            "mini Batch Loss: 3.3276376724243164\n",
            "mini Batch Loss: 2.8152754306793213\n",
            "mini Batch Loss: 2.8952221870422363\n",
            "mini Batch Loss: 2.8106894493103027\n",
            "mini Batch Loss: 2.773606777191162\n",
            "mini Batch Loss: 2.9742820262908936\n",
            "mini Batch Loss: 2.5793521404266357\n",
            "mini Batch Loss: 2.566192626953125\n",
            "mini Batch Loss: 3.131136655807495\n",
            "mini Batch Loss: 2.66422700881958\n",
            "mini Batch Loss: 2.708142042160034\n",
            "mini Batch Loss: 2.959012508392334\n",
            "mini Batch Loss: 3.0080819129943848\n",
            "mini Batch Loss: 2.817002773284912\n",
            "mini Batch Loss: 2.437187433242798\n",
            "mini Batch Loss: 2.8914566040039062\n",
            "mini Batch Loss: 3.14384388923645\n",
            "mini Batch Loss: 3.4112606048583984\n",
            "mini Batch Loss: 3.4517107009887695\n",
            "mini Batch Loss: 3.0579442977905273\n",
            "mini Batch Loss: 3.423995018005371\n",
            "Training Batch: 481 | Training Loss: 3.423995018005371\n",
            "Training Batch: 481 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_481.pt\n",
            "dist a: tensor([14.2488, 14.1514, 17.2118, 14.0126, 18.3585, 19.7723, 18.2625, 13.0938,\n",
            "        18.0105, 17.4225, 16.6197, 17.3543, 21.1385, 15.8385, 14.6747, 19.1092,\n",
            "        27.7083, 14.5314, 18.2937, 17.3637, 18.6910, 12.7914, 11.7158, 19.2680,\n",
            "        16.3244, 23.8534, 13.9233, 19.3683, 15.9290, 13.7358, 10.6271, 14.0099,\n",
            "        16.9491, 13.7118, 24.4812, 14.7160, 11.7515, 14.2407, 15.9622, 17.2146,\n",
            "        20.1469, 17.8690, 15.5270, 14.6800, 26.6569, 13.0269, 20.9381, 21.1833,\n",
            "        16.9461,  9.7643, 26.4186, 17.4518, 26.6769, 13.9607, 20.0740, 14.3872,\n",
            "        15.8506, 21.4273, 20.9750, 13.4177, 15.1852, 17.7993, 19.2326, 23.1140],\n",
            "       device='cuda:0'), dist b: tensor([34.1479, 20.0998, 25.7712, 13.4118, 18.7677, 26.2708, 23.5102, 19.3126,\n",
            "        22.4205, 26.2252, 15.2331, 20.0326, 24.8379, 18.9531, 21.6196, 18.5778,\n",
            "        30.1655, 30.6215, 21.7424, 20.9706, 23.5689, 26.9412, 22.8776, 19.7990,\n",
            "        17.6363, 23.9835, 18.5327, 25.7847, 15.1821, 18.9303, 22.3159, 14.0320,\n",
            "        30.5730, 28.8131, 29.6136, 20.2607, 25.3685, 13.1815, 19.2505, 25.0035,\n",
            "        13.4073, 20.8501, 13.5145, 19.6023, 20.7280, 26.9364, 22.3120, 22.6211,\n",
            "        25.3201, 28.6835, 30.1254, 16.5354, 18.3931, 14.3165, 14.5758, 17.9239,\n",
            "        24.0499, 16.5103, 15.6739, 26.4056, 19.9517, 19.1466, 25.5691, 27.7102],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "mini Batch Loss: 3.0184760093688965\n",
            "mini Batch Loss: 2.2973814010620117\n",
            "mini Batch Loss: 3.4891271591186523\n",
            "mini Batch Loss: 2.5666730403900146\n",
            "mini Batch Loss: 2.3152687549591064\n",
            "mini Batch Loss: 3.5661211013793945\n",
            "mini Batch Loss: 3.110945701599121\n",
            "mini Batch Loss: 3.4539060592651367\n",
            "mini Batch Loss: 2.461233615875244\n",
            "mini Batch Loss: 3.6838765144348145\n",
            "mini Batch Loss: 2.9319405555725098\n",
            "mini Batch Loss: 2.586536169052124\n",
            "mini Batch Loss: 2.815251111984253\n",
            "mini Batch Loss: 2.222046136856079\n",
            "mini Batch Loss: 3.1689412593841553\n",
            "mini Batch Loss: 2.2643418312072754\n",
            "mini Batch Loss: 2.6488044261932373\n",
            "mini Batch Loss: 3.7709622383117676\n",
            "mini Batch Loss: 2.418870449066162\n",
            "mini Batch Loss: 2.800740957260132\n",
            "mini Batch Loss: 3.0522546768188477\n",
            "mini Batch Loss: 1.926668405532837\n",
            "mini Batch Loss: 2.4702532291412354\n",
            "mini Batch Loss: 2.6170437335968018\n",
            "mini Batch Loss: 3.438488006591797\n",
            "mini Batch Loss: 2.8664932250976562\n",
            "mini Batch Loss: 3.0590243339538574\n",
            "mini Batch Loss: 2.29440975189209\n",
            "mini Batch Loss: 3.5828628540039062\n",
            "mini Batch Loss: 2.8391177654266357\n",
            "Training Batch: 511 | Training Loss: 2.8391177654266357\n",
            "Training Batch: 511 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_511.pt\n",
            "dist a: tensor([14.2076, 14.6872, 18.3289, 14.9716, 19.1843, 17.5709, 19.1951, 12.8197,\n",
            "        17.6827, 18.8554, 16.1363, 18.3288, 20.7758, 16.8179, 14.9801, 20.0736,\n",
            "        21.8722, 12.7313, 17.1319, 18.3201, 20.3252, 13.2665, 11.9464, 18.7598,\n",
            "        15.7842, 24.8575, 13.5432, 20.7177, 17.8861, 13.6699, 11.6006, 14.2416,\n",
            "        15.6588, 15.4941, 22.8822, 14.2997, 12.8149, 13.8219, 16.1462, 17.2517,\n",
            "        18.6085, 19.5270, 16.9874, 16.0358, 24.9294, 13.3654, 20.4087, 24.7115,\n",
            "        16.6381,  9.8661, 26.8145, 16.7884, 27.5332, 13.6548, 22.1995, 13.1838,\n",
            "        16.0867, 22.5564, 20.7314, 13.3413, 14.6396, 18.8611, 21.0459, 25.0827],\n",
            "       device='cuda:0'), dist b: tensor([34.6076, 21.7896, 27.7786, 13.4049, 20.3231, 29.6350, 23.3219, 19.1751,\n",
            "        23.0646, 27.1012, 14.8943, 20.2342, 25.0862, 18.3451, 23.5979, 17.7714,\n",
            "        23.7175, 30.1543, 21.2898, 20.1788, 25.9166, 27.9824, 23.5137, 18.9761,\n",
            "        16.9330, 22.7861, 20.0857, 27.1734, 15.6496, 18.0918, 21.5587, 14.3958,\n",
            "        28.2199, 31.8662, 28.0662, 22.1785, 24.0193, 12.8039, 21.7992, 24.2097,\n",
            "        14.3850, 22.3712, 14.2789, 21.1915, 19.9238, 28.2433, 24.2575, 24.8442,\n",
            "        26.4814, 27.4515, 31.6667, 16.3957, 19.7709, 14.9853, 15.0627, 20.6153,\n",
            "        22.5413, 16.9929, 15.9405, 27.0602, 21.4338, 16.5896, 29.6587, 27.4487],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.765625 \n",
            "tensor(0.7656)\n",
            "mini Batch Loss: 3.233590841293335\n",
            "mini Batch Loss: 2.789186954498291\n",
            "mini Batch Loss: 2.3315842151641846\n",
            "mini Batch Loss: 2.9949185848236084\n",
            "mini Batch Loss: 3.0041427612304688\n",
            "mini Batch Loss: 2.603635311126709\n",
            "mini Batch Loss: 3.256972074508667\n",
            "mini Batch Loss: 2.656919479370117\n",
            "mini Batch Loss: 2.6595888137817383\n",
            "mini Batch Loss: 2.902730703353882\n",
            "mini Batch Loss: 2.0807838439941406\n",
            "mini Batch Loss: 2.0669593811035156\n",
            "mini Batch Loss: 2.8270630836486816\n",
            "mini Batch Loss: 2.634211301803589\n",
            "mini Batch Loss: 2.685828685760498\n",
            "mini Batch Loss: 2.705092191696167\n",
            "mini Batch Loss: 4.3179030418396\n",
            "mini Batch Loss: 3.399052143096924\n",
            "mini Batch Loss: 2.9717140197753906\n",
            "mini Batch Loss: 2.4844095706939697\n",
            "mini Batch Loss: 3.068397045135498\n",
            "mini Batch Loss: 2.3010153770446777\n",
            "mini Batch Loss: 3.612539529800415\n",
            "mini Batch Loss: 2.96435546875\n",
            "mini Batch Loss: 2.6428894996643066\n",
            "mini Batch Loss: 3.48382830619812\n",
            "mini Batch Loss: 2.39078950881958\n",
            "mini Batch Loss: 3.3415017127990723\n",
            "mini Batch Loss: 3.545193672180176\n",
            "mini Batch Loss: 3.428642511367798\n",
            "Training Batch: 541 | Training Loss: 3.428642511367798\n",
            "Training Batch: 541 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_541.pt\n",
            "dist a: tensor([13.8514, 14.8909, 16.4074, 15.1383, 17.3987, 18.5576, 17.6271, 13.2383,\n",
            "        18.2705, 18.9483, 13.2503, 19.4366, 20.2227, 17.4835, 15.2522, 19.0844,\n",
            "        25.8729, 12.3730, 15.4157, 18.5574, 22.2173, 13.2621, 12.3812, 18.7209,\n",
            "        16.8924, 22.2779, 13.0364, 19.8227, 18.7502, 14.6338, 12.0229, 14.7005,\n",
            "        14.2461, 15.0602, 19.4512, 15.2124, 12.8476, 14.1731, 16.2163, 17.0446,\n",
            "        21.1368, 17.7410, 17.5637, 14.4687, 24.5212, 13.0908, 20.9124, 25.3316,\n",
            "        15.2229, 10.1237, 24.8135, 16.6501, 28.9149, 14.2016, 20.8131, 13.5126,\n",
            "        17.0199, 21.1976, 20.2043, 13.2599, 13.5776, 18.7368, 19.8833, 23.9744],\n",
            "       device='cuda:0'), dist b: tensor([31.3257, 22.2938, 25.6144, 14.1246, 20.5886, 28.9221, 23.1310, 18.2296,\n",
            "        21.3374, 28.6581, 14.7331, 19.3625, 25.7055, 18.7237, 22.0727, 17.8039,\n",
            "        28.9710, 29.2568, 22.0141, 20.7104, 21.1161, 26.3582, 20.9844, 19.2752,\n",
            "        17.1951, 21.0199, 18.6612, 26.0617, 15.7217, 18.5245, 20.5815, 14.8330,\n",
            "        27.9589, 31.2745, 23.6377, 19.9561, 22.8505, 11.7714, 20.5735, 24.6823,\n",
            "        13.6563, 21.8600, 13.9025, 20.3887, 19.3026, 25.8577, 25.3547, 23.1585,\n",
            "        25.1654, 25.2297, 28.2844, 17.2263, 18.5760, 16.6163, 15.3728, 19.8209,\n",
            "        19.2229, 18.1262, 16.5830, 24.2587, 17.1860, 16.7551, 28.3290, 26.4252],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "mini Batch Loss: 1.5969018936157227\n",
            "mini Batch Loss: 2.878830671310425\n",
            "mini Batch Loss: 2.614985466003418\n",
            "mini Batch Loss: 3.4467029571533203\n",
            "mini Batch Loss: 2.2930374145507812\n",
            "mini Batch Loss: 3.1986289024353027\n",
            "mini Batch Loss: 2.6857728958129883\n",
            "mini Batch Loss: 2.693315029144287\n",
            "mini Batch Loss: 2.6563496589660645\n",
            "mini Batch Loss: 2.045703887939453\n",
            "mini Batch Loss: 2.946107864379883\n",
            "mini Batch Loss: 3.015876531600952\n",
            "mini Batch Loss: 3.578744411468506\n",
            "mini Batch Loss: 3.835706949234009\n",
            "mini Batch Loss: 2.3117971420288086\n",
            "mini Batch Loss: 3.2147645950317383\n",
            "mini Batch Loss: 2.0819671154022217\n",
            "mini Batch Loss: 3.163712739944458\n",
            "mini Batch Loss: 3.0563712120056152\n",
            "mini Batch Loss: 3.274418592453003\n",
            "mini Batch Loss: 2.397952079772949\n",
            "mini Batch Loss: 2.9399709701538086\n",
            "mini Batch Loss: 2.373929738998413\n",
            "mini Batch Loss: 1.840692162513733\n",
            "mini Batch Loss: 3.235297203063965\n",
            "mini Batch Loss: 2.9007391929626465\n",
            "mini Batch Loss: 3.7574663162231445\n",
            "mini Batch Loss: 2.708852529525757\n",
            "mini Batch Loss: 2.51420259475708\n",
            "mini Batch Loss: 3.0705301761627197\n",
            "Training Batch: 571 | Training Loss: 3.0705301761627197\n",
            "Training Batch: 571 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_571.pt\n",
            "dist a: tensor([14.7583, 14.5340, 14.5754, 15.1040, 18.6616, 18.6988, 17.3212, 14.1671,\n",
            "        19.3090, 17.9332, 16.0019, 17.5340, 19.8117, 15.2812, 15.4505, 18.7833,\n",
            "        22.8140, 13.5913, 16.1836, 17.4545, 19.5777, 12.6397, 12.5224, 17.9731,\n",
            "        18.1753, 20.2838, 12.9139, 18.4714, 16.7771, 13.3098, 11.4822, 14.4379,\n",
            "        15.3360, 14.9625, 23.3055, 17.0345, 12.6314, 14.4289, 15.8808, 16.9425,\n",
            "        18.1086, 17.4131, 16.7113, 14.1734, 23.3900, 12.7398, 21.7238, 23.6399,\n",
            "        16.7486, 10.5344, 23.6055, 17.0604, 26.3298, 13.5488, 22.4999, 13.1240,\n",
            "        16.0534, 21.8624, 20.6927, 12.5483, 13.6189, 19.9919, 19.6161, 22.7820],\n",
            "       device='cuda:0'), dist b: tensor([33.7493, 21.0769, 23.6579, 13.6807, 20.8172, 27.2473, 24.3690, 18.0643,\n",
            "        21.8597, 25.8239, 14.2949, 18.4117, 24.4143, 17.5584, 22.9921, 17.7973,\n",
            "        25.4603, 31.2336, 18.3325, 19.8079, 24.6867, 26.0195, 20.9208, 17.5325,\n",
            "        15.8220, 21.8719, 17.4482, 24.3310, 15.8216, 18.2867, 20.6403, 15.6737,\n",
            "        29.5777, 29.3554, 27.6164, 19.5136, 24.4265, 11.7832, 19.6667, 23.6008,\n",
            "        13.6492, 21.9655, 15.0254, 21.2233, 20.1863, 24.4467, 22.7860, 21.3614,\n",
            "        22.8503, 25.9287, 28.3333, 16.5115, 20.1556, 16.3689, 14.6669, 17.9234,\n",
            "        19.3248, 18.5736, 16.2854, 25.2831, 16.9920, 17.2317, 27.0967, 26.7328],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "mini Batch Loss: 3.0071122646331787\n",
            "mini Batch Loss: 2.311518669128418\n",
            "mini Batch Loss: 3.2034335136413574\n",
            "mini Batch Loss: 2.4751808643341064\n",
            "mini Batch Loss: 2.7833714485168457\n",
            "mini Batch Loss: 3.256093740463257\n",
            "mini Batch Loss: 2.7327609062194824\n",
            "mini Batch Loss: 2.8323326110839844\n",
            "mini Batch Loss: 2.7019712924957275\n",
            "mini Batch Loss: 3.1808223724365234\n",
            "mini Batch Loss: 2.1921095848083496\n",
            "mini Batch Loss: 2.368624687194824\n",
            "mini Batch Loss: 3.465503454208374\n",
            "mini Batch Loss: 2.434894323348999\n",
            "mini Batch Loss: 2.091308116912842\n",
            "mini Batch Loss: 3.715165853500366\n",
            "mini Batch Loss: 2.760812282562256\n",
            "mini Batch Loss: 2.3905158042907715\n",
            "mini Batch Loss: 3.488187551498413\n",
            "mini Batch Loss: 2.8765926361083984\n",
            "mini Batch Loss: 2.2527880668640137\n",
            "mini Batch Loss: 2.6519479751586914\n",
            "mini Batch Loss: 3.18149995803833\n",
            "mini Batch Loss: 3.176525831222534\n",
            "mini Batch Loss: 2.7359695434570312\n",
            "mini Batch Loss: 2.203094005584717\n",
            "mini Batch Loss: 2.4140074253082275\n",
            "mini Batch Loss: 2.947375774383545\n",
            "mini Batch Loss: 3.408668041229248\n",
            "mini Batch Loss: 2.9368982315063477\n",
            "Training Batch: 601 | Training Loss: 2.9368982315063477\n",
            "Training Batch: 601 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_601.pt\n",
            "dist a: tensor([15.0430, 14.0031, 15.9208, 15.0034, 17.0635, 18.4397, 17.1099, 13.1154,\n",
            "        19.5800, 18.0343, 15.1164, 18.4402, 20.6734, 15.0513, 15.0822, 20.0773,\n",
            "        24.5598, 11.7837, 17.5122, 19.1851, 21.6907, 12.7110, 12.6008, 17.3618,\n",
            "        18.3094, 20.9632, 12.1386, 16.1666, 15.6895, 14.4059, 11.4493, 13.9072,\n",
            "        15.5389, 15.2501, 23.3361, 16.7519, 11.8362, 14.7493, 15.7078, 16.4585,\n",
            "        19.6091, 17.9362, 17.9042, 13.7428, 23.0051, 13.1288, 20.9455, 23.5672,\n",
            "        18.0055, 10.4503, 24.2103, 18.1171, 26.0638, 14.2665, 24.3221, 13.8308,\n",
            "        15.8890, 21.4692, 20.2150, 14.2337, 13.4355, 22.2666, 19.8492, 21.4254],\n",
            "       device='cuda:0'), dist b: tensor([34.5326, 21.2311, 26.0786, 14.6907, 19.9763, 27.9872, 25.4500, 17.1365,\n",
            "        20.7893, 27.4557, 13.3150, 18.6391, 26.2992, 17.2697, 23.1618, 19.0393,\n",
            "        28.9706, 31.2932, 19.0608, 20.5776, 23.1333, 27.3310, 19.7418, 17.8947,\n",
            "        14.9945, 22.4275, 16.8031, 25.3264, 15.0899, 18.9374, 22.6250, 14.8374,\n",
            "        29.4862, 29.9125, 29.7037, 18.8556, 25.1020, 11.7187, 20.7775, 23.7255,\n",
            "        12.9494, 22.3217, 15.7003, 21.6683, 18.8567, 24.7754, 24.2177, 20.8030,\n",
            "        22.7714, 27.3282, 28.9117, 17.9755, 19.0895, 15.7570, 13.8038, 18.2477,\n",
            "        19.5926, 19.1283, 16.0798, 23.3843, 15.7425, 15.9358, 25.8596, 26.6288],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "mini Batch Loss: 3.2083263397216797\n",
            "mini Batch Loss: 2.7106828689575195\n",
            "mini Batch Loss: 3.018794059753418\n",
            "mini Batch Loss: 2.155503273010254\n",
            "mini Batch Loss: 2.3255207538604736\n",
            "mini Batch Loss: 3.0545547008514404\n",
            "mini Batch Loss: 2.402921199798584\n",
            "mini Batch Loss: 3.073840856552124\n",
            "mini Batch Loss: 2.8134589195251465\n",
            "mini Batch Loss: 2.33111834526062\n",
            "mini Batch Loss: 2.837458610534668\n",
            "mini Batch Loss: 2.870997667312622\n",
            "mini Batch Loss: 2.967233896255493\n",
            "mini Batch Loss: 4.038777828216553\n",
            "mini Batch Loss: 2.647937774658203\n",
            "mini Batch Loss: 2.7643799781799316\n",
            "mini Batch Loss: 2.8253045082092285\n",
            "mini Batch Loss: 2.2494993209838867\n",
            "mini Batch Loss: 2.3089516162872314\n",
            "mini Batch Loss: 2.871990442276001\n",
            "mini Batch Loss: 2.7161827087402344\n",
            "mini Batch Loss: 3.0371546745300293\n",
            "mini Batch Loss: 2.7326865196228027\n",
            "mini Batch Loss: 3.195800304412842\n",
            "mini Batch Loss: 2.4683642387390137\n",
            "mini Batch Loss: 3.5519516468048096\n",
            "mini Batch Loss: 2.7842230796813965\n",
            "mini Batch Loss: 2.4049839973449707\n",
            "mini Batch Loss: 2.1488451957702637\n",
            "mini Batch Loss: 3.0904173851013184\n",
            "Training Batch: 631 | Training Loss: 3.0904173851013184\n",
            "Training Batch: 631 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_631.pt\n",
            "dist a: tensor([14.4956, 14.2165, 16.0944, 14.7142, 17.0975, 17.6361, 16.9777, 12.7573,\n",
            "        17.7403, 17.9110, 16.0396, 17.7569, 21.3433, 16.4667, 15.6089, 18.6850,\n",
            "        21.7506, 12.6156, 15.8795, 19.1051, 18.8577, 13.5891, 12.5384, 18.5312,\n",
            "        16.0955, 21.0039, 13.4390, 17.4371, 16.1647, 15.3032, 10.9513, 14.0895,\n",
            "        16.0762, 15.3790, 22.1915, 17.0195, 12.4761, 14.5168, 15.1344, 17.1784,\n",
            "        18.3918, 17.0213, 18.3926, 14.0290, 23.9361, 12.8252, 19.6584, 23.4578,\n",
            "        19.9668, 10.3020, 27.3671, 16.8458, 25.4174, 14.3538, 22.1928, 13.3014,\n",
            "        15.6163, 23.3265, 20.4380, 13.3800, 14.2622, 23.2784, 20.9718, 23.3325],\n",
            "       device='cuda:0'), dist b: tensor([35.0382, 22.7627, 24.5293, 14.0973, 18.4083, 29.0401, 25.6754, 19.6740,\n",
            "        21.8515, 24.1191, 14.3517, 20.2780, 24.0688, 17.3978, 22.1867, 16.0615,\n",
            "        25.5088, 31.3942, 19.8126, 24.0227, 26.0911, 26.9750, 22.3850, 18.5104,\n",
            "        15.6085, 22.5266, 17.0412, 24.3738, 15.1841, 22.1798, 21.0073, 14.8420,\n",
            "        28.2659, 25.7745, 27.7856, 20.6648, 23.0270, 12.1454, 22.2781, 24.5126,\n",
            "        13.1176, 23.6364, 16.3871, 20.6402, 16.2960, 26.2449, 23.6458, 22.2622,\n",
            "        21.2886, 27.2887, 31.7616, 16.3691, 19.5206, 14.6053, 13.6792, 17.1643,\n",
            "        21.8237, 18.2755, 17.4369, 24.7469, 17.8894, 14.6062, 26.2597, 27.3272],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.734375 \n",
            "tensor(0.7344)\n",
            "mini Batch Loss: 3.013312578201294\n",
            "mini Batch Loss: 3.8534388542175293\n",
            "mini Batch Loss: 3.3326187133789062\n",
            "mini Batch Loss: 3.039370536804199\n",
            "mini Batch Loss: 2.546708822250366\n",
            "mini Batch Loss: 2.189201831817627\n",
            "mini Batch Loss: 2.6892285346984863\n",
            "mini Batch Loss: 3.34639048576355\n",
            "mini Batch Loss: 2.177004814147949\n",
            "mini Batch Loss: 2.383867025375366\n",
            "mini Batch Loss: 2.5772032737731934\n",
            "mini Batch Loss: 3.154621124267578\n",
            "mini Batch Loss: 2.521449565887451\n",
            "mini Batch Loss: 2.7040648460388184\n",
            "mini Batch Loss: 3.322718381881714\n",
            "mini Batch Loss: 2.625366687774658\n",
            "mini Batch Loss: 2.433403253555298\n",
            "mini Batch Loss: 2.4109272956848145\n",
            "mini Batch Loss: 2.6952691078186035\n",
            "mini Batch Loss: 2.2313408851623535\n",
            "mini Batch Loss: 2.846616744995117\n",
            "mini Batch Loss: 2.7474310398101807\n",
            "mini Batch Loss: 2.7138757705688477\n",
            "mini Batch Loss: 2.5208797454833984\n",
            "mini Batch Loss: 2.4700400829315186\n",
            "mini Batch Loss: 3.2164483070373535\n",
            "mini Batch Loss: 2.4593796730041504\n",
            "mini Batch Loss: 2.6082072257995605\n",
            "mini Batch Loss: 2.534425735473633\n",
            "mini Batch Loss: 3.125128984451294\n",
            "Training Batch: 661 | Training Loss: 3.125128984451294\n",
            "Training Batch: 661 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_661.pt\n",
            "dist a: tensor([15.0221, 14.3293, 16.6831, 15.8832, 14.5310, 17.4373, 16.8214, 12.3519,\n",
            "        17.5741, 17.4955, 15.6919, 17.8984, 21.3883, 16.0530, 15.1989, 18.6369,\n",
            "        24.6512, 12.8659, 17.1262, 18.6535, 19.5447, 12.5614, 12.7712, 18.5525,\n",
            "        16.7246, 21.8968, 12.8154, 18.7622, 16.8419, 15.6134, 11.1985, 13.9700,\n",
            "        16.1918, 15.6174, 24.2352, 17.6213, 11.8737, 15.1303, 15.1244, 16.0630,\n",
            "        17.4569, 17.7141, 18.4935, 13.9493, 23.1582, 12.8101, 18.2544, 26.1096,\n",
            "        20.1493,  9.7799, 25.3830, 17.4618, 27.0436, 13.2411, 25.9616, 12.9078,\n",
            "        16.3089, 22.3004, 20.2353, 13.6521, 14.0158, 21.1267, 22.0507, 23.9031],\n",
            "       device='cuda:0'), dist b: tensor([34.7107, 20.4886, 27.6349, 15.3272, 20.6195, 29.7418, 24.6076, 18.4977,\n",
            "        20.1390, 27.5425, 15.9595, 19.3891, 25.4354, 18.5900, 20.8918, 17.7781,\n",
            "        28.1278, 31.1288, 19.6920, 24.5620, 22.6891, 28.2525, 22.7085, 18.5549,\n",
            "        15.6531, 23.4381, 16.6513, 25.2331, 15.5137, 20.6312, 21.0643, 15.4939,\n",
            "        27.7488, 30.3594, 29.4545, 19.2465, 23.2091, 12.4050, 23.5057, 25.0602,\n",
            "        12.7039, 23.3988, 15.1783, 23.5681, 19.0681, 24.7058, 23.1730, 22.5075,\n",
            "        24.2033, 26.8207, 31.2297, 16.5571, 18.0450, 15.1327, 13.8378, 19.0480,\n",
            "        20.2620, 19.0651, 17.4011, 23.3012, 17.2657, 17.6039, 26.9548, 27.7549],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.765625 \n",
            "tensor(0.7656)\n",
            "mini Batch Loss: 3.0893561840057373\n",
            "mini Batch Loss: 2.5233325958251953\n",
            "mini Batch Loss: 2.2095446586608887\n",
            "mini Batch Loss: 2.593626022338867\n",
            "mini Batch Loss: 2.2692503929138184\n",
            "mini Batch Loss: 2.4730916023254395\n",
            "mini Batch Loss: 3.3510632514953613\n",
            "mini Batch Loss: 2.3065011501312256\n",
            "mini Batch Loss: 2.506258964538574\n",
            "mini Batch Loss: 2.839148998260498\n",
            "mini Batch Loss: 2.823047161102295\n",
            "mini Batch Loss: 2.3935976028442383\n",
            "mini Batch Loss: 3.1322903633117676\n",
            "mini Batch Loss: 1.9561851024627686\n",
            "mini Batch Loss: 3.066969394683838\n",
            "mini Batch Loss: 2.193676471710205\n",
            "mini Batch Loss: 2.838175058364868\n",
            "mini Batch Loss: 2.301382064819336\n",
            "mini Batch Loss: 3.014310836791992\n",
            "mini Batch Loss: 3.575216293334961\n",
            "mini Batch Loss: 1.8504902124404907\n",
            "mini Batch Loss: 2.666779041290283\n",
            "mini Batch Loss: 2.3765835762023926\n",
            "mini Batch Loss: 2.400634765625\n",
            "mini Batch Loss: 2.689652919769287\n",
            "mini Batch Loss: 2.3299779891967773\n",
            "mini Batch Loss: 2.5025129318237305\n",
            "mini Batch Loss: 2.1190381050109863\n",
            "mini Batch Loss: 2.6153464317321777\n",
            "mini Batch Loss: 2.4813313484191895\n",
            "Training Batch: 691 | Training Loss: 2.4813313484191895\n",
            "Training Batch: 691 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_691.pt\n",
            "dist a: tensor([14.2528, 15.5513, 15.9710, 15.3907, 18.0953, 16.4066, 17.7208, 12.9307,\n",
            "        17.6801, 17.7758, 17.4424, 17.8317, 20.8910, 18.1834, 15.3198, 19.0415,\n",
            "        24.1506, 12.5949, 15.3982, 18.2577, 18.5736, 13.1732, 12.7061, 17.8746,\n",
            "        15.8854, 21.7703, 13.7047, 18.3368, 16.6203, 14.7844, 10.7224, 14.1761,\n",
            "        14.7547, 15.6591, 23.8922, 18.5020, 12.4061, 15.3865, 14.2598, 16.1992,\n",
            "        19.2700, 17.7677, 18.2393, 14.3168, 24.8629, 11.7371, 19.3341, 24.5826,\n",
            "        17.5005,  9.3711, 26.8214, 15.2549, 28.2740, 13.7027, 23.9938, 14.2058,\n",
            "        16.7711, 23.4625, 20.2687, 12.8501, 14.9082, 20.8242, 21.7940, 22.3848],\n",
            "       device='cuda:0'), dist b: tensor([34.0157, 22.3982, 26.6989, 15.0869, 19.8508, 29.0184, 25.4988, 18.9939,\n",
            "        22.2744, 27.7935, 14.9973, 19.9367, 24.6457, 17.1046, 21.8536, 17.9718,\n",
            "        29.2740, 31.1186, 20.0717, 22.9577, 27.2008, 28.1456, 22.6671, 18.8898,\n",
            "        16.7865, 24.0281, 17.7127, 22.8738, 14.6339, 20.5148, 21.1576, 14.9552,\n",
            "        27.2388, 27.9812, 29.9576, 20.0613, 23.0149, 11.7414, 22.0319, 26.0753,\n",
            "        13.5024, 22.2017, 14.7757, 22.3031, 19.9544, 24.5294, 23.3599, 22.6324,\n",
            "        21.1598, 23.8236, 32.2426, 16.4397, 18.2325, 15.9062, 13.7423, 17.4251,\n",
            "        21.1606, 19.9802, 17.7662, 23.7874, 16.9044, 17.8633, 26.4588, 26.1631],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.765625 \n",
            "tensor(0.7656)\n",
            "mini Batch Loss: 3.0801424980163574\n",
            "mini Batch Loss: 3.138814926147461\n",
            "mini Batch Loss: 2.4756579399108887\n",
            "mini Batch Loss: 2.308142900466919\n",
            "mini Batch Loss: 2.940197706222534\n",
            "mini Batch Loss: 2.0308737754821777\n",
            "mini Batch Loss: 1.9271100759506226\n",
            "mini Batch Loss: 1.8266403675079346\n",
            "mini Batch Loss: 2.7769436836242676\n",
            "mini Batch Loss: 2.8967180252075195\n",
            "mini Batch Loss: 2.4036033153533936\n",
            "mini Batch Loss: 3.2697527408599854\n",
            "mini Batch Loss: 2.7051162719726562\n",
            "mini Batch Loss: 2.827993392944336\n",
            "mini Batch Loss: 2.3064470291137695\n",
            "mini Batch Loss: 2.770550489425659\n",
            "mini Batch Loss: 3.591752052307129\n",
            "mini Batch Loss: 2.2821097373962402\n",
            "mini Batch Loss: 2.5045318603515625\n",
            "mini Batch Loss: 2.377495765686035\n",
            "mini Batch Loss: 2.4674339294433594\n",
            "mini Batch Loss: 2.5834274291992188\n",
            "mini Batch Loss: 2.90019154548645\n",
            "mini Batch Loss: 2.3182454109191895\n",
            "mini Batch Loss: 3.2027132511138916\n",
            "mini Batch Loss: 1.9903522729873657\n",
            "mini Batch Loss: 3.061880111694336\n",
            "mini Batch Loss: 2.45415997505188\n",
            "mini Batch Loss: 2.407935857772827\n",
            "mini Batch Loss: 2.9648592472076416\n",
            "Training Batch: 721 | Training Loss: 2.9648592472076416\n",
            "Training Batch: 721 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_721.pt\n",
            "dist a: tensor([14.6493, 14.6074, 17.2022, 14.3479, 16.7221, 17.1987, 18.8802, 13.7760,\n",
            "        18.9925, 18.3047, 17.7679, 17.2876, 20.9757, 19.2112, 15.4637, 18.1768,\n",
            "        26.7262, 14.2308, 15.9222, 19.6555, 17.8907, 12.2364, 12.8112, 18.2272,\n",
            "        16.4569, 22.6689, 13.9037, 17.9695, 16.8922, 16.5711, 11.8892, 14.5754,\n",
            "        16.8668, 15.0272, 24.5604, 18.7798, 11.9234, 16.6991, 14.9632, 15.9513,\n",
            "        18.3362, 19.8581, 17.6503, 15.6893, 25.8168, 12.2602, 19.7892, 25.2642,\n",
            "        17.5853, 10.0990, 28.5682, 15.2894, 26.5149, 14.3144, 26.5126, 13.6758,\n",
            "        16.1135, 23.4240, 21.0552, 14.6350, 15.2990, 17.7913, 24.1474, 22.8513],\n",
            "       device='cuda:0'), dist b: tensor([33.6069, 19.1646, 27.6623, 15.5134, 19.1066, 28.4490, 27.7618, 19.0752,\n",
            "        22.2312, 27.5276, 16.6996, 20.4956, 23.9450, 20.1105, 21.4367, 17.7270,\n",
            "        31.5293, 31.3157, 19.9845, 23.3832, 24.7670, 27.3751, 22.4885, 20.1104,\n",
            "        16.4253, 23.3635, 16.8953, 25.2132, 17.0086, 21.6720, 19.1992, 14.5596,\n",
            "        31.5580, 26.6024, 28.2563, 18.7267, 23.0331, 12.7395, 21.0248, 25.7365,\n",
            "        13.0030, 22.7544, 14.6148, 21.5998, 21.7526, 23.9701, 23.3995, 23.6602,\n",
            "        21.2472, 23.6302, 32.8434, 16.7247, 18.4916, 16.2735, 13.9256, 17.2889,\n",
            "        22.1955, 19.4276, 18.0438, 25.2282, 17.7564, 21.0486, 29.5434, 29.1460],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.78125 \n",
            "tensor(0.7812)\n",
            "mini Batch Loss: 2.30004620552063\n",
            "mini Batch Loss: 3.097998857498169\n",
            "mini Batch Loss: 2.262938976287842\n",
            "mini Batch Loss: 2.766433000564575\n",
            "mini Batch Loss: 2.405651807785034\n",
            "mini Batch Loss: 3.1548714637756348\n",
            "mini Batch Loss: 2.200638771057129\n",
            "mini Batch Loss: 2.4761078357696533\n",
            "mini Batch Loss: 2.2166800498962402\n",
            "mini Batch Loss: 3.5286006927490234\n",
            "mini Batch Loss: 2.702169895172119\n",
            "mini Batch Loss: 2.1871650218963623\n",
            "mini Batch Loss: 2.842512607574463\n",
            "mini Batch Loss: 3.2643377780914307\n",
            "mini Batch Loss: 3.32633900642395\n",
            "mini Batch Loss: 2.9820234775543213\n",
            "mini Batch Loss: 2.685459613800049\n",
            "mini Batch Loss: 2.5848541259765625\n",
            "mini Batch Loss: 2.2895634174346924\n",
            "mini Batch Loss: 2.7454681396484375\n",
            "mini Batch Loss: 2.678342342376709\n",
            "mini Batch Loss: 2.1337356567382812\n",
            "mini Batch Loss: 2.5834851264953613\n",
            "mini Batch Loss: 2.6516079902648926\n",
            "mini Batch Loss: 2.601374626159668\n",
            "mini Batch Loss: 2.3252315521240234\n",
            "mini Batch Loss: 2.43125057220459\n",
            "mini Batch Loss: 2.6861445903778076\n",
            "mini Batch Loss: 2.378115177154541\n",
            "mini Batch Loss: 2.3956003189086914\n",
            "Training Batch: 751 | Training Loss: 2.3956003189086914\n",
            "Training Batch: 751 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_751.pt\n",
            "dist a: tensor([13.4722, 14.9109, 17.8471, 14.0186, 14.7372, 16.8751, 18.1718, 13.4007,\n",
            "        16.2646, 17.6868, 14.6042, 17.7576, 19.1371, 15.7805, 16.1246, 19.2445,\n",
            "        26.1244, 13.3217, 16.4796, 19.6334, 20.4755, 12.3494, 12.5094, 17.8512,\n",
            "        18.8284, 23.2778, 12.0902, 16.7885, 15.7256, 15.0024, 13.1095, 13.8217,\n",
            "        17.7216, 16.5275, 23.3383, 16.7717, 11.7036, 14.0704, 15.5987, 17.0599,\n",
            "        18.8983, 17.2834, 18.7939, 16.3196, 25.6194, 11.1023, 18.6394, 26.5598,\n",
            "        19.3369, 10.3350, 25.2241, 15.2482, 26.3664, 14.1437, 27.9785, 13.5445,\n",
            "        17.4416, 22.5645, 21.4828, 13.2598, 15.1408, 20.2788, 21.1109, 23.1315],\n",
            "       device='cuda:0'), dist b: tensor([30.7548, 19.6740, 30.4125, 14.0419, 21.9767, 28.9577, 23.7848, 17.2314,\n",
            "        21.6723, 27.6788, 14.9928, 19.0290, 22.8795, 19.0788, 21.1262, 21.1337,\n",
            "        30.1902, 31.5230, 17.4815, 22.9112, 21.9914, 25.9996, 19.6768, 19.1765,\n",
            "        16.4499, 24.3815, 15.2612, 25.3582, 17.3426, 20.8598, 18.2523, 15.5390,\n",
            "        29.8522, 27.5847, 26.7539, 17.2139, 25.6591, 11.7521, 21.4334, 25.0637,\n",
            "        13.1282, 21.0425, 15.3820, 24.5103, 19.0898, 23.2046, 25.7400, 25.4033,\n",
            "        20.5058, 26.0206, 30.5404, 17.5231, 18.5244, 16.3804, 14.0335, 18.6922,\n",
            "        21.8401, 18.7742, 18.6149, 26.5703, 17.9247, 18.5729, 29.0513, 28.1852],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 2.8483848571777344\n",
            "mini Batch Loss: 3.1499598026275635\n",
            "mini Batch Loss: 3.0669543743133545\n",
            "mini Batch Loss: 3.395214080810547\n",
            "mini Batch Loss: 2.968536853790283\n",
            "mini Batch Loss: 1.9239287376403809\n",
            "mini Batch Loss: 2.6645212173461914\n",
            "mini Batch Loss: 2.774101972579956\n",
            "mini Batch Loss: 2.470811605453491\n",
            "mini Batch Loss: 3.4919915199279785\n",
            "mini Batch Loss: 2.8651580810546875\n",
            "mini Batch Loss: 3.1874537467956543\n",
            "mini Batch Loss: 3.1818408966064453\n",
            "mini Batch Loss: 2.753619909286499\n",
            "mini Batch Loss: 3.9420559406280518\n",
            "mini Batch Loss: 2.8385379314422607\n",
            "mini Batch Loss: 2.5678954124450684\n",
            "mini Batch Loss: 2.3684210777282715\n",
            "mini Batch Loss: 2.6491916179656982\n",
            "mini Batch Loss: 3.077455997467041\n",
            "mini Batch Loss: 3.0505237579345703\n",
            "mini Batch Loss: 3.0636448860168457\n",
            "mini Batch Loss: 2.9671154022216797\n",
            "mini Batch Loss: 2.5217959880828857\n",
            "mini Batch Loss: 2.755866765975952\n",
            "mini Batch Loss: 2.86647891998291\n",
            "mini Batch Loss: 2.3659451007843018\n",
            "mini Batch Loss: 2.9448068141937256\n",
            "mini Batch Loss: 2.189396381378174\n",
            "mini Batch Loss: 2.9407615661621094\n",
            "Training Batch: 781 | Training Loss: 2.9407615661621094\n",
            "Training Batch: 781 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_781.pt\n",
            "dist a: tensor([13.4884, 15.7949, 17.6173, 14.4024, 16.4936, 16.3490, 19.7394, 12.9422,\n",
            "        14.2479, 17.3178, 14.5066, 16.5753, 19.0625, 14.4252, 15.5834, 20.2402,\n",
            "        23.2534, 12.9182, 15.7203, 18.7332, 18.9144, 12.6275, 12.6743, 17.6422,\n",
            "        18.7387, 19.5548, 12.5214, 17.5306, 16.2935, 14.6266, 12.6726, 14.3089,\n",
            "        13.9855, 14.4365, 22.3616, 17.4374, 12.0160, 14.3019, 14.7169, 18.0605,\n",
            "        20.4054, 18.5188, 16.9662, 17.3448, 25.5059, 11.4592, 19.3032, 23.6838,\n",
            "        17.3293, 10.2654, 24.0783, 15.3079, 26.0844, 14.3594, 24.7992, 13.0772,\n",
            "        16.7154, 22.7854, 20.2722, 13.3050, 14.7529, 16.7879, 21.2237, 22.0095],\n",
            "       device='cuda:0'), dist b: tensor([30.0059, 24.3864, 28.0626, 13.7492, 20.9216, 24.1752, 24.1946, 17.3088,\n",
            "        23.1693, 27.2561, 13.9374, 19.1313, 22.3774, 18.7789, 22.4297, 18.6371,\n",
            "        26.5494, 28.8178, 16.9491, 22.9600, 24.7654, 24.4447, 19.5408, 18.5770,\n",
            "        17.5050, 20.9377, 15.8293, 21.9700, 15.9105, 19.8174, 18.4174, 15.7189,\n",
            "        27.7333, 25.5650, 25.5640, 18.7723, 23.6307, 11.7702, 21.9567, 25.1783,\n",
            "        13.5944, 20.8751, 14.6228, 21.4954, 20.5643, 23.2637, 25.2064, 24.1708,\n",
            "        19.5919, 23.1846, 27.4356, 17.2675, 18.8794, 17.0568, 14.6001, 17.0629,\n",
            "        20.6605, 19.7998, 16.9989, 22.1677, 16.1416, 22.7386, 29.0248, 26.8081],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "mini Batch Loss: 1.9410130977630615\n",
            "mini Batch Loss: 2.1154513359069824\n",
            "mini Batch Loss: 3.0422346591949463\n",
            "mini Batch Loss: 3.5239009857177734\n",
            "mini Batch Loss: 2.4001033306121826\n",
            "mini Batch Loss: 2.939373016357422\n",
            "mini Batch Loss: 2.2527146339416504\n",
            "mini Batch Loss: 2.4856109619140625\n",
            "mini Batch Loss: 3.2585370540618896\n",
            "mini Batch Loss: 2.6615896224975586\n",
            "mini Batch Loss: 2.55130672454834\n",
            "mini Batch Loss: 3.24249529838562\n",
            "mini Batch Loss: 2.249060869216919\n",
            "mini Batch Loss: 2.5374085903167725\n",
            "mini Batch Loss: 2.6288657188415527\n",
            "mini Batch Loss: 2.5584402084350586\n",
            "mini Batch Loss: 3.779773235321045\n",
            "mini Batch Loss: 2.4938530921936035\n",
            "mini Batch Loss: 2.943514823913574\n",
            "mini Batch Loss: 3.138766288757324\n",
            "mini Batch Loss: 2.521939754486084\n",
            "mini Batch Loss: 2.4504523277282715\n",
            "mini Batch Loss: 3.124859094619751\n",
            "mini Batch Loss: 2.6318652629852295\n",
            "mini Batch Loss: 2.854369640350342\n",
            "mini Batch Loss: 2.6559863090515137\n",
            "mini Batch Loss: 3.476547956466675\n",
            "mini Batch Loss: 1.997023344039917\n",
            "mini Batch Loss: 2.8390274047851562\n",
            "mini Batch Loss: 3.254366397857666\n",
            "Training Batch: 811 | Training Loss: 3.254366397857666\n",
            "Training Batch: 811 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_811.pt\n",
            "dist a: tensor([14.4566, 16.5241, 18.1876, 13.8998, 16.3956, 16.3486, 19.0323, 12.8361,\n",
            "        16.9664, 17.8127, 15.8521, 17.9148, 21.2524, 15.1686, 17.1818, 19.1528,\n",
            "        23.0739, 13.8210, 16.2547, 18.8983, 18.5516, 12.6876, 12.6775, 17.7546,\n",
            "        17.8673, 21.5059, 15.5850, 17.8368, 18.4386, 14.7678, 11.9704, 14.8709,\n",
            "        13.8074, 14.4210, 22.0437, 15.7275, 12.3392, 15.2893, 15.0899, 18.2354,\n",
            "        19.5759, 18.6331, 16.5028, 16.5534, 27.1794, 11.2620, 19.1872, 25.1844,\n",
            "        18.6463, 10.3034, 23.9433, 16.4300, 25.1242, 13.5634, 24.1821, 13.1855,\n",
            "        17.3469, 22.6536, 21.4105, 12.4743, 15.5630, 15.9079, 22.2111, 23.0919],\n",
            "       device='cuda:0'), dist b: tensor([31.0077, 24.0386, 29.8035, 14.4783, 22.0072, 26.6111, 24.2271, 17.7063,\n",
            "        21.5854, 28.5906, 16.3765, 19.3654, 23.2552, 18.4598, 23.0700, 18.5792,\n",
            "        26.5003, 31.0623, 17.8253, 24.2280, 24.5717, 26.0828, 20.5250, 20.7103,\n",
            "        17.8908, 24.1991, 17.2128, 23.1375, 15.6713, 21.4431, 19.9022, 14.3492,\n",
            "        29.2088, 26.8802, 24.6467, 19.0519, 23.7599, 12.4605, 22.5420, 25.5239,\n",
            "        14.5537, 22.3228, 16.2440, 18.7699, 20.3683, 23.5853, 27.0587, 27.8361,\n",
            "        22.0028, 26.2863, 30.0621, 16.7092, 17.9584, 17.4190, 14.9490, 18.1602,\n",
            "        21.5180, 18.7071, 17.3709, 26.3369, 16.8053, 24.1522, 31.0130, 28.0415],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 3.0746304988861084\n",
            "mini Batch Loss: 2.5892441272735596\n",
            "mini Batch Loss: 2.8105549812316895\n",
            "mini Batch Loss: 2.5724189281463623\n",
            "mini Batch Loss: 3.027053117752075\n",
            "mini Batch Loss: 2.1451575756073\n",
            "mini Batch Loss: 2.9018752574920654\n",
            "mini Batch Loss: 2.3366823196411133\n",
            "mini Batch Loss: 1.5540660619735718\n",
            "mini Batch Loss: 2.35612154006958\n",
            "mini Batch Loss: 2.4882118701934814\n",
            "mini Batch Loss: 1.7869198322296143\n",
            "mini Batch Loss: 2.890596866607666\n",
            "mini Batch Loss: 2.5931787490844727\n",
            "mini Batch Loss: 1.9040924310684204\n",
            "mini Batch Loss: 3.4183051586151123\n",
            "mini Batch Loss: 2.1363143920898438\n",
            "mini Batch Loss: 2.778244972229004\n",
            "mini Batch Loss: 2.6443724632263184\n",
            "mini Batch Loss: 4.069002628326416\n",
            "mini Batch Loss: 3.197871685028076\n",
            "mini Batch Loss: 3.7532215118408203\n",
            "mini Batch Loss: 2.7168025970458984\n",
            "mini Batch Loss: 3.475003242492676\n",
            "mini Batch Loss: 2.9985313415527344\n",
            "mini Batch Loss: 2.713038206100464\n",
            "mini Batch Loss: 3.0777432918548584\n",
            "mini Batch Loss: 2.533599376678467\n",
            "mini Batch Loss: 2.4102847576141357\n",
            "mini Batch Loss: 2.7609786987304688\n",
            "Training Batch: 841 | Training Loss: 2.7609786987304688\n",
            "Training Batch: 841 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_841.pt\n",
            "dist a: tensor([15.2408, 17.0977, 18.4522, 14.5283, 16.3319, 17.5782, 19.6701, 13.7983,\n",
            "        15.9454, 19.7066, 16.3557, 19.1718, 21.4318, 16.0484, 19.5129, 18.7564,\n",
            "        20.2368, 15.9556, 16.7865, 18.1600, 17.5884, 12.8838, 13.4644, 18.8714,\n",
            "        17.4263, 24.3051, 16.1064, 19.5719, 17.2652, 16.5989, 12.5906, 14.6288,\n",
            "        14.9833, 13.5417, 24.8417, 18.5940, 12.1349, 14.9943, 14.2023, 17.2237,\n",
            "        17.6836, 17.5978, 16.1369, 16.6026, 25.9638, 11.9661, 19.3693, 23.6064,\n",
            "        16.6250,  9.9963, 25.1066, 17.1240, 23.4595, 13.0885, 21.5779, 13.6621,\n",
            "        15.8992, 23.7765, 23.4936, 12.0203, 16.1233, 16.7218, 23.0404, 23.4137],\n",
            "       device='cuda:0'), dist b: tensor([32.7745, 23.6782, 29.1995, 14.1745, 21.5202, 24.1433, 24.5229, 18.1470,\n",
            "        23.4293, 28.0294, 17.5649, 19.6523, 23.0598, 18.4589, 22.2421, 18.5179,\n",
            "        24.4723, 30.8877, 18.6435, 23.8792, 25.6073, 24.4064, 21.2593, 20.5972,\n",
            "        17.3527, 25.7546, 17.6927, 22.9813, 15.4333, 22.1829, 18.7701, 14.7198,\n",
            "        28.2445, 25.9258, 25.1236, 19.9685, 23.1630, 12.5348, 20.5575, 22.9604,\n",
            "        14.6614, 20.0744, 16.3146, 20.6388, 20.2250, 24.1838, 26.9901, 25.7730,\n",
            "        20.0750, 25.1515, 31.2625, 16.1972, 18.3011, 17.9363, 16.0838, 18.2662,\n",
            "        22.0099, 20.1929, 18.0163, 24.9090, 19.4687, 22.9164, 30.0917, 28.4698],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.8125 \n",
            "tensor(0.8125)\n",
            "mini Batch Loss: 2.1724114418029785\n",
            "mini Batch Loss: 2.0685696601867676\n",
            "mini Batch Loss: 2.9410974979400635\n",
            "mini Batch Loss: 2.7479000091552734\n",
            "mini Batch Loss: 2.69968843460083\n",
            "mini Batch Loss: 2.9297983646392822\n",
            "mini Batch Loss: 3.0276360511779785\n",
            "mini Batch Loss: 2.8040666580200195\n",
            "mini Batch Loss: 2.8414177894592285\n",
            "mini Batch Loss: 2.3640546798706055\n",
            "mini Batch Loss: 3.2599782943725586\n",
            "mini Batch Loss: 3.059237003326416\n",
            "mini Batch Loss: 2.343014717102051\n",
            "mini Batch Loss: 2.7367947101593018\n",
            "mini Batch Loss: 3.0150182247161865\n",
            "mini Batch Loss: 2.660254955291748\n",
            "mini Batch Loss: 2.7437009811401367\n",
            "mini Batch Loss: 3.407443046569824\n",
            "mini Batch Loss: 3.3847317695617676\n",
            "mini Batch Loss: 1.976138710975647\n",
            "mini Batch Loss: 2.1281819343566895\n",
            "mini Batch Loss: 2.907834529876709\n",
            "mini Batch Loss: 2.671943187713623\n",
            "mini Batch Loss: 2.5017290115356445\n",
            "mini Batch Loss: 2.876897096633911\n",
            "mini Batch Loss: 1.6419715881347656\n",
            "mini Batch Loss: 2.792156219482422\n",
            "mini Batch Loss: 2.5213494300842285\n",
            "mini Batch Loss: 2.3847200870513916\n",
            "mini Batch Loss: 1.8282071352005005\n",
            "Training Batch: 871 | Training Loss: 1.8282071352005005\n",
            "Training Batch: 871 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_871.pt\n",
            "dist a: tensor([13.9716, 15.3900, 15.9691, 13.9499, 15.2583, 16.4995, 17.5702, 14.0587,\n",
            "        16.1660, 19.6891, 16.1581, 18.7657, 21.2160, 14.2767, 18.5476, 18.9554,\n",
            "        27.8178, 14.3803, 15.7447, 17.6868, 19.3024, 12.6478, 13.4441, 19.0599,\n",
            "        17.5127, 21.2529, 15.1698, 18.8554, 18.5381, 16.9109, 15.0721, 14.0561,\n",
            "        16.1205, 13.7888, 26.7675, 17.5832, 11.9935, 14.6947, 15.2418, 16.9282,\n",
            "        18.7498, 17.5216, 14.8052, 16.0613, 30.0769, 11.7568, 19.4071, 25.6761,\n",
            "        19.0564,  9.3702, 23.5158, 16.3588, 25.8791, 15.4292, 24.7081, 14.0738,\n",
            "        16.2055, 24.7689, 21.7673, 18.8991, 16.0848, 21.6618, 19.6796, 22.2984],\n",
            "       device='cuda:0'), dist b: tensor([32.8156, 20.1903, 26.5369, 13.7478, 20.0244, 23.8129, 24.1251, 17.4239,\n",
            "        22.4393, 32.6199, 13.9173, 19.9791, 24.1817, 18.7011, 23.9887, 18.1463,\n",
            "        31.7356, 32.0365, 17.0381, 24.4076, 24.0458, 26.5394, 20.7341, 21.0404,\n",
            "        17.8653, 23.8746, 14.2630, 24.3636, 17.3621, 23.1919, 18.2386, 14.7140,\n",
            "        33.2724, 30.4607, 27.1888, 17.5114, 23.9878, 12.1397, 24.4477, 23.4696,\n",
            "        14.6654, 18.8853, 15.2751, 18.6445, 20.6861, 23.1503, 29.8169, 27.0342,\n",
            "        24.9885, 25.9770, 30.4434, 16.7519, 18.6757, 20.3055, 15.5798, 19.8171,\n",
            "        20.8296, 19.2090, 17.6903, 22.6933, 19.4486, 19.7810, 29.7559, 28.0487],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.78125 \n",
            "tensor(0.7812)\n",
            "mini Batch Loss: 1.9102808237075806\n",
            "mini Batch Loss: 2.651911497116089\n",
            "mini Batch Loss: 2.9957501888275146\n",
            "mini Batch Loss: 2.207303762435913\n",
            "mini Batch Loss: 2.4218292236328125\n",
            "mini Batch Loss: 2.7600369453430176\n",
            "mini Batch Loss: 2.4367215633392334\n",
            "mini Batch Loss: 2.110562801361084\n",
            "mini Batch Loss: 3.122089385986328\n",
            "mini Batch Loss: 2.3632049560546875\n",
            "mini Batch Loss: 1.607041835784912\n",
            "mini Batch Loss: 2.89050555229187\n",
            "mini Batch Loss: 1.897345781326294\n",
            "mini Batch Loss: 2.008212089538574\n",
            "mini Batch Loss: 2.5827813148498535\n",
            "mini Batch Loss: 3.2710518836975098\n",
            "mini Batch Loss: 2.7855443954467773\n",
            "mini Batch Loss: 2.210151195526123\n",
            "mini Batch Loss: 2.6773462295532227\n",
            "mini Batch Loss: 2.706512928009033\n",
            "mini Batch Loss: 1.9027010202407837\n",
            "mini Batch Loss: 2.666931629180908\n",
            "mini Batch Loss: 2.39174747467041\n",
            "mini Batch Loss: 3.0602614879608154\n",
            "mini Batch Loss: 3.0797581672668457\n",
            "mini Batch Loss: 2.7445991039276123\n",
            "mini Batch Loss: 2.5372824668884277\n",
            "mini Batch Loss: 2.444037914276123\n",
            "mini Batch Loss: 1.9033210277557373\n",
            "mini Batch Loss: 2.965282678604126\n",
            "Training Batch: 901 | Training Loss: 2.965282678604126\n",
            "Training Batch: 901 | Model saved to: /content/drive/My Drive/test2/model_epoch_1_batch_901.pt\n",
            "dist a: tensor([14.3384, 16.3291, 17.0402, 13.2971, 17.0136, 16.5952, 19.3823, 13.9877,\n",
            "        15.8353, 18.6986, 16.4680, 18.1640, 21.2297, 14.3940, 18.3143, 18.6850,\n",
            "        25.5257, 14.6992, 15.6499, 17.3322, 17.8110, 13.2319, 13.4623, 19.3723,\n",
            "        16.6798, 22.3687, 15.0627, 20.8116, 18.0966, 16.7463, 12.0251, 13.9816,\n",
            "        14.6596, 15.1333, 24.3223, 16.6930, 11.8739, 15.6482, 15.0866, 16.3123,\n",
            "        17.4628, 17.4979, 16.0176, 14.3329, 29.5024, 11.4554, 19.1694, 25.2319,\n",
            "        17.8605, 10.0956, 24.4240, 16.1076, 24.2557, 14.8740, 25.4351, 14.5324,\n",
            "        16.0701, 23.8762, 22.2060, 15.7325, 17.8873, 18.7606, 21.0992, 22.5073],\n",
            "       device='cuda:0'), dist b: tensor([34.7232, 21.9023, 26.9421, 13.0271, 19.7939, 24.3806, 24.3570, 18.3557,\n",
            "        24.0678, 30.0544, 14.3547, 19.4687, 22.6940, 17.5065, 24.3616, 17.0969,\n",
            "        27.9465, 31.0754, 19.5258, 24.2798, 26.3555, 27.4293, 20.2431, 22.6127,\n",
            "        18.0577, 25.0347, 15.7476, 23.2334, 15.9974, 24.1221, 20.4645, 14.7193,\n",
            "        29.3816, 27.1052, 26.3701, 16.8472, 23.9755, 11.8442, 24.4044, 21.1803,\n",
            "        14.4427, 19.6876, 15.2508, 19.8484, 20.1722, 23.6435, 28.4533, 25.8855,\n",
            "        22.2105, 24.7286, 30.9195, 17.4443, 18.0590, 18.9967, 15.1123, 22.0953,\n",
            "        22.4933, 18.7004, 17.6940, 22.2690, 23.6543, 20.6600, 31.2473, 26.9638],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.8125 \n",
            "tensor(0.8125)\n",
            "mini Batch Loss: 2.5344982147216797\n",
            "mini Batch Loss: 2.326982021331787\n",
            "mini Batch Loss: 3.2211670875549316\n",
            "mini Batch Loss: 2.7959113121032715\n",
            "mini Batch Loss: 2.9297444820404053\n",
            "mini Batch Loss: 2.5419869422912598\n",
            "mini Batch Loss: 2.076366662979126\n",
            "mini Batch Loss: 2.4892430305480957\n",
            "mini Batch Loss: 2.7260098457336426\n",
            "mini Batch Loss: 2.975780963897705\n",
            "mini Batch Loss: 2.2727880477905273\n",
            "mini Batch Loss: 2.7618236541748047\n",
            "mini Batch Loss: 3.110445976257324\n",
            "mini Batch Loss: 3.4073925018310547\n",
            "mini Batch Loss: 2.018606662750244\n",
            "mini Batch Loss: 2.031116008758545\n",
            "mini Batch Loss: 2.628939628601074\n",
            "mini Batch Loss: 3.172694206237793\n",
            "mini Batch Loss: 3.618286609649658\n",
            "mini Batch Loss: 2.840161085128784\n",
            "[1] average loss per epoch: 3.045\n",
            "Saved model checkpoint to /content/drive/My Drive/test2/model_epoch_0.pt\n",
            "dist a: tensor([14.7346, 18.2229, 17.9004, 13.7015, 17.1378, 16.7035, 19.6870, 14.1521,\n",
            "        16.5130, 18.5596, 21.3687, 19.8480, 21.9997, 14.3533, 16.9440, 18.4767,\n",
            "        22.9334, 16.3721, 16.1076, 17.8330, 17.4193, 12.1379, 13.8147, 18.4836,\n",
            "        18.2759, 22.5580, 14.2527, 19.8612, 18.2120, 16.9553, 11.7606, 14.0963,\n",
            "        14.5604, 14.7805, 22.5439, 15.8438, 11.5091, 15.8298, 15.2355, 15.0780,\n",
            "        16.2959, 17.9183, 17.3548, 13.3215, 25.3425, 11.0275, 19.2948, 23.6729,\n",
            "        16.2255, 10.3209, 25.5373, 17.3512, 25.1248, 14.6714, 25.6110, 14.3367,\n",
            "        14.2538, 22.0254, 23.9481, 14.3851, 16.0434, 16.8749, 23.4274, 21.3770],\n",
            "       device='cuda:0'), dist b: tensor([31.9620, 22.1136, 27.9829, 14.0639, 20.7262, 23.3005, 23.3759, 17.6575,\n",
            "        25.4295, 28.7724, 16.4912, 19.6447, 23.2085, 19.5990, 23.3683, 20.1920,\n",
            "        25.5814, 31.0055, 18.6461, 24.2609, 25.0172, 26.5410, 19.5112, 22.7975,\n",
            "        17.5284, 22.9672, 17.2604, 22.3534, 16.1531, 23.8159, 20.9159, 14.6986,\n",
            "        27.6846, 27.2880, 25.0305, 16.2532, 22.5841, 11.8464, 23.0999, 22.4705,\n",
            "        14.3568, 20.1980, 16.0761, 20.0645, 21.1979, 22.7497, 27.1880, 25.4056,\n",
            "        20.5065, 24.0114, 29.5384, 17.1079, 17.0941, 16.7121, 14.9714, 20.4644,\n",
            "        23.7022, 18.5014, 18.9148, 23.4049, 21.9270, 25.0294, 31.8473, 26.5410],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "trained_net = train(net, criterion, optimizer, 1, train_loader, val_loader)"
      ],
      "id": "8b7f8cdf"
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "5tNsSuYAlA2Q"
      },
      "outputs": [],
      "source": [
        "trained_net = net"
      ],
      "id": "5tNsSuYAlA2Q"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LEvesrMqQac",
        "outputId": "ca7ed36a-4413-4fe9-cf3c-f6efeb617d9f"
      },
      "id": "1LEvesrMqQac",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): TripletNet(\n",
              "    (embeddingnet): EmbeddingNet(\n",
              "      (features): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "        (4): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      )\n",
              "      (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# load model from checkpoint\n",
        "model_loaded = TripletNet(FeatureExtractNET())\n",
        "checkpoint = torch.load('/content/drive/MyDrive/model_epoch_0.pt')\n",
        "model_loaded.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "\n",
        "model_loaded.to(device)\n",
        "model_loaded.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uUh7Yk99ecP",
        "outputId": "5e8ccbde-772b-47c7-8ce7-3b0dd16f56c5"
      },
      "id": "8uUh7Yk99ecP",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TripletNet(\n",
              "  (embeddingnet): EmbeddingNet(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (4): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (5): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (6): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (7): Sequential(\n",
              "        (0): BasicBlock(\n",
              "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): BasicBlock(\n",
              "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    )\n",
              "    (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFZPwoPglJFH",
        "outputId": "1ca57478-fcd0-4595-9ca0-63dda0f76269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dist a: tensor([14.7346, 18.2229, 17.9004, 13.7015, 17.1378, 16.7035, 19.6870, 14.1521,\n",
            "        16.5130, 18.5596, 21.3687, 19.8480, 21.9997, 14.3533, 16.9440, 18.4767,\n",
            "        22.9334, 16.3721, 16.1076, 17.8330, 17.4193, 12.1379, 13.8147, 18.4836,\n",
            "        18.2759, 22.5580, 14.2527, 19.8612, 18.2120, 16.9553, 11.7606, 14.0963,\n",
            "        14.5604, 14.7805, 22.5439, 15.8438, 11.5091, 15.8298, 15.2355, 15.0780,\n",
            "        16.2959, 17.9183, 17.3548, 13.3215, 25.3425, 11.0275, 19.2948, 23.6729,\n",
            "        16.2255, 10.3209, 25.5373, 17.3512, 25.1248, 14.6714, 25.6110, 14.3367,\n",
            "        14.2538, 22.0254, 23.9481, 14.3851, 16.0434, 16.8749, 23.4274, 21.3770],\n",
            "       device='cuda:0'), dist b: tensor([31.9620, 22.1136, 27.9829, 14.0639, 20.7262, 23.3005, 23.3759, 17.6575,\n",
            "        25.4295, 28.7724, 16.4912, 19.6447, 23.2085, 19.5990, 23.3683, 20.1920,\n",
            "        25.5814, 31.0055, 18.6461, 24.2609, 25.0172, 26.5410, 19.5112, 22.7975,\n",
            "        17.5284, 22.9672, 17.2604, 22.3534, 16.1531, 23.8159, 20.9159, 14.6986,\n",
            "        27.6846, 27.2880, 25.0305, 16.2532, 22.5841, 11.8464, 23.0999, 22.4705,\n",
            "        14.3568, 20.1980, 16.0761, 20.0645, 21.1979, 22.7497, 27.1880, 25.4056,\n",
            "        20.5065, 24.0114, 29.5384, 17.1079, 17.0941, 16.7121, 14.9714, 20.4644,\n",
            "        23.7022, 18.5014, 18.9148, 23.4049, 21.9270, 25.0294, 31.8473, 26.5410],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n"
          ]
        }
      ],
      "source": [
        "valloader_iterator = iter(val_loader)\n",
        "mean_accuracy = val_accuracy(trained_net, val_loader, valloader_iterator)\n",
        "print(mean_accuracy)"
      ],
      "id": "dFZPwoPglJFH"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iB9aCp_jT2OH",
        "outputId": "dc4544a0-0282-486e-9db9-80abec106331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START PREDICTION ==================\n",
            "batch:  0\n",
            "batch:  1\n",
            "batch:  2\n",
            "batch:  3\n",
            "batch:  4\n",
            "batch:  5\n",
            "batch:  6\n",
            "batch:  7\n",
            "batch:  8\n",
            "batch:  9\n",
            "batch:  10\n",
            "batch:  11\n",
            "batch:  12\n",
            "batch:  13\n",
            "batch:  14\n",
            "batch:  15\n",
            "batch:  16\n",
            "batch:  17\n",
            "batch:  18\n",
            "batch:  19\n",
            "batch:  20\n",
            "batch:  21\n",
            "batch:  22\n",
            "batch:  23\n",
            "batch:  24\n",
            "batch:  25\n",
            "batch:  26\n",
            "batch:  27\n",
            "batch:  28\n",
            "batch:  29\n",
            "batch:  30\n",
            "batch:  31\n",
            "batch:  32\n",
            "batch:  33\n",
            "batch:  34\n",
            "batch:  35\n",
            "batch:  36\n",
            "batch:  37\n",
            "batch:  38\n",
            "batch:  39\n",
            "batch:  40\n",
            "batch:  41\n",
            "batch:  42\n",
            "batch:  43\n",
            "batch:  44\n",
            "batch:  45\n",
            "batch:  46\n",
            "batch:  47\n",
            "batch:  48\n",
            "batch:  49\n",
            "batch:  50\n",
            "batch:  51\n",
            "batch:  52\n",
            "batch:  53\n",
            "batch:  54\n",
            "batch:  55\n",
            "batch:  56\n",
            "batch:  57\n",
            "batch:  58\n",
            "batch:  59\n",
            "batch:  60\n",
            "batch:  61\n",
            "batch:  62\n",
            "batch:  63\n",
            "batch:  64\n",
            "batch:  65\n",
            "batch:  66\n",
            "batch:  67\n",
            "batch:  68\n",
            "batch:  69\n",
            "batch:  70\n",
            "batch:  71\n",
            "batch:  72\n",
            "batch:  73\n",
            "batch:  74\n",
            "batch:  75\n",
            "batch:  76\n",
            "batch:  77\n",
            "batch:  78\n",
            "batch:  79\n",
            "batch:  80\n",
            "batch:  81\n",
            "batch:  82\n",
            "batch:  83\n",
            "batch:  84\n",
            "batch:  85\n",
            "batch:  86\n",
            "batch:  87\n",
            "batch:  88\n",
            "batch:  89\n",
            "batch:  90\n",
            "batch:  91\n",
            "batch:  92\n",
            "batch:  93\n",
            "batch:  94\n",
            "batch:  95\n",
            "batch:  96\n",
            "batch:  97\n",
            "batch:  98\n",
            "batch:  99\n",
            "batch:  100\n",
            "batch:  101\n",
            "batch:  102\n",
            "batch:  103\n",
            "batch:  104\n",
            "batch:  105\n",
            "batch:  106\n",
            "batch:  107\n",
            "batch:  108\n",
            "batch:  109\n",
            "batch:  110\n",
            "batch:  111\n",
            "batch:  112\n",
            "batch:  113\n",
            "batch:  114\n",
            "batch:  115\n",
            "batch:  116\n",
            "batch:  117\n",
            "batch:  118\n",
            "batch:  119\n",
            "batch:  120\n",
            "batch:  121\n",
            "batch:  122\n",
            "batch:  123\n",
            "batch:  124\n",
            "batch:  125\n",
            "batch:  126\n",
            "batch:  127\n",
            "batch:  128\n",
            "batch:  129\n",
            "batch:  130\n",
            "batch:  131\n",
            "batch:  132\n",
            "batch:  133\n",
            "batch:  134\n",
            "batch:  135\n",
            "batch:  136\n",
            "batch:  137\n",
            "batch:  138\n",
            "batch:  139\n",
            "batch:  140\n",
            "batch:  141\n",
            "batch:  142\n",
            "batch:  143\n",
            "batch:  144\n",
            "batch:  145\n",
            "batch:  146\n",
            "batch:  147\n",
            "batch:  148\n",
            "batch:  149\n",
            "batch:  150\n",
            "batch:  151\n",
            "batch:  152\n",
            "batch:  153\n",
            "batch:  154\n",
            "batch:  155\n",
            "batch:  156\n",
            "batch:  157\n",
            "batch:  158\n",
            "batch:  159\n",
            "batch:  160\n",
            "batch:  161\n",
            "batch:  162\n",
            "batch:  163\n",
            "batch:  164\n",
            "batch:  165\n",
            "batch:  166\n",
            "batch:  167\n",
            "batch:  168\n",
            "batch:  169\n",
            "batch:  170\n",
            "batch:  171\n",
            "batch:  172\n",
            "batch:  173\n",
            "batch:  174\n",
            "batch:  175\n",
            "batch:  176\n",
            "batch:  177\n",
            "batch:  178\n",
            "batch:  179\n",
            "batch:  180\n",
            "batch:  181\n",
            "batch:  182\n",
            "batch:  183\n",
            "batch:  184\n",
            "batch:  185\n",
            "batch:  186\n",
            "batch:  187\n",
            "batch:  188\n",
            "batch:  189\n",
            "batch:  190\n",
            "batch:  191\n",
            "batch:  192\n",
            "batch:  193\n",
            "batch:  194\n",
            "batch:  195\n",
            "batch:  196\n",
            "batch:  197\n",
            "batch:  198\n",
            "batch:  199\n",
            "batch:  200\n",
            "batch:  201\n",
            "batch:  202\n",
            "batch:  203\n",
            "batch:  204\n",
            "batch:  205\n",
            "batch:  206\n",
            "batch:  207\n",
            "batch:  208\n",
            "batch:  209\n",
            "batch:  210\n",
            "batch:  211\n",
            "batch:  212\n",
            "batch:  213\n",
            "batch:  214\n",
            "batch:  215\n",
            "batch:  216\n",
            "batch:  217\n",
            "batch:  218\n",
            "batch:  219\n",
            "batch:  220\n",
            "batch:  221\n",
            "batch:  222\n",
            "batch:  223\n",
            "batch:  224\n",
            "batch:  225\n",
            "batch:  226\n",
            "batch:  227\n",
            "batch:  228\n",
            "batch:  229\n",
            "batch:  230\n",
            "batch:  231\n",
            "batch:  232\n",
            "batch:  233\n",
            "batch:  234\n",
            "batch:  235\n",
            "batch:  236\n",
            "batch:  237\n",
            "batch:  238\n",
            "batch:  239\n",
            "batch:  240\n",
            "batch:  241\n",
            "batch:  242\n",
            "batch:  243\n",
            "batch:  244\n",
            "batch:  245\n",
            "batch:  246\n",
            "batch:  247\n",
            "batch:  248\n",
            "batch:  249\n",
            "batch:  250\n",
            "batch:  251\n",
            "batch:  252\n",
            "batch:  253\n",
            "batch:  254\n",
            "batch:  255\n",
            "batch:  256\n",
            "batch:  257\n",
            "batch:  258\n",
            "batch:  259\n",
            "batch:  260\n",
            "batch:  261\n",
            "batch:  262\n",
            "batch:  263\n",
            "batch:  264\n",
            "batch:  265\n",
            "batch:  266\n",
            "batch:  267\n",
            "batch:  268\n",
            "batch:  269\n",
            "batch:  270\n",
            "batch:  271\n",
            "batch:  272\n",
            "batch:  273\n",
            "batch:  274\n",
            "batch:  275\n",
            "batch:  276\n",
            "batch:  277\n",
            "batch:  278\n",
            "batch:  279\n",
            "batch:  280\n",
            "batch:  281\n",
            "batch:  282\n",
            "batch:  283\n",
            "batch:  284\n",
            "batch:  285\n",
            "batch:  286\n",
            "batch:  287\n",
            "batch:  288\n",
            "batch:  289\n",
            "batch:  290\n",
            "batch:  291\n",
            "batch:  292\n",
            "batch:  293\n",
            "batch:  294\n",
            "batch:  295\n",
            "batch:  296\n",
            "batch:  297\n",
            "batch:  298\n",
            "batch:  299\n",
            "batch:  300\n",
            "batch:  301\n",
            "batch:  302\n",
            "batch:  303\n",
            "batch:  304\n",
            "batch:  305\n",
            "batch:  306\n",
            "batch:  307\n",
            "batch:  308\n",
            "batch:  309\n",
            "batch:  310\n",
            "batch:  311\n",
            "batch:  312\n",
            "batch:  313\n",
            "batch:  314\n",
            "batch:  315\n",
            "batch:  316\n",
            "batch:  317\n",
            "batch:  318\n",
            "batch:  319\n",
            "batch:  320\n",
            "batch:  321\n",
            "batch:  322\n",
            "batch:  323\n",
            "batch:  324\n",
            "batch:  325\n",
            "batch:  326\n",
            "batch:  327\n",
            "batch:  328\n",
            "batch:  329\n",
            "batch:  330\n",
            "batch:  331\n",
            "batch:  332\n",
            "batch:  333\n",
            "batch:  334\n",
            "batch:  335\n",
            "batch:  336\n",
            "batch:  337\n",
            "batch:  338\n",
            "batch:  339\n",
            "batch:  340\n",
            "batch:  341\n",
            "batch:  342\n",
            "batch:  343\n",
            "batch:  344\n",
            "batch:  345\n",
            "batch:  346\n",
            "batch:  347\n",
            "batch:  348\n",
            "batch:  349\n",
            "batch:  350\n",
            "batch:  351\n",
            "batch:  352\n",
            "batch:  353\n",
            "batch:  354\n",
            "batch:  355\n",
            "batch:  356\n",
            "batch:  357\n",
            "batch:  358\n",
            "batch:  359\n",
            "batch:  360\n",
            "batch:  361\n",
            "batch:  362\n",
            "batch:  363\n",
            "batch:  364\n",
            "batch:  365\n",
            "batch:  366\n",
            "batch:  367\n",
            "batch:  368\n",
            "batch:  369\n",
            "batch:  370\n",
            "batch:  371\n",
            "batch:  372\n",
            "batch:  373\n",
            "batch:  374\n",
            "batch:  375\n",
            "batch:  376\n",
            "batch:  377\n",
            "batch:  378\n",
            "batch:  379\n",
            "batch:  380\n",
            "batch:  381\n",
            "batch:  382\n",
            "batch:  383\n",
            "batch:  384\n",
            "batch:  385\n",
            "batch:  386\n",
            "batch:  387\n",
            "batch:  388\n",
            "batch:  389\n",
            "batch:  390\n",
            "batch:  391\n",
            "batch:  392\n",
            "batch:  393\n",
            "batch:  394\n",
            "batch:  395\n",
            "batch:  396\n",
            "batch:  397\n",
            "batch:  398\n",
            "batch:  399\n",
            "batch:  400\n",
            "batch:  401\n",
            "batch:  402\n",
            "batch:  403\n",
            "batch:  404\n",
            "batch:  405\n",
            "batch:  406\n",
            "batch:  407\n",
            "batch:  408\n",
            "batch:  409\n",
            "batch:  410\n",
            "batch:  411\n",
            "batch:  412\n",
            "batch:  413\n",
            "batch:  414\n",
            "batch:  415\n",
            "batch:  416\n",
            "batch:  417\n",
            "batch:  418\n",
            "batch:  419\n",
            "batch:  420\n",
            "batch:  421\n",
            "batch:  422\n",
            "batch:  423\n",
            "batch:  424\n",
            "batch:  425\n",
            "batch:  426\n",
            "batch:  427\n",
            "batch:  428\n",
            "batch:  429\n",
            "batch:  430\n",
            "batch:  431\n",
            "batch:  432\n",
            "batch:  433\n",
            "batch:  434\n",
            "batch:  435\n",
            "batch:  436\n",
            "batch:  437\n",
            "batch:  438\n",
            "batch:  439\n",
            "batch:  440\n",
            "batch:  441\n",
            "batch:  442\n",
            "batch:  443\n",
            "batch:  444\n",
            "batch:  445\n",
            "batch:  446\n",
            "batch:  447\n",
            "batch:  448\n",
            "batch:  449\n",
            "batch:  450\n",
            "batch:  451\n",
            "batch:  452\n",
            "batch:  453\n",
            "batch:  454\n",
            "batch:  455\n",
            "batch:  456\n",
            "batch:  457\n",
            "batch:  458\n",
            "batch:  459\n",
            "batch:  460\n",
            "batch:  461\n",
            "batch:  462\n",
            "batch:  463\n",
            "batch:  464\n",
            "batch:  465\n",
            "batch:  466\n",
            "batch:  467\n",
            "batch:  468\n",
            "batch:  469\n",
            "batch:  470\n",
            "batch:  471\n",
            "batch:  472\n",
            "batch:  473\n",
            "batch:  474\n",
            "batch:  475\n",
            "batch:  476\n",
            "batch:  477\n",
            "batch:  478\n",
            "batch:  479\n",
            "batch:  480\n",
            "batch:  481\n",
            "batch:  482\n",
            "batch:  483\n",
            "batch:  484\n",
            "batch:  485\n",
            "batch:  486\n",
            "batch:  487\n",
            "batch:  488\n",
            "batch:  489\n",
            "batch:  490\n",
            "batch:  491\n",
            "batch:  492\n",
            "batch:  493\n",
            "batch:  494\n",
            "batch:  495\n",
            "batch:  496\n",
            "batch:  497\n",
            "batch:  498\n",
            "batch:  499\n",
            "batch:  500\n",
            "batch:  501\n",
            "batch:  502\n",
            "batch:  503\n",
            "batch:  504\n",
            "batch:  505\n",
            "batch:  506\n",
            "batch:  507\n",
            "batch:  508\n",
            "batch:  509\n",
            "batch:  510\n",
            "batch:  511\n",
            "batch:  512\n",
            "batch:  513\n",
            "batch:  514\n",
            "batch:  515\n",
            "batch:  516\n",
            "batch:  517\n",
            "batch:  518\n",
            "batch:  519\n",
            "batch:  520\n",
            "batch:  521\n",
            "batch:  522\n",
            "batch:  523\n",
            "batch:  524\n",
            "batch:  525\n",
            "batch:  526\n",
            "batch:  527\n",
            "batch:  528\n",
            "batch:  529\n",
            "batch:  530\n",
            "batch:  531\n",
            "batch:  532\n",
            "batch:  533\n",
            "batch:  534\n",
            "batch:  535\n",
            "batch:  536\n",
            "batch:  537\n",
            "batch:  538\n",
            "batch:  539\n",
            "batch:  540\n",
            "batch:  541\n",
            "batch:  542\n",
            "batch:  543\n",
            "batch:  544\n",
            "batch:  545\n",
            "batch:  546\n",
            "batch:  547\n",
            "batch:  548\n",
            "batch:  549\n",
            "batch:  550\n",
            "batch:  551\n",
            "batch:  552\n",
            "batch:  553\n",
            "batch:  554\n",
            "batch:  555\n",
            "batch:  556\n",
            "batch:  557\n",
            "batch:  558\n",
            "batch:  559\n",
            "batch:  560\n",
            "batch:  561\n",
            "batch:  562\n",
            "batch:  563\n",
            "batch:  564\n",
            "batch:  565\n",
            "batch:  566\n",
            "batch:  567\n",
            "batch:  568\n",
            "batch:  569\n",
            "batch:  570\n",
            "batch:  571\n",
            "batch:  572\n",
            "batch:  573\n",
            "batch:  574\n",
            "batch:  575\n",
            "batch:  576\n",
            "batch:  577\n",
            "batch:  578\n",
            "batch:  579\n",
            "batch:  580\n",
            "batch:  581\n",
            "batch:  582\n",
            "batch:  583\n",
            "batch:  584\n",
            "batch:  585\n",
            "batch:  586\n",
            "batch:  587\n",
            "batch:  588\n",
            "batch:  589\n",
            "batch:  590\n",
            "batch:  591\n",
            "batch:  592\n",
            "batch:  593\n",
            "batch:  594\n",
            "batch:  595\n",
            "batch:  596\n",
            "batch:  597\n",
            "batch:  598\n",
            "batch:  599\n",
            "batch:  600\n",
            "batch:  601\n",
            "batch:  602\n",
            "batch:  603\n",
            "batch:  604\n",
            "batch:  605\n",
            "batch:  606\n",
            "batch:  607\n",
            "batch:  608\n",
            "batch:  609\n",
            "batch:  610\n",
            "batch:  611\n",
            "batch:  612\n",
            "batch:  613\n",
            "batch:  614\n",
            "batch:  615\n",
            "batch:  616\n",
            "batch:  617\n",
            "batch:  618\n",
            "batch:  619\n",
            "batch:  620\n",
            "batch:  621\n",
            "batch:  622\n",
            "batch:  623\n",
            "batch:  624\n",
            "batch:  625\n",
            "batch:  626\n",
            "batch:  627\n",
            "batch:  628\n",
            "batch:  629\n",
            "batch:  630\n",
            "batch:  631\n",
            "batch:  632\n",
            "batch:  633\n",
            "batch:  634\n",
            "batch:  635\n",
            "batch:  636\n",
            "batch:  637\n",
            "batch:  638\n",
            "batch:  639\n",
            "batch:  640\n",
            "batch:  641\n",
            "batch:  642\n",
            "batch:  643\n",
            "batch:  644\n",
            "batch:  645\n",
            "batch:  646\n",
            "batch:  647\n",
            "batch:  648\n",
            "batch:  649\n",
            "batch:  650\n",
            "batch:  651\n",
            "batch:  652\n",
            "batch:  653\n",
            "batch:  654\n",
            "batch:  655\n",
            "batch:  656\n",
            "batch:  657\n",
            "batch:  658\n",
            "batch:  659\n",
            "batch:  660\n",
            "batch:  661\n",
            "batch:  662\n",
            "batch:  663\n",
            "batch:  664\n",
            "batch:  665\n",
            "batch:  666\n",
            "batch:  667\n",
            "batch:  668\n",
            "batch:  669\n",
            "batch:  670\n",
            "batch:  671\n",
            "batch:  672\n",
            "batch:  673\n",
            "batch:  674\n",
            "batch:  675\n",
            "batch:  676\n",
            "batch:  677\n",
            "batch:  678\n",
            "batch:  679\n",
            "batch:  680\n",
            "batch:  681\n",
            "batch:  682\n",
            "batch:  683\n",
            "batch:  684\n",
            "batch:  685\n",
            "batch:  686\n",
            "batch:  687\n",
            "batch:  688\n",
            "batch:  689\n",
            "batch:  690\n",
            "batch:  691\n",
            "batch:  692\n",
            "batch:  693\n",
            "batch:  694\n",
            "batch:  695\n",
            "batch:  696\n",
            "batch:  697\n",
            "batch:  698\n",
            "batch:  699\n",
            "batch:  700\n",
            "batch:  701\n",
            "batch:  702\n",
            "batch:  703\n",
            "batch:  704\n",
            "batch:  705\n",
            "batch:  706\n",
            "batch:  707\n",
            "batch:  708\n",
            "batch:  709\n",
            "batch:  710\n",
            "batch:  711\n",
            "batch:  712\n",
            "batch:  713\n",
            "batch:  714\n",
            "batch:  715\n",
            "batch:  716\n",
            "batch:  717\n",
            "batch:  718\n",
            "batch:  719\n",
            "batch:  720\n",
            "batch:  721\n",
            "batch:  722\n",
            "batch:  723\n",
            "batch:  724\n",
            "batch:  725\n",
            "batch:  726\n",
            "batch:  727\n",
            "batch:  728\n",
            "batch:  729\n",
            "batch:  730\n",
            "batch:  731\n",
            "batch:  732\n",
            "batch:  733\n",
            "batch:  734\n",
            "batch:  735\n",
            "batch:  736\n",
            "batch:  737\n",
            "batch:  738\n",
            "batch:  739\n",
            "batch:  740\n",
            "batch:  741\n",
            "batch:  742\n",
            "batch:  743\n",
            "batch:  744\n",
            "batch:  745\n",
            "batch:  746\n",
            "batch:  747\n",
            "batch:  748\n",
            "batch:  749\n",
            "batch:  750\n",
            "batch:  751\n",
            "batch:  752\n",
            "batch:  753\n",
            "batch:  754\n",
            "batch:  755\n",
            "batch:  756\n",
            "batch:  757\n",
            "batch:  758\n",
            "batch:  759\n",
            "batch:  760\n",
            "batch:  761\n",
            "batch:  762\n",
            "batch:  763\n",
            "batch:  764\n",
            "batch:  765\n",
            "batch:  766\n",
            "batch:  767\n",
            "batch:  768\n",
            "batch:  769\n",
            "batch:  770\n",
            "batch:  771\n",
            "batch:  772\n",
            "batch:  773\n",
            "batch:  774\n",
            "batch:  775\n",
            "batch:  776\n",
            "batch:  777\n",
            "batch:  778\n",
            "batch:  779\n",
            "batch:  780\n",
            "batch:  781\n",
            "batch:  782\n",
            "batch:  783\n",
            "batch:  784\n",
            "batch:  785\n",
            "batch:  786\n",
            "batch:  787\n",
            "batch:  788\n",
            "batch:  789\n",
            "batch:  790\n",
            "batch:  791\n",
            "batch:  792\n",
            "batch:  793\n",
            "batch:  794\n",
            "batch:  795\n",
            "batch:  796\n",
            "batch:  797\n",
            "batch:  798\n",
            "batch:  799\n",
            "batch:  800\n",
            "batch:  801\n",
            "batch:  802\n",
            "batch:  803\n",
            "batch:  804\n",
            "batch:  805\n",
            "batch:  806\n",
            "batch:  807\n",
            "batch:  808\n",
            "batch:  809\n",
            "batch:  810\n",
            "batch:  811\n",
            "batch:  812\n",
            "batch:  813\n",
            "batch:  814\n",
            "batch:  815\n",
            "batch:  816\n",
            "batch:  817\n",
            "batch:  818\n",
            "batch:  819\n",
            "batch:  820\n",
            "batch:  821\n",
            "batch:  822\n",
            "batch:  823\n",
            "batch:  824\n",
            "batch:  825\n",
            "batch:  826\n",
            "batch:  827\n",
            "batch:  828\n",
            "batch:  829\n",
            "batch:  830\n",
            "batch:  831\n",
            "batch:  832\n",
            "batch:  833\n",
            "batch:  834\n",
            "batch:  835\n",
            "batch:  836\n",
            "batch:  837\n",
            "batch:  838\n",
            "batch:  839\n",
            "batch:  840\n",
            "batch:  841\n",
            "batch:  842\n",
            "batch:  843\n",
            "batch:  844\n",
            "batch:  845\n",
            "batch:  846\n",
            "batch:  847\n",
            "batch:  848\n",
            "batch:  849\n",
            "batch:  850\n",
            "batch:  851\n",
            "batch:  852\n",
            "batch:  853\n",
            "batch:  854\n",
            "batch:  855\n",
            "batch:  856\n",
            "batch:  857\n",
            "batch:  858\n",
            "batch:  859\n",
            "batch:  860\n",
            "batch:  861\n",
            "batch:  862\n",
            "batch:  863\n",
            "batch:  864\n",
            "batch:  865\n",
            "batch:  866\n",
            "batch:  867\n",
            "batch:  868\n",
            "batch:  869\n",
            "batch:  870\n",
            "batch:  871\n",
            "batch:  872\n",
            "batch:  873\n",
            "batch:  874\n",
            "batch:  875\n",
            "batch:  876\n",
            "batch:  877\n",
            "batch:  878\n",
            "batch:  879\n",
            "batch:  880\n",
            "batch:  881\n",
            "batch:  882\n",
            "batch:  883\n",
            "batch:  884\n",
            "batch:  885\n",
            "batch:  886\n",
            "batch:  887\n",
            "batch:  888\n",
            "batch:  889\n",
            "batch:  890\n",
            "batch:  891\n",
            "batch:  892\n",
            "batch:  893\n",
            "batch:  894\n",
            "batch:  895\n",
            "batch:  896\n",
            "batch:  897\n",
            "batch:  898\n",
            "batch:  899\n",
            "batch:  900\n",
            "batch:  901\n",
            "batch:  902\n",
            "batch:  903\n",
            "batch:  904\n",
            "batch:  905\n",
            "batch:  906\n",
            "batch:  907\n",
            "batch:  908\n",
            "batch:  909\n",
            "batch:  910\n",
            "batch:  911\n",
            "batch:  912\n",
            "batch:  913\n",
            "batch:  914\n",
            "batch:  915\n",
            "batch:  916\n",
            "batch:  917\n",
            "batch:  918\n",
            "batch:  919\n",
            "batch:  920\n",
            "batch:  921\n",
            "batch:  922\n",
            "batch:  923\n",
            "batch:  924\n",
            "batch:  925\n",
            "batch:  926\n",
            "batch:  927\n",
            "batch:  928\n",
            "batch:  929\n",
            "batch:  930\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-51f78ee7ee0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0m_arrays_for_stack_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ],
      "source": [
        "print('================== START PREDICTION ==================')\n",
        "\n",
        "# Change to evaluation mode\n",
        "model_loaded.eval()\n",
        "\n",
        "redicted_labels = np.zeros(59544)\n",
        "pred_test=[]\n",
        "\n",
        "#Predict labels 1 or 0 for each test triplet\n",
        "for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
        "\n",
        "    data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "    # wrap in torch.autograd.Variable\n",
        "    data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # compute output and loss\n",
        "        embedded_x, embedded_y, embedded_z = trained_net(data1, data2, data3)\n",
        "\n",
        "    dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "    dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "    #print(np.squeeze(embedded_a.cpu().detach().numpy()).shape)\n",
        "    \n",
        "\n",
        "    pred_test.append(1*(dist_a <= dist_b))\n",
        "\n",
        "\n",
        "    print('batch: ', batch_idx)\n",
        "\n",
        "\n",
        "predicted_labels = np.hstack(pred_test)\n",
        "print(predicted_labels)\n",
        "\n",
        "#Write submisison file\n",
        "df = pd.DataFrame(predicted_labels)\n",
        "df.to_csv('/content/drive/MyDrive/test2/submission_1epoch.txt', index=False, header=None) #write CSV"
      ],
      "id": "iB9aCp_jT2OH"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_np = []\n",
        "for i in range(len(pred_test)):\n",
        "  pred_test_cpu = pred_test[i].cpu().detach().numpy()\n",
        "  pred_test_np += list(pred_test_cpu)\n",
        "len(pred_test_np)\n",
        "predicted_labels = np.hstack(pred_test_np)\n",
        "print(predicted_labels)\n",
        "\n",
        "#Write submisison file\n",
        "df = pd.DataFrame(predicted_labels)\n",
        "df.to_csv('/content/drive/MyDrive/test2/submission_1epoch.txt', index=False, header=None) #write CSV"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqMINu0rwiv2",
        "outputId": "4d7d157f-e0f4-4fba-bd9f-01917c59b210"
      },
      "id": "eqMINu0rwiv2",
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = len(open(\"/content/drive/MyDrive/test2/submission_1epoch.txt\",'rU').readlines())\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc5c6182Do0U",
        "outputId": "754ed929-ac66-415c-dc1c-0745b01d469b"
      },
      "id": "rc5c6182Do0U",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: 'U' mode is deprecated\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nq4tku-jQRuh"
      },
      "id": "nq4tku-jQRuh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Task 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}