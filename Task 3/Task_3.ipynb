{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVAaWUZmJuEf",
        "outputId": "b91af99f-6f64-40e8-cf88-dee64c7b7c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "BVAaWUZmJuEf"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9cafface"
      },
      "outputs": [],
      "source": [
        "# Solve the imshow dead kernel problem\n",
        "import os    \n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "id": "9cafface"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc461e5c",
        "outputId": "20aba5f8-b892-4c97-bbcb-ebe0e19cc0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START LOADING DATA ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start loading the data\n",
        "'''\n",
        "print('================== START LOADING DATA ==================')"
      ],
      "id": "bc461e5c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aa85839b"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "aa85839b"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e8218f58"
      },
      "outputs": [],
      "source": [
        "# Split the training set into a 80% training set and 20% validation set\n",
        "import random\n",
        "\n",
        "def split_huge_file(file,out1,out2,percentage=0.75,seed=2022):\n",
        "    \"\"\"Splits a file in 2 given the approximate `percentage` to go in the large file.\"\"\"\n",
        "    random.seed(seed)\n",
        "    with open(file, 'r',encoding=\"utf-8\") as fin, \\\n",
        "         open(out1, 'w') as foutBig, \\\n",
        "         open(out2, 'w') as foutSmall:\n",
        "\n",
        "        for line in fin:\n",
        "            r = random.random() \n",
        "            if r < percentage:\n",
        "                foutBig.write(line)\n",
        "            else:\n",
        "                foutSmall.write(line)"
      ],
      "id": "e8218f58"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7a883b1c"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/My Drive/'\n",
        "split_huge_file(os.path.join(path, f'train_triplets.txt'), 'train_triplets_splits.txt', 'val_triplets_splits.txt', percentage=0.99, seed=2022)"
      ],
      "id": "7a883b1c"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "RWybssqKbO2N"
      },
      "id": "RWybssqKbO2N",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhLDDexX2vpg",
        "outputId": "30648088-bd6d-4c32-cc12-86fd548b7233"
      },
      "id": "WhLDDexX2vpg",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May  4 20:49:27 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8aea5632"
      },
      "outputs": [],
      "source": [
        "# Image loader helper function\n",
        "def default_image_loader(path):\n",
        "    return Image.open(path).convert('RGB')"
      ],
      "id": "8aea5632"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0fd6fd88"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "im = Image.open(r\"/content/drive/My Drive/food/00003.jpg\")"
      ],
      "id": "0fd6fd88"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cbb865c1"
      },
      "outputs": [],
      "source": [
        "# display(im)"
      ],
      "id": "cbb865c1"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9407678a",
        "outputId": "69c63df5-a41b-4921-fcff-58deee383862"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(329, 468, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data = np.asarray(im)\n",
        "data.shape"
      ],
      "id": "9407678a"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7960455c"
      },
      "outputs": [],
      "source": [
        "# im.resize((354,242))"
      ],
      "id": "7960455c"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "33fe9cfd"
      },
      "outputs": [],
      "source": [
        "class TripletImageLoader(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_path, triplets_file_name, transform=None, loader=default_image_loader):\n",
        "        \"\"\" base_path: The path contains the text file of the training triplets\n",
        "            triplets_file_name: The text file with each line containing three integers, \n",
        "            where integer i refers to the i-th image in the filenames file.  \n",
        "            Each line contains three integers (a triplet).\n",
        "            For example, the triplet \"00723 00478 02630\" denotes that the dish in image \"00723.jpg\" is more similar in taste \n",
        "            to the dish in image \"00478.jpg\" than to the dish in image \"02630.jpg\" according to a human annotator.\n",
        "         \"\"\"\n",
        "        self.base_path = base_path  \n",
        "        triplets = []\n",
        "        for line in open(triplets_file_name):\n",
        "            triplets.append((line.split()[0], line.split()[1], line.split()[2])) # anchor, positive, negative\n",
        "        self.triplets = triplets\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path1, path2, path3 = self.triplets[index]\n",
        "        img1 = self.loader(os.path.join(self.base_path, f'{path1}.jpg'))\n",
        "        img2 = self.loader(os.path.join(self.base_path, f'{path2}.jpg'))\n",
        "        img3 = self.loader(os.path.join(self.base_path, f'{path3}.jpg'))\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "\n",
        "        return img1, img2, img3\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)"
      ],
      "id": "33fe9cfd"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "38209c7b"
      },
      "outputs": [],
      "source": [
        "# Initialization: importing the packages that we will use\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Google colab offers time limited use of GPU for free\n",
        "\n",
        "################# Configuration  ######################\n",
        "IMAGE_SIZE = (242, 354) # bigger image size improves performance but makes training slower.\n",
        "\n",
        "# Training parameters \n",
        "BATCH_SIZE = 64"
      ],
      "id": "38209c7b"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9974abbb",
        "outputId": "0353b347-ca71-494b-a7cb-100678386b70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ],
      "id": "9974abbb"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zS2MM05sNKp-",
        "outputId": "09d40669-d8b9-47fc-89db-f037f89e6ea8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "device"
      ],
      "id": "zS2MM05sNKp-"
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/test_triplets.txt /content"
      ],
      "metadata": {
        "id": "Ay-2AUvx-Hyu"
      },
      "id": "Ay-2AUvx-Hyu",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3b63d3d1"
      },
      "outputs": [],
      "source": [
        "# Dataset and Trasformations\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "############# Datasets and Dataloaders ################\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(), # The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(p=0.5), # we want our network to be robust over geometrical transformations that leave the image semantically invariant\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), #  We transform them to Tensors of normalized range [-1, 1].\n",
        "    # (mean, mean, mean) , (std, std, std): output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "path = '/content/drive/MyDrive/food'\n",
        "train_dataset = TripletImageLoader(path.rstrip('\\n'), 'train_triplets_splits.txt', transform=transform_train)\n",
        "val_dataset = TripletImageLoader(path.rstrip('\\n'), 'val_triplets_splits.txt', transform=transform_val)\n",
        "test_dataset = TripletImageLoader(path.rstrip('\\n'), 'test_triplets.txt', transform=transform_test)"
      ],
      "id": "3b63d3d1"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ff181f",
        "outputId": "aff7dd27-9f98-4b60-d85d-b092c5c991c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58933, 582, 59544)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "id": "a7ff181f"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66669e53",
        "outputId": "1151ff7b-0dd8-4a37-9b6f-f4c62b1317b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.6848,  0.6188,  0.6295,  ...,  0.6845,  0.6174,  0.5918],\n",
              "          [ 0.7634,  0.6081,  0.6502,  ...,  0.7868,  0.7373,  0.6763],\n",
              "          [ 0.7259,  0.7257,  0.6671,  ...,  0.7282,  0.7121,  0.6763],\n",
              "          ...,\n",
              "          [ 0.1368,  0.1779,  0.1858,  ...,  0.5491,  0.5174,  0.5192],\n",
              "          [ 0.1719,  0.1433,  0.1679,  ...,  0.5418,  0.5576,  0.5111],\n",
              "          [ 0.2298,  0.0134,  0.0636,  ...,  0.6480,  0.7295,  0.7176]],\n",
              " \n",
              "         [[ 0.3083,  0.2423,  0.2557,  ...,  0.2424,  0.3027,  0.3446],\n",
              "          [ 0.3778,  0.2235,  0.2622,  ...,  0.3537,  0.3888,  0.3711],\n",
              "          [ 0.3208,  0.3215,  0.2648,  ...,  0.3097,  0.3167,  0.2928],\n",
              "          ...,\n",
              "          [-0.2550, -0.2139, -0.2107,  ...,  0.0953,  0.1322,  0.1737],\n",
              "          [-0.2046, -0.2332, -0.2138,  ...,  0.0879,  0.1528,  0.1360],\n",
              "          [-0.1141, -0.3449, -0.3147,  ...,  0.1256,  0.0728, -0.0351]],\n",
              " \n",
              "         [[ 0.1512,  0.0762,  0.0626,  ..., -0.0076, -0.0635, -0.0872],\n",
              "          [ 0.2222,  0.0503,  0.0729,  ...,  0.0857,  0.1187,  0.0999],\n",
              "          [ 0.1609,  0.1532,  0.0806,  ...,  0.0070,  0.1151,  0.1430],\n",
              "          ...,\n",
              "          [-0.6279, -0.5769, -0.5614,  ..., -0.1400, -0.3230, -0.3943],\n",
              "          [-0.5889, -0.6165, -0.5872,  ..., -0.1474, -0.1789, -0.2451],\n",
              "          [-0.4484, -0.6471, -0.5584,  ..., -0.1159, -0.0343, -0.0555]]]),\n",
              " tensor([[[0.9716, 0.9817, 0.9216,  ..., 0.9213, 0.8747, 0.8964],\n",
              "          [0.8742, 0.9024, 0.9108,  ..., 0.9460, 0.9252, 0.9207],\n",
              "          [0.9315, 0.9169, 0.9928,  ..., 0.9602, 0.9514, 0.9227],\n",
              "          ...,\n",
              "          [0.9928, 0.8873, 0.8463,  ..., 0.9299, 0.9639, 0.9754],\n",
              "          [0.9918, 0.8900, 0.8946,  ..., 0.9904, 0.8834, 0.8569],\n",
              "          [0.9392, 0.9270, 0.9678,  ..., 1.0000, 0.9470, 0.9445]],\n",
              " \n",
              "         [[0.9498, 0.9751, 0.8980,  ..., 0.8986, 0.8418, 0.8492],\n",
              "          [0.8506, 0.8799, 0.8897,  ..., 0.9100, 0.8667, 0.8481],\n",
              "          [0.9057, 0.8961, 0.9808,  ..., 0.9032, 0.8778, 0.8328],\n",
              "          ...,\n",
              "          [0.9762, 0.8023, 0.7349,  ..., 0.9023, 0.9496, 0.9684],\n",
              "          [0.9093, 0.7772, 0.7547,  ..., 0.9391, 0.8346, 0.8207],\n",
              "          [0.8007, 0.7855, 0.8103,  ..., 0.9511, 0.8843, 0.8824]],\n",
              " \n",
              "         [[0.8159, 0.8326, 0.7496,  ..., 0.8866, 0.8447, 0.8637],\n",
              "          [0.7200, 0.7490, 0.7521,  ..., 0.8754, 0.8447, 0.8475],\n",
              "          [0.7949, 0.7840, 0.8595,  ..., 0.8347, 0.8253, 0.7896],\n",
              "          ...,\n",
              "          [0.4972, 0.3007, 0.2132,  ..., 0.4547, 0.4431, 0.3899],\n",
              "          [0.4153, 0.2669, 0.2286,  ..., 0.4663, 0.2925, 0.1991],\n",
              "          [0.2980, 0.2647, 0.2715,  ..., 0.4342, 0.3089, 0.2374]]]),\n",
              " tensor([[[ 0.0331,  0.0900,  0.1960,  ..., -0.1534, -0.2033, -0.2207],\n",
              "          [ 0.1129,  0.1367,  0.2146,  ..., -0.1964, -0.1716, -0.1799],\n",
              "          [ 0.2135,  0.2242,  0.2509,  ..., -0.1864, -0.2052, -0.1794],\n",
              "          ...,\n",
              "          [-0.8423, -0.7983, -0.8123,  ..., -0.9410, -0.9332, -0.9569],\n",
              "          [-0.8787, -0.7546, -0.7393,  ..., -0.9468, -0.9598, -0.9783],\n",
              "          [-0.8815, -0.8117, -0.6378,  ..., -0.9609, -0.9906, -0.9991]],\n",
              " \n",
              "         [[-0.6539, -0.6050, -0.5067,  ..., -0.8570, -0.9107, -0.9321],\n",
              "          [-0.6197, -0.6015, -0.5323,  ..., -0.8866, -0.8696, -0.8779],\n",
              "          [-0.5927, -0.5852, -0.5585,  ..., -0.8533, -0.8764, -0.8541],\n",
              "          ...,\n",
              "          [-0.8133, -0.7810, -0.8200,  ..., -0.9331, -0.9019, -0.9193],\n",
              "          [-0.8412, -0.7344, -0.7470,  ..., -0.9389, -0.9328, -0.9415],\n",
              "          [-0.8298, -0.7848, -0.6522,  ..., -0.9530, -0.9725, -0.9763]],\n",
              " \n",
              "         [[-0.9876, -0.9549, -0.8518,  ..., -0.9287, -0.9644, -0.9599],\n",
              "          [-0.9759, -0.9650, -0.8879,  ..., -0.9650, -0.9260, -0.9124],\n",
              "          [-0.9720, -0.9597, -0.9330,  ..., -0.9375, -0.9421, -0.8945],\n",
              "          ...,\n",
              "          [-0.7238, -0.6822, -0.7102,  ..., -0.9018, -0.8783, -0.8957],\n",
              "          [-0.7376, -0.6251, -0.6372,  ..., -0.9075, -0.9006, -0.9128],\n",
              "          [-0.7064, -0.6645, -0.5424,  ..., -0.9149, -0.9333, -0.9434]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "test_dataset[0]"
      ],
      "id": "66669e53"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9804534d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
      ],
      "id": "9804534d"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a237b3",
        "outputId": "8f6e0fe5-0b81-4c0d-8a78-f5fcdfcd3a4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(921, 10, 931)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ],
      "id": "f7a237b3"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cdce85ca"
      },
      "outputs": [],
      "source": [
        "# Visualization of Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    plt.figure()\n",
        "    plt.imshow(img.permute(1, 2, 0))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images_anchor, images_positive, images_negative = dataiter.next()\n",
        "\n",
        "# # show images\n",
        "# imshow(torchvision.utils.make_grid(images_anchor))\n",
        "# imshow(torchvision.utils.make_grid(images_positive))\n",
        "# imshow(torchvision.utils.make_grid(images_negative))"
      ],
      "id": "cdce85ca"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40002326",
        "outputId": "b4683b15-48b3-4954-af6a-edbeb2dd74df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== DATA LOADED ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Data loaded\n",
        "'''\n",
        "print('================== DATA LOADED ==================')"
      ],
      "id": "40002326"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87028c17",
        "outputId": "4a18e8bd-b568-4eb0-8a0e-4475fe45b0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== START CONSTRUCTING NETWORK ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Start constructing the network\n",
        "'''\n",
        "print('================== START CONSTRUCTING NETWORK ==================')"
      ],
      "id": "87028c17"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fae256a1"
      },
      "outputs": [],
      "source": [
        "# import torch.optim\n",
        "# import torch.utils.data\n",
        "# import torch\n",
        "# import torchvision\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# import torch.utils.data\n",
        "# import torch.backends.cudnn as cudnn\n",
        "\n",
        "# #########################NET##############################\n",
        "\n",
        "# #The backbone for the CNNS with shared weights\n",
        "# def backbone(**kwargs):\n",
        "#     \"\"\"\n",
        "#     Construct a ResNet-101 model.\n",
        "#     Returns:\n",
        "#         Embeddingnet(model): The CNN with the specified model as its backbone is instantiated\n",
        "#     \"\"\"\n",
        "#     #model = torch.hub.load('pytorch/vision:v1.7.1', 'resnet101', pretrained=True)\n",
        "#     model = models.resnet18(pretrained=True)\n",
        "#     #model = models.resnet34(pretrained=True)\n",
        "#     #model = models.vgg11_bn()\n",
        "#     #model = torch.hub.load('pytorch/vision:v0.8.2', 'alexnet', pretrained=True)\n",
        "#     #model = models.alexnet(pretrained=True)            #used in the paper\n",
        "#     #print('Layers',model.children)\n",
        "#     #model = models.resnet50(pretrained=True)\n",
        "#     #model = models.inception_v3(pretrained=True)\n",
        "#     #model = torchvision.models.resnet.ResNet(\n",
        "#         #torchvision.models.resnet.BasicBlock, [2, 1, 1, 1])\n",
        "\n",
        "#     return EmbeddingNet(model)\n",
        "\n",
        "# #The overall network consisting of three embedding nets with shared weights\n",
        "# class TripletNet(nn.Module):\n",
        "#     \"\"\"Triplet Network.\"\"\"\n",
        "\n",
        "#     def __init__(self, embeddingnet):\n",
        "#         \"\"\"Triplet Network Builder.\"\"\"\n",
        "#         super(TripletNet, self).__init__()\n",
        "#         self.embeddingnet = embeddingnet\n",
        "#         #print(self.embeddingnet.children())\n",
        "\n",
        "#     def forward(self, a, p, n):\n",
        "#         \"\"\"Forward pass.\"\"\"\n",
        "#         # anchor\n",
        "#         embedded_a = self.embeddingnet(a)\n",
        "\n",
        "#         # positive examples\n",
        "#         embedded_p = self.embeddingnet(p)\n",
        "\n",
        "#         # negative examples\n",
        "#         embedded_n = self.embeddingnet(n)\n",
        "\n",
        "#         return embedded_a, embedded_p, embedded_n\n",
        "\n",
        "# #The CNN used by Triplet Net with 'model' as its backbone and a final fully connected Layer\n",
        "# class EmbeddingNet(nn.Module):\n",
        "#     \"\"\"EmbeddingNet using the specified model in backbone().\"\"\"\n",
        "\n",
        "#     def __init__(self, resnet):\n",
        "#         \"\"\"Initialize EmbeddingNet model.\"\"\"\n",
        "#         super(EmbeddingNet, self).__init__()\n",
        "#         # Everything excluding the last linear layer\n",
        "#         self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "#         num_ftrs =  resnet.fc.in_features\n",
        "#         self.fc1 = nn.Linear(num_ftrs, 1024)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"Forward pass of EmbeddingNet.\"\"\"\n",
        "#         out = self.features(x)\n",
        "#         out = out.view(out.size(0), -1)\n",
        "#         out = self.fc1(out)\n",
        "#         return out"
      ],
      "id": "fae256a1"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7605800f"
      },
      "outputs": [],
      "source": [
        "# Construct a triplet net\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.utils.data\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "#########################NET##############################\n",
        "\n",
        "#The backbone for the CNNS with shared weights\n",
        "def FeatureExtractNET(**kwargs):\n",
        "    \"\"\"\n",
        "    Construct a ResNet-101 model.\n",
        "    Returns: The CNN for feature extraction with a fully connected layer\n",
        "    \"\"\"\n",
        "    model = models.resnet18(pretrained=True)\n",
        "\n",
        "    return EmbeddingNet(model)\n",
        "\n",
        "#The CNN used by Triplet Net with 'model' as its backbone and a final fully connected Layer\n",
        "class EmbeddingNet(nn.Module):\n",
        "    \"\"\"EmbeddingNet using the specified model in backbone().\"\"\"\n",
        "\n",
        "    def __init__(self, resnet):\n",
        "        \"\"\"Initialize EmbeddingNet model.\"\"\"\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "        # Everything excluding the last linear layer\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        num_ftrs =  resnet.fc.in_features\n",
        "        self.fc1 = nn.Linear(num_ftrs, 1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass of EmbeddingNet.\"\"\"\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        return out\n",
        "    \n",
        "#The overall network consisting of three embedding nets with shared weights\n",
        "class TripletNet(nn.Module):\n",
        "    \"\"\"Triplet Network.\"\"\"\n",
        "\n",
        "    def __init__(self, embeddingnet):\n",
        "        \"\"\"Triplet Network Builder.\"\"\"\n",
        "        super(TripletNet, self).__init__()\n",
        "        self.embeddingnet = embeddingnet\n",
        "\n",
        "    def forward(self, a, p, n):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # anchor\n",
        "        embedded_a = self.embeddingnet(a)\n",
        "\n",
        "        # positive examples\n",
        "        embedded_p = self.embeddingnet(p)\n",
        "\n",
        "        # negative examples\n",
        "        embedded_n = self.embeddingnet(n)\n",
        "\n",
        "        return embedded_a, embedded_p, embedded_n"
      ],
      "id": "7605800f"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7843631c",
        "outputId": "545b8366-b5d9-4295-e5b5-196bf427aec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Initialize CUDA support for TripletNet model ...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): TripletNet(\n",
              "    (embeddingnet): EmbeddingNet(\n",
              "      (features): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "        (4): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (7): Sequential(\n",
              "          (0): BasicBlock(\n",
              "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "          (1): BasicBlock(\n",
              "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      )\n",
              "      (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "net = TripletNet(FeatureExtractNET())\n",
        "\n",
        "#Move the net to GPU for training\n",
        "print(\"==> Initialize CUDA support for TripletNet model ...\")\n",
        "net = torch.nn.DataParallel(net).cuda()\n",
        "cudnn.benchmark = True\n",
        "net"
      ],
      "id": "7843631c"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wO2TH6TjURu8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "wO2TH6TjURu8"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3626cfdd"
      },
      "outputs": [],
      "source": [
        "# batch = next(iter(train_loader))\n",
        "# batch[0], batch[1], batch[2] = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()\n",
        "# net(batch[0],batch[1],batch[2])[0].size()"
      ],
      "id": "3626cfdd"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "b71be7bc"
      },
      "outputs": [],
      "source": [
        "# import hiddenlayer as hl\n",
        "\n",
        "# transforms = [hl.transforms.Prune('Constant')] # Removes Constant nodes from graph.\n",
        "\n",
        "# graph = hl.build_graph(net, (batch[0], batch[1], batch[2]), transforms=transforms)\n",
        "# graph.theme = hl.graph.THEMES['blue'].copy()\n",
        "# graph.save('rnn_hiddenlayer_1', format='png')"
      ],
      "id": "b71be7bc"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5fa4121e"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.TripletMarginLoss(margin=5.0, p=2)\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(),\n",
        "                            lr=0.0005,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=2e-3,#The value used in the paper is 1e-3\n",
        "                            nesterov=True)"
      ],
      "id": "5fa4121e"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vQXSC0QUb2y9"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "vQXSC0QUb2y9"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "239e50ac"
      },
      "outputs": [],
      "source": [
        "# from torch.autograd import Variable\n",
        "# for epoch in range(1):\n",
        "\n",
        "#         running_loss = 0.0\n",
        "#         loss_train = 0.0\n",
        "#         for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
        "\n",
        "# #             if is_gpu:\n",
        "# #                 data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "#             # wrap in torch.autograd.Variable\n",
        "#             data1, data2, data3 = Variable(\n",
        "#                 data1), Variable(data2), Variable(data3)\n",
        "#             print('anchor', data1.size())\n",
        "#             print('positive', data2.size())\n",
        "#             print('negative', data3.size())\n",
        "\n",
        "#             # compute output and loss\n",
        "#             embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
        "#             loss = criterion(embedded_a, embedded_p, embedded_n)\n",
        "#             print(loss)\n",
        "\n",
        "#             # compute gradient and do optimizer step\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             # print the loss\n",
        "#             running_loss += loss.data\n",
        "\n",
        "# #             loss_train_cls = torch.sum(\n",
        "# #                 1 * (criterion_val(embedded_a, embedded_p,\n",
        "# #                                    embedded_n) > 0)) / train_batch_size  # CHANGED, MAY NEED TO REVERT BACK\n",
        "\n",
        "# #             loss_train += loss_train_cls.data\n",
        "\n",
        "#             if batch_idx % 30 == 0:\n",
        "#                 print(\"mini Batch Loss: {}\".format(loss.data))"
      ],
      "id": "239e50ac"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3dca38",
        "outputId": "bd1708b2-01e2-4271-e2d6-57640e6e4d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== NETWORK CONSTRUCTED ==================\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Network constructed\n",
        "'''\n",
        "print('================== NETWORK CONSTRUCTED ==================')"
      ],
      "id": "1c3dca38"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5vdmS_syHlgh"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# pdist = nn.PairwiseDistance(p=2)\n",
        "# input1 = torch.randn(64, 1024)\n",
        "# input2 = torch.randn(64, 1024)\n",
        "# input3 = torch.randn(64, 1024)\n",
        "# print(input1.size())\n",
        "# print(input2.size())\n",
        "# print(input3.size())\n",
        "# dist1 = pdist(input1, input2)\n",
        "# dist2 = pdist(input1, input3)\n",
        "# print(dist1.size())\n",
        "# print(dist2.size())\n",
        "# pred = dist1 - dist2\n",
        "# print(pred.size())\n",
        "# sum = 0\n",
        "# for i in range(pred.size()[0]):\n",
        "#   if pred[i] < 0:\n",
        "#     sum+=1\n",
        "# print(sum/pred.size()[0])\n",
        "# print((pred < 0).sum()*1.0/pred.size()[0])"
      ],
      "id": "5vdmS_syHlgh"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "49lQY_gyLSqM"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# for i in range(10):\n",
        "#   a = random.randint(0,10)\n",
        "#   print(a)"
      ],
      "id": "49lQY_gyLSqM"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "M5uSm08ngNj7"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def accuracy(dista, distb):\n",
        "    margin = 0\n",
        "    pred = (dista - distb).cpu().data\n",
        "    return (pred < 0).sum()*1.0/dista.size()[0]\n",
        "\n",
        "def val_accuracy(trainednet, valloader, valloader_iter):\n",
        "  # sum_accuracy = 0\n",
        "  # num_batch_evaluated = 0\n",
        "  \n",
        "  # Pick one batches for evaluation\n",
        "  # selected_batch = random.randint(0, len(valloader))\n",
        "\n",
        "  # print('Batch selected for evaluation: ', selected_batch)\n",
        "  try:\n",
        "    data1, data2, data3 = next(valloader_iter)\n",
        "  except StopIteration:\n",
        "    valloader_iterator = iter(valloader)\n",
        "    data1, data2, data3 = next(valloader_iterator)\n",
        "\n",
        "  data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "  # wrap in torch.autograd.Variable\n",
        "  data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # compute output and loss\n",
        "    embedded_x, embedded_y, embedded_z = trainednet(data1, data2, data3)\n",
        "    dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "    dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "    print('dist a: {0}, dist b: {1}'.format(dist_a, dist_b))\n",
        "    batch_accuracy = accuracy(dist_a, dist_b)\n",
        "    print('random batch accuracy: {0} '.format(batch_accuracy))\n",
        "\n",
        "  # mean_accuracy = sum_accuracy / num_batch_evaluated\n",
        "  return batch_accuracy"
      ],
      "id": "M5uSm08ngNj7"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6bw80HHgiuxo"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# batch = next(iter(train_loader))\n",
        "# batch[0], batch[1], batch[2] = batch[0].cuda(), batch[1].cuda(), batch[2].cuda()\n",
        "# embedded_x, embedded_y, embedded_z = net(batch[0],batch[1],batch[2])\n",
        "# dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "# dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "# accuracy = accuracy(dist_a, dist_b)\n",
        "# print(accuracy)"
      ],
      "id": "6bw80HHgiuxo"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "d1e25fe5"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, epochs, trainloader, valloader, testloader):\n",
        "\n",
        "  # Create an iterator object for valloader, for selecting a random batch from val set for validation\n",
        "  valloader_iterator = iter(valloader)\n",
        "\n",
        "  # Empty the cache of CUDA  \n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  print('================== START TRAINING ==================')\n",
        "  # Change to train mode\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "      running_loss = 0\n",
        "      for batch_idx, (data0, data1, data2) in enumerate(trainloader):\n",
        "          anchor, positive, negative = data0, data1, data2\n",
        "          anchor = Variable(anchor)\n",
        "          positive = Variable(positive)\n",
        "          negative = Variable(negative)\n",
        "          # print('anchor', anchor.size())\n",
        "          # print('positive', positive.size())\n",
        "          # print('negative', negative.size())\n",
        "          \n",
        "          # Calculate the output of three networks\n",
        "          embedded_a, embedded_p, embedded_n = model(anchor, positive, negative)\n",
        "          \n",
        "          # Calculate the loss\n",
        "          loss = criterion(embedded_a, embedded_p, embedded_n)\n",
        "          print(\"mini Batch Loss: {}\".format(loss.data))\n",
        "          \n",
        "          # Zero the gradient\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          # Back prop and update\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if batch_idx % 30 == 0:\n",
        "            print(\"Training Batch: {0} | Training Loss: {1}\".format(batch_idx+1, loss.data))\n",
        "            save_path = f'/content/drive/My Drive/test4/model_epoch_{epoch+1}_batch_{batch_idx+1}.pt'\n",
        "            torch.save({'Batch': batch_idx, 'model_state_dict': model.state_dict()}, save_path)\n",
        "            print(\"Training Batch: {0} | Model saved to: {1}\".format(batch_idx+1, save_path))\n",
        "\n",
        "            ''' For Validation'''\n",
        "            # Change to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            mean_accuracy = val_accuracy(model, valloader, valloader_iterator)\n",
        "            print(mean_accuracy)\n",
        "\n",
        "            # Change back to train mode\n",
        "            model.train()\n",
        "\n",
        "          # Empty the cache of CUDA  \n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "          \n",
        "      print(f'[{epoch + 1}] average loss per epoch: {running_loss / len(train_loader):.3f}')\n",
        "      # # save checkpoint of model\n",
        "      # if epoch % 5 == 0 and epoch > 0:\n",
        "\n",
        "      save_path = f'/content/drive/My Drive/test4/model_epoch_{epoch+1}.pt'\n",
        "      torch.save({'epoch': epoch, 'model_state_dict': model.state_dict()}, save_path)\n",
        "      print(f'Saved model checkpoint to {save_path}')\n",
        "\n",
        "      ''' For Validation'''\n",
        "      # Change to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      mean_accuracy = val_accuracy(model, valloader, valloader_iterator)\n",
        "      print(mean_accuracy)\n",
        "\n",
        "      ''' For Prediction '''\n",
        "      print('================== START PREDICTION ==================')\n",
        "\n",
        "      redicted_labels = np.zeros(59544)\n",
        "      pred_test=[]\n",
        "\n",
        "      #Predict labels 1 or 0 for each test triplet\n",
        "      for batch_idx, (data1, data2, data3) in enumerate(testloader):\n",
        "\n",
        "          data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "          # wrap in torch.autograd.Variable\n",
        "          data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              # compute output and loss\n",
        "              embedded_x, embedded_y, embedded_z = model(data1, data2, data3)\n",
        "\n",
        "          dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "          dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "          #print(np.squeeze(embedded_a.cpu().detach().numpy()).shape)\n",
        "          \n",
        "\n",
        "          pred_test.append(1*(dist_a <= dist_b))\n",
        "\n",
        "\n",
        "          print('batch: ', batch_idx)\n",
        "\n",
        "      pred_test_np = []\n",
        "      for i in range(len(pred_test)):\n",
        "        pred_test_cpu = pred_test[i].cpu().detach().numpy()\n",
        "        pred_test_np += list(pred_test_cpu)\n",
        "      len(pred_test_np)\n",
        "      predicted_labels = np.hstack(pred_test_np)\n",
        "      print(predicted_labels)\n",
        "\n",
        "      #Write submisison file\n",
        "      df = pd.DataFrame(predicted_labels)\n",
        "      df.to_csv('/content/drive/MyDrive/test4/submission_{0}epoch.txt'.format(epoch+1), index=False, header=None) #write CSV\n",
        "\n",
        "      # Change back to train mode\n",
        "      model.train()\n",
        "\n",
        "  \n",
        "  print('Finished Training')\n",
        "  return model"
      ],
      "id": "d1e25fe5"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b7f8cdf",
        "outputId": "6a850c4b-03bb-4a01-c3e0-8aa6f87e3326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Batch: 31 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_31.pt\n",
            "dist a: tensor([16.3177, 19.4898, 19.0976, 15.7521, 12.2989, 20.1234, 16.3719, 15.3322,\n",
            "        14.3209, 18.7239, 16.6309, 18.9483, 24.1435, 16.3089, 16.3873, 20.8733,\n",
            "        23.4259, 11.1702, 13.6868, 20.2825, 24.0938, 11.6771, 19.2451, 16.0508,\n",
            "        19.9496, 31.9052, 14.0625, 19.7313, 24.3125, 15.4148, 15.0764, 14.0743,\n",
            "        18.6702, 18.0552, 24.7759, 21.5849, 19.2262, 14.3237, 16.0447, 15.6598,\n",
            "        23.3654, 17.2606, 16.2563, 14.8127, 27.3434, 13.5901, 20.8960, 23.0153,\n",
            "        12.8804, 11.6007, 25.9997, 16.7951, 23.5936, 15.6419, 27.4130, 17.1484,\n",
            "        16.3952, 23.3732, 21.3208, 15.6884, 20.2018, 18.4953, 22.9102, 21.0585],\n",
            "       device='cuda:0'), dist b: tensor([31.3837, 26.6528, 33.8447, 14.2925, 22.4139, 22.5329, 24.0710, 22.1979,\n",
            "        28.3035, 29.3484, 17.9573, 17.4363, 29.0396, 18.2308, 27.2203, 18.4778,\n",
            "        27.4246, 26.4151, 23.5355, 37.9532, 24.9801, 28.3245, 21.8289, 23.7113,\n",
            "        20.3058, 33.9126, 26.6831, 24.8737, 19.2706, 18.9334, 26.1562, 18.8276,\n",
            "        28.3337, 24.2781, 25.1985, 22.0058, 25.8533, 15.8716, 16.5448, 26.3982,\n",
            "        18.0816, 21.0928, 18.8820, 20.8196, 20.9983, 25.5124, 32.0477, 23.9876,\n",
            "        20.9072, 21.8121, 31.4913, 21.8114, 20.5542, 20.9637, 12.0105, 20.3035,\n",
            "        22.7328, 23.1991, 14.6258, 38.0803, 23.9496, 21.4010, 34.2511, 24.7629],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 1.3089501857757568\n",
            "mini Batch Loss: 1.5250822305679321\n",
            "mini Batch Loss: 0.7539118528366089\n",
            "mini Batch Loss: 2.269501209259033\n",
            "mini Batch Loss: 0.5431297421455383\n",
            "mini Batch Loss: 0.6042319536209106\n",
            "mini Batch Loss: 1.2229609489440918\n",
            "mini Batch Loss: 2.0134501457214355\n",
            "mini Batch Loss: 1.5549885034561157\n",
            "mini Batch Loss: 1.1164374351501465\n",
            "mini Batch Loss: 1.047756552696228\n",
            "mini Batch Loss: 0.9697771668434143\n",
            "mini Batch Loss: 1.091308832168579\n",
            "mini Batch Loss: 1.2954761981964111\n",
            "mini Batch Loss: 1.4584875106811523\n",
            "mini Batch Loss: 0.9200940132141113\n",
            "mini Batch Loss: 1.9261271953582764\n",
            "mini Batch Loss: 2.3427734375\n",
            "mini Batch Loss: 2.335231304168701\n",
            "mini Batch Loss: 1.256972074508667\n",
            "mini Batch Loss: 1.926025629043579\n",
            "mini Batch Loss: 0.8264178037643433\n",
            "mini Batch Loss: 1.9805327653884888\n",
            "mini Batch Loss: 1.3723533153533936\n",
            "mini Batch Loss: 1.7322149276733398\n",
            "mini Batch Loss: 2.3549046516418457\n",
            "mini Batch Loss: 2.101994276046753\n",
            "mini Batch Loss: 2.252030372619629\n",
            "mini Batch Loss: 1.1423074007034302\n",
            "mini Batch Loss: 0.5795259475708008\n",
            "Training Batch: 61 | Training Loss: 0.5795259475708008\n",
            "Training Batch: 61 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_61.pt\n",
            "dist a: tensor([17.6731, 16.8393, 18.0423, 14.7511, 14.1789, 20.1883, 15.1840, 16.3907,\n",
            "        13.4793, 18.0310, 15.4191, 21.3080, 22.4648, 14.1481, 17.9558, 19.2876,\n",
            "        25.0823, 11.0365, 14.1063, 20.4135, 24.8076, 12.3882, 18.8704, 15.0021,\n",
            "        18.1839, 32.3831, 13.5745, 17.7808, 23.3992, 15.2772, 12.6846, 12.4111,\n",
            "        17.0963, 16.5646, 24.1597, 17.0235, 18.7212, 11.9006, 14.9982, 18.4693,\n",
            "        17.8168, 18.7187, 15.8693, 13.3808, 27.5069, 15.4078, 19.8068, 24.0525,\n",
            "        14.2754, 11.5126, 22.8470, 15.5827, 22.2710, 15.5525, 27.6405, 17.0325,\n",
            "        17.7748, 23.2387, 19.2956, 17.5767, 19.8742, 18.3386, 21.5071, 20.5638],\n",
            "       device='cuda:0'), dist b: tensor([34.3532, 18.8584, 31.8247, 14.0629, 20.5948, 25.5673, 22.4422, 22.0737,\n",
            "        29.5787, 30.5962, 17.2500, 18.7107, 28.1983, 15.5252, 29.3557, 17.8548,\n",
            "        29.4408, 26.7968, 22.6853, 30.7024, 28.4979, 27.8836, 22.3251, 23.1018,\n",
            "        21.7683, 35.0300, 22.2656, 25.1144, 18.7165, 16.9595, 24.8866, 18.7075,\n",
            "        26.0924, 26.1259, 27.4913, 20.8755, 26.7496, 15.7466, 17.7866, 24.5031,\n",
            "        16.4918, 22.0523, 17.9128, 22.1353, 20.0737, 25.9390, 31.7574, 23.8034,\n",
            "        22.6078, 21.5907, 32.3312, 20.1680, 20.7136, 19.3315, 12.0402, 23.9103,\n",
            "        21.5090, 20.9312, 13.7398, 34.1750, 26.8571, 22.9364, 34.0538, 22.3954],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.7159770727157593\n",
            "mini Batch Loss: 0.8172481656074524\n",
            "mini Batch Loss: 0.2605437636375427\n",
            "mini Batch Loss: 0.7022173404693604\n",
            "mini Batch Loss: 1.4841296672821045\n",
            "mini Batch Loss: 0.6811841130256653\n",
            "mini Batch Loss: 1.4755034446716309\n",
            "mini Batch Loss: 0.252651572227478\n",
            "mini Batch Loss: 0.9184609651565552\n",
            "mini Batch Loss: 1.8261741399765015\n",
            "mini Batch Loss: 1.974609613418579\n",
            "mini Batch Loss: 1.7395401000976562\n",
            "mini Batch Loss: 1.7655855417251587\n",
            "mini Batch Loss: 1.4731600284576416\n",
            "mini Batch Loss: 1.4831269979476929\n",
            "mini Batch Loss: 2.2315430641174316\n",
            "mini Batch Loss: 0.9449738264083862\n",
            "mini Batch Loss: 0.5055859088897705\n",
            "mini Batch Loss: 1.656101107597351\n",
            "mini Batch Loss: 1.4136912822723389\n",
            "mini Batch Loss: 0.9723616242408752\n",
            "mini Batch Loss: 0.6261650919914246\n",
            "mini Batch Loss: 1.6835918426513672\n",
            "mini Batch Loss: 0.5600069761276245\n",
            "mini Batch Loss: 1.3144054412841797\n",
            "mini Batch Loss: 0.6750096082687378\n",
            "mini Batch Loss: 1.0148512125015259\n",
            "mini Batch Loss: 0.689760148525238\n",
            "mini Batch Loss: 3.5809824466705322\n",
            "mini Batch Loss: 1.9910624027252197\n",
            "Training Batch: 91 | Training Loss: 1.9910624027252197\n",
            "Training Batch: 91 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_91.pt\n",
            "dist a: tensor([13.0106, 12.4545, 11.9650, 11.7405, 12.1319, 15.3408, 12.9262, 12.9140,\n",
            "        11.8431, 13.3123, 14.4239, 16.6100, 17.6207, 12.1236, 14.8597, 14.9419,\n",
            "        17.8489,  8.3701, 10.7564, 15.6090, 18.2652,  8.7376, 15.3568, 11.7121,\n",
            "        15.6262, 27.3356,  9.7985, 13.2858, 17.9900, 13.4080, 10.7028, 10.4172,\n",
            "        16.2666, 13.5290, 19.1879, 13.4564, 14.6403,  9.3774, 11.4627, 13.5119,\n",
            "        11.1724, 15.3169, 13.3758, 10.6932, 20.5152, 11.1559, 17.2858, 15.3494,\n",
            "        10.9830,  9.1416, 21.0678, 12.9174, 17.7961, 11.9576, 22.9343, 13.2862,\n",
            "        12.8395, 16.5823, 15.5210, 15.0809, 16.6555, 15.4045, 17.1962, 16.5135],\n",
            "       device='cuda:0'), dist b: tensor([28.6526, 15.4794, 23.1095, 12.0887, 16.1927, 16.3611, 18.9142, 18.1009,\n",
            "        24.8382, 20.8593, 13.8188, 15.7264, 21.2178, 13.7951, 24.8254, 15.9611,\n",
            "        20.1745, 20.6276, 18.3239, 23.2439, 21.7624, 23.7897, 16.5681, 16.7389,\n",
            "        15.8198, 25.7457, 17.5959, 20.9765, 13.1701, 13.2553, 19.8778, 14.1238,\n",
            "        23.5072, 20.1639, 21.8935, 17.4363, 21.4464, 11.5033, 12.6664, 20.6939,\n",
            "        13.3312, 19.6757, 13.9432, 15.8088, 17.4444, 22.6691, 22.7312, 16.7010,\n",
            "        17.3106, 17.7448, 27.3195, 16.0701, 17.9418, 16.1124, 10.8332, 18.6180,\n",
            "        16.0077, 17.7347, 12.0770, 27.7938, 21.0204, 15.8543, 28.4275, 19.1259],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 1.4760006666183472\n",
            "mini Batch Loss: 1.4392225742340088\n",
            "mini Batch Loss: 1.0710219144821167\n",
            "mini Batch Loss: 0.4388890266418457\n",
            "mini Batch Loss: 1.7755848169326782\n",
            "mini Batch Loss: 1.9132559299468994\n",
            "mini Batch Loss: 2.014080047607422\n",
            "mini Batch Loss: 0.5569612383842468\n",
            "mini Batch Loss: 1.1043659448623657\n",
            "mini Batch Loss: 1.6537303924560547\n",
            "mini Batch Loss: 1.9832634925842285\n",
            "mini Batch Loss: 1.0335664749145508\n",
            "mini Batch Loss: 2.4221858978271484\n",
            "mini Batch Loss: 0.5949915051460266\n",
            "mini Batch Loss: 0.49608010053634644\n",
            "mini Batch Loss: 1.0015596151351929\n",
            "mini Batch Loss: 2.560635805130005\n",
            "mini Batch Loss: 0.6608742475509644\n",
            "mini Batch Loss: 0.7851539254188538\n",
            "mini Batch Loss: 0.35849228501319885\n",
            "mini Batch Loss: 1.7959201335906982\n",
            "mini Batch Loss: 2.942605972290039\n",
            "mini Batch Loss: 1.6498265266418457\n",
            "mini Batch Loss: 1.4024577140808105\n",
            "mini Batch Loss: 1.0321303606033325\n",
            "mini Batch Loss: 2.0864322185516357\n",
            "mini Batch Loss: 0.6023514270782471\n",
            "mini Batch Loss: 2.6691393852233887\n",
            "mini Batch Loss: 1.490997314453125\n",
            "mini Batch Loss: 0.7239018678665161\n",
            "Training Batch: 121 | Training Loss: 0.7239018678665161\n",
            "Training Batch: 121 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_121.pt\n",
            "dist a: tensor([15.5202, 15.5562, 15.1773, 14.9000, 14.0016, 20.4617, 13.5019, 16.5651,\n",
            "        17.2071, 16.7878, 17.4289, 17.9250, 25.5212, 15.6499, 17.0059, 18.1390,\n",
            "        22.6562,  9.9337, 14.3303, 19.2493, 22.2766, 11.1311, 16.6570, 15.2577,\n",
            "        18.1544, 32.7685, 13.0029, 17.4093, 22.2489, 17.8174, 13.0211, 14.3819,\n",
            "        20.7456, 19.5321, 23.5333, 16.9368, 16.1061, 12.7643, 16.8073, 17.7639,\n",
            "        15.1038, 16.7572, 13.3657, 13.5272, 27.6943, 12.0369, 21.2952, 22.8936,\n",
            "        12.9618, 13.0767, 27.3025, 14.2360, 26.9032, 16.3567, 29.8047, 15.0609,\n",
            "        16.1491, 20.8871, 20.5989, 15.4996, 21.1898, 20.2089, 21.4272, 20.8750],\n",
            "       device='cuda:0'), dist b: tensor([30.5504, 19.0187, 29.0440, 13.6834, 21.4877, 21.3627, 24.3193, 22.6025,\n",
            "        30.4439, 28.3567, 16.9663, 19.1183, 28.1629, 18.2997, 26.5963, 17.5252,\n",
            "        21.9943, 27.1002, 22.0816, 31.9567, 24.8203, 26.9862, 22.0727, 22.1344,\n",
            "        17.7252, 31.2902, 20.9781, 28.7712, 17.0243, 18.8368, 26.3557, 18.7005,\n",
            "        30.0583, 29.1535, 26.7161, 21.1079, 23.2865, 15.1927, 17.1992, 25.8659,\n",
            "        15.4438, 20.2399, 18.2592, 22.0190, 22.2752, 26.2490, 28.0271, 25.4072,\n",
            "        23.4958, 23.3321, 33.4215, 19.0243, 18.4457, 21.9548, 12.4268, 24.1845,\n",
            "        22.3253, 19.9693, 14.6911, 36.7419, 25.9140, 19.6441, 34.7599, 21.3515],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "mini Batch Loss: 1.798422932624817\n",
            "mini Batch Loss: 1.3255794048309326\n",
            "mini Batch Loss: 2.601897716522217\n",
            "mini Batch Loss: 1.5263584852218628\n",
            "mini Batch Loss: 0.5063362121582031\n",
            "mini Batch Loss: 1.6896789073944092\n",
            "mini Batch Loss: 3.1085174083709717\n",
            "mini Batch Loss: 1.157758116722107\n",
            "mini Batch Loss: 2.025500774383545\n",
            "mini Batch Loss: 1.8853105306625366\n",
            "mini Batch Loss: 1.0205599069595337\n",
            "mini Batch Loss: 1.2929861545562744\n",
            "mini Batch Loss: 3.0627684593200684\n",
            "mini Batch Loss: 1.1987113952636719\n",
            "mini Batch Loss: 1.711490511894226\n",
            "mini Batch Loss: 0.8491402864456177\n",
            "mini Batch Loss: 0.5936257839202881\n",
            "mini Batch Loss: 1.3767998218536377\n",
            "mini Batch Loss: 0.7264307737350464\n",
            "mini Batch Loss: 0.912126898765564\n",
            "mini Batch Loss: 1.5758624076843262\n",
            "mini Batch Loss: 0.41965168714523315\n",
            "mini Batch Loss: 0.4008617103099823\n",
            "mini Batch Loss: 1.0302257537841797\n",
            "mini Batch Loss: 1.3310165405273438\n",
            "mini Batch Loss: 0.8316059112548828\n",
            "mini Batch Loss: 1.1684913635253906\n",
            "mini Batch Loss: 1.8087496757507324\n",
            "mini Batch Loss: 0.6503231525421143\n",
            "mini Batch Loss: 0.7499479055404663\n",
            "Training Batch: 151 | Training Loss: 0.7499479055404663\n",
            "Training Batch: 151 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_151.pt\n",
            "dist a: tensor([10.7083, 12.2242, 11.7454, 11.5345, 10.1272, 14.0688, 11.1087, 10.7039,\n",
            "        10.5342, 12.0562, 12.6866, 14.6137, 17.2888, 11.7625, 13.7768, 13.1310,\n",
            "        18.3072,  7.9648, 11.1257, 14.2236, 13.8087,  8.2977, 11.7365, 12.6323,\n",
            "        13.2546, 21.7541,  8.2679, 13.3406, 16.0537, 12.3245,  8.8825, 10.0610,\n",
            "        12.7159, 15.2561, 17.7281, 14.1221, 12.5845,  8.1918, 11.5058, 11.8768,\n",
            "        11.2122, 10.3634,  9.5332,  9.7563, 17.4421,  8.7159, 14.8714, 18.7314,\n",
            "        10.3638, 10.2345, 18.5408, 10.4358, 17.1648, 11.2663, 20.3321, 13.5015,\n",
            "        13.5830, 16.1100, 14.0950, 11.6676, 14.7931, 16.8081, 15.2822, 15.6717],\n",
            "       device='cuda:0'), dist b: tensor([23.4159, 15.0862, 23.1322, 10.5508, 13.4798, 16.1659, 16.7963, 15.4684,\n",
            "        23.9973, 21.4037, 12.1145, 15.1282, 19.7837, 11.3076, 20.5820, 13.7081,\n",
            "        17.9411, 20.0017, 15.1681, 24.1073, 18.7574, 21.1912, 16.6724, 16.9000,\n",
            "        13.3457, 22.4039, 16.3929, 20.7989, 13.0173, 13.5028, 19.6457, 14.9292,\n",
            "        18.8571, 20.0433, 22.2660, 18.5110, 17.5918, 10.3411, 13.3810, 18.7909,\n",
            "        12.7802, 13.9238, 13.5553, 14.4321, 15.4832, 18.3448, 22.7504, 20.0301,\n",
            "        18.4099, 15.9244, 25.7037, 13.2240, 14.7297, 15.4960,  8.6592, 15.6160,\n",
            "        13.8591, 16.7888, 11.5629, 23.6126, 17.8001, 11.3482, 23.4656, 16.5386],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 1.4940448999404907\n",
            "mini Batch Loss: 0.7065138816833496\n",
            "mini Batch Loss: 0.815579354763031\n",
            "mini Batch Loss: 1.6235631704330444\n",
            "mini Batch Loss: 0.9548125267028809\n",
            "mini Batch Loss: 0.1687580645084381\n",
            "mini Batch Loss: 1.6093158721923828\n",
            "mini Batch Loss: 1.5610744953155518\n",
            "mini Batch Loss: 1.6615976095199585\n",
            "mini Batch Loss: 1.2562694549560547\n",
            "mini Batch Loss: 1.2061707973480225\n",
            "mini Batch Loss: 1.1366738080978394\n",
            "mini Batch Loss: 1.8267971277236938\n",
            "mini Batch Loss: 1.8542364835739136\n",
            "mini Batch Loss: 1.3726688623428345\n",
            "mini Batch Loss: 2.4779345989227295\n",
            "mini Batch Loss: 1.695056438446045\n",
            "mini Batch Loss: 1.9041266441345215\n",
            "mini Batch Loss: 1.050722360610962\n",
            "mini Batch Loss: 0.6795383095741272\n",
            "mini Batch Loss: 2.5993900299072266\n",
            "mini Batch Loss: 0.7576344609260559\n",
            "mini Batch Loss: 0.5380241870880127\n",
            "mini Batch Loss: 0.6356652975082397\n",
            "mini Batch Loss: 1.5461301803588867\n",
            "mini Batch Loss: 1.7575877904891968\n",
            "mini Batch Loss: 1.2877631187438965\n",
            "mini Batch Loss: 0.5374329686164856\n",
            "mini Batch Loss: 1.9804222583770752\n",
            "mini Batch Loss: 0.21889816224575043\n",
            "Training Batch: 181 | Training Loss: 0.21889816224575043\n",
            "Training Batch: 181 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_181.pt\n",
            "dist a: tensor([12.7302, 14.6066, 14.1661, 13.4585, 12.3245, 14.9263, 12.4722, 13.3815,\n",
            "        12.3045, 16.0577, 13.3136, 16.5690, 18.0356, 14.8225, 15.6727, 16.6741,\n",
            "        20.0616, 10.1175, 11.2534, 16.3157, 18.0214,  9.2008, 15.1614, 14.3888,\n",
            "        14.7503, 25.5509, 10.5126, 16.9368, 18.0028, 13.4018,  9.7920, 14.5046,\n",
            "        15.7306, 17.4165, 19.8875, 16.5650, 14.4562, 10.3076, 11.4050, 14.0476,\n",
            "        13.6785, 13.2396, 11.0276, 11.5074, 20.5158, 11.5665, 17.4598, 20.2777,\n",
            "        11.8610, 12.9955, 23.5619, 13.8038, 21.7372, 14.2741, 20.6736, 15.8696,\n",
            "        16.4162, 19.2052, 16.0205, 13.5104, 18.2788, 17.7789, 18.6970, 18.4056],\n",
            "       device='cuda:0'), dist b: tensor([26.5985, 16.8297, 26.9179, 12.1568, 17.0245, 19.1815, 20.2827, 19.2700,\n",
            "        26.5073, 23.6288, 16.3833, 16.1702, 19.7782, 13.6701, 20.4218, 15.6518,\n",
            "        17.8304, 21.7980, 20.2910, 22.5173, 20.7911, 22.4648, 17.9078, 19.8668,\n",
            "        16.0701, 23.8684, 19.0879, 23.3745, 16.3132, 13.9573, 22.6339, 19.7361,\n",
            "        22.2125, 20.1313, 21.7098, 20.0482, 19.0214, 13.1294, 12.7438, 21.3046,\n",
            "        15.5535, 17.5297, 16.6670, 16.6563, 18.7784, 19.6911, 23.8474, 22.0694,\n",
            "        20.5023, 18.0681, 30.0139, 16.3072, 16.5499, 18.9318, 10.4501, 17.3967,\n",
            "        15.6705, 18.7269, 13.6435, 29.3911, 18.5596, 12.7726, 25.1428, 19.2047],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.78125 \n",
            "tensor(0.7812)\n",
            "mini Batch Loss: 1.6414601802825928\n",
            "mini Batch Loss: 2.688176155090332\n",
            "mini Batch Loss: 1.4396085739135742\n",
            "mini Batch Loss: 1.6963083744049072\n",
            "mini Batch Loss: 1.4198963642120361\n",
            "mini Batch Loss: 1.875022292137146\n",
            "mini Batch Loss: 1.8302428722381592\n",
            "mini Batch Loss: 1.2580503225326538\n",
            "mini Batch Loss: 2.253818988800049\n",
            "mini Batch Loss: 2.0347530841827393\n",
            "mini Batch Loss: 1.5733277797698975\n",
            "mini Batch Loss: 1.9721200466156006\n",
            "mini Batch Loss: 1.30942702293396\n",
            "mini Batch Loss: 1.7819052934646606\n",
            "mini Batch Loss: 1.62257981300354\n",
            "mini Batch Loss: 2.048502206802368\n",
            "mini Batch Loss: 2.633985996246338\n",
            "mini Batch Loss: 1.1814446449279785\n",
            "mini Batch Loss: 2.02363920211792\n",
            "mini Batch Loss: 1.7447795867919922\n",
            "mini Batch Loss: 1.8804597854614258\n",
            "mini Batch Loss: 1.1101549863815308\n",
            "mini Batch Loss: 2.0798113346099854\n",
            "mini Batch Loss: 1.5456924438476562\n",
            "mini Batch Loss: 1.4125595092773438\n",
            "mini Batch Loss: 2.2756595611572266\n",
            "mini Batch Loss: 1.702148199081421\n",
            "mini Batch Loss: 1.2755494117736816\n",
            "mini Batch Loss: 1.7206110954284668\n",
            "mini Batch Loss: 1.3384240865707397\n",
            "Training Batch: 211 | Training Loss: 1.3384240865707397\n",
            "Training Batch: 211 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_211.pt\n",
            "dist a: tensor([17.3766, 21.8305, 18.5258, 16.1030, 15.8396, 19.1778, 15.5938, 20.5742,\n",
            "        18.8129, 23.1588, 18.7613, 21.5536, 24.5281, 18.6874, 19.0321, 24.9897,\n",
            "        24.8506, 13.2493, 16.5669, 22.5994, 23.6578, 12.7719, 18.8091, 16.9869,\n",
            "        20.3564, 33.3156, 15.8244, 23.4081, 27.9159, 15.4323, 14.2025, 16.6269,\n",
            "        20.8472, 24.0577, 26.7896, 22.0920, 18.6387, 13.4253, 19.8568, 21.2544,\n",
            "        16.3256, 19.7884, 17.1086, 17.3151, 29.9492, 15.4714, 24.1627, 29.9318,\n",
            "        18.9640, 13.7915, 27.6390, 18.8922, 29.5187, 17.7605, 27.7848, 17.1469,\n",
            "        20.4382, 25.0984, 25.6008, 18.9761, 25.0868, 19.9401, 27.2948, 22.5375],\n",
            "       device='cuda:0'), dist b: tensor([32.5723, 23.4155, 32.2164, 15.7029, 24.5745, 30.2881, 28.5795, 27.0916,\n",
            "        31.4600, 34.9341, 21.2709, 20.1162, 26.8628, 20.3622, 29.1133, 19.5057,\n",
            "        25.5615, 30.3287, 27.3744, 32.9628, 26.6933, 29.3287, 24.9354, 26.2303,\n",
            "        21.6712, 40.2228, 26.3604, 32.7723, 21.5998, 17.2101, 31.5620, 24.1172,\n",
            "        30.5870, 28.8382, 30.1632, 21.5770, 28.8123, 15.9464, 26.2924, 29.4113,\n",
            "        20.7554, 23.0029, 23.8124, 18.9960, 24.7363, 23.8181, 34.1392, 26.8697,\n",
            "        26.6744, 24.2733, 36.9158, 22.8996, 22.7152, 24.6468, 12.1693, 23.5584,\n",
            "        19.8727, 25.0824, 16.9613, 35.9419, 29.0098, 22.7516, 40.5577, 23.0431],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.8125 \n",
            "tensor(0.8125)\n",
            "mini Batch Loss: 1.2819507122039795\n",
            "mini Batch Loss: 1.6008309125900269\n",
            "mini Batch Loss: 1.865100383758545\n",
            "mini Batch Loss: 1.1435986757278442\n",
            "mini Batch Loss: 1.33021080493927\n",
            "mini Batch Loss: 1.0803675651550293\n",
            "mini Batch Loss: 1.4047019481658936\n",
            "mini Batch Loss: 1.6293563842773438\n",
            "mini Batch Loss: 1.4518072605133057\n",
            "mini Batch Loss: 1.5476133823394775\n",
            "mini Batch Loss: 1.2421727180480957\n",
            "mini Batch Loss: 1.1902098655700684\n",
            "mini Batch Loss: 1.1013118028640747\n",
            "mini Batch Loss: 1.8971625566482544\n",
            "mini Batch Loss: 1.3022770881652832\n",
            "mini Batch Loss: 1.098467469215393\n",
            "mini Batch Loss: 1.6028990745544434\n",
            "mini Batch Loss: 1.1416664123535156\n",
            "mini Batch Loss: 0.7134782075881958\n",
            "mini Batch Loss: 1.030818223953247\n",
            "mini Batch Loss: 1.3295778036117554\n",
            "mini Batch Loss: 1.6662801504135132\n",
            "mini Batch Loss: 1.4899417161941528\n",
            "mini Batch Loss: 1.9973419904708862\n",
            "mini Batch Loss: 1.3768047094345093\n",
            "mini Batch Loss: 1.939113974571228\n",
            "mini Batch Loss: 1.2560453414916992\n",
            "mini Batch Loss: 1.1156022548675537\n",
            "mini Batch Loss: 1.0778167247772217\n",
            "mini Batch Loss: 1.7214221954345703\n",
            "Training Batch: 241 | Training Loss: 1.7214221954345703\n",
            "Training Batch: 241 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_241.pt\n",
            "dist a: tensor([13.1441, 15.6275, 15.2791, 12.9684, 14.7059, 15.5451, 15.1311, 15.0577,\n",
            "        14.0864, 17.5278, 17.0339, 16.5566, 20.1789, 14.9162, 15.2074, 17.3988,\n",
            "        24.8477, 10.3645, 14.7772, 17.9817, 19.4688, 11.0647, 15.0501, 12.1769,\n",
            "        18.2231, 29.3694, 13.7148, 20.0871, 20.5619, 15.6483, 11.4769, 12.9910,\n",
            "        17.4910, 16.2190, 21.9440, 17.2776, 16.6264, 10.6499, 15.4813, 16.2092,\n",
            "        15.3124, 14.7511, 13.8666, 12.8546, 25.9959, 12.1500, 22.9981, 21.8129,\n",
            "        12.9527, 12.8762, 27.9460, 15.3869, 26.4358, 14.7791, 21.6666, 14.8697,\n",
            "        14.6326, 20.6989, 20.1345, 15.0739, 18.8436, 17.7686, 21.8995, 19.5082],\n",
            "       device='cuda:0'), dist b: tensor([29.7870, 17.0517, 25.8507, 13.1511, 17.9475, 22.5116, 24.0191, 19.7979,\n",
            "        27.3443, 26.8030, 20.2819, 19.7243, 21.5824, 16.0097, 25.4171, 17.5050,\n",
            "        26.1111, 24.8685, 24.2195, 24.2000, 23.5432, 26.1473, 20.0945, 20.1952,\n",
            "        16.8907, 32.8596, 22.0174, 28.5901, 15.4174, 15.9921, 21.3324, 18.8853,\n",
            "        25.6571, 24.1676, 24.2090, 20.1827, 24.4190, 14.8484, 22.1945, 26.5119,\n",
            "        16.8836, 18.7188, 18.0678, 16.1364, 22.6309, 21.2952, 24.6838, 24.3844,\n",
            "        25.7671, 20.6977, 35.2126, 18.6859, 19.1024, 20.9390, 11.0081, 20.4861,\n",
            "        20.1573, 19.8822, 17.0402, 30.8494, 23.3930, 17.4105, 33.8670, 18.9182],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 1.302976131439209\n",
            "mini Batch Loss: 2.651289701461792\n",
            "mini Batch Loss: 2.326162099838257\n",
            "mini Batch Loss: 0.864649772644043\n",
            "mini Batch Loss: 1.110276699066162\n",
            "mini Batch Loss: 1.4443496465682983\n",
            "mini Batch Loss: 1.0453141927719116\n",
            "mini Batch Loss: 1.321384072303772\n",
            "mini Batch Loss: 1.0345418453216553\n",
            "mini Batch Loss: 2.0870285034179688\n",
            "mini Batch Loss: 0.2671873867511749\n",
            "mini Batch Loss: 0.8561007380485535\n",
            "mini Batch Loss: 0.4339367151260376\n",
            "mini Batch Loss: 0.6839615106582642\n",
            "mini Batch Loss: 1.5828624963760376\n",
            "mini Batch Loss: 1.019073724746704\n",
            "mini Batch Loss: 2.0370631217956543\n",
            "mini Batch Loss: 1.093696117401123\n",
            "mini Batch Loss: 2.02876615524292\n",
            "mini Batch Loss: 0.8115127682685852\n",
            "mini Batch Loss: 1.695150375366211\n",
            "mini Batch Loss: 1.0944525003433228\n",
            "mini Batch Loss: 0.9136550426483154\n",
            "mini Batch Loss: 1.0922802686691284\n",
            "mini Batch Loss: 2.0112147331237793\n",
            "mini Batch Loss: 1.086393117904663\n",
            "mini Batch Loss: 1.921078085899353\n",
            "mini Batch Loss: 0.3906310796737671\n",
            "mini Batch Loss: 1.968761920928955\n",
            "mini Batch Loss: 0.9802322387695312\n",
            "Training Batch: 271 | Training Loss: 0.9802322387695312\n",
            "Training Batch: 271 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_271.pt\n",
            "dist a: tensor([12.9366, 15.1962, 14.2510, 12.3916, 14.4955, 16.6712, 15.0809, 14.3545,\n",
            "        12.1533, 19.5111, 15.6463, 15.6340, 20.1764, 12.7101, 16.6624, 14.7439,\n",
            "        24.7890,  9.3368, 13.4474, 19.4182, 20.4988, 12.2516, 14.9617, 10.8584,\n",
            "        20.2207, 27.4655, 10.7724, 18.2057, 24.7188, 12.3669, 12.7947, 15.3227,\n",
            "        17.7007, 16.4320, 19.8419, 15.7207, 15.9210, 11.2519, 14.4111, 17.1879,\n",
            "        13.4865, 13.1307, 16.1193, 13.5067, 27.2140, 13.1793, 21.1474, 22.5033,\n",
            "        12.9094, 11.7921, 25.9411, 16.0215, 18.8312, 15.3389, 22.6570, 15.0232,\n",
            "        14.3758, 20.0131, 16.5685, 14.8905, 17.6097, 18.7010, 20.4383, 20.3610],\n",
            "       device='cuda:0'), dist b: tensor([28.7271, 21.3514, 26.0118, 13.2784, 19.8003, 21.6080, 21.5797, 19.6976,\n",
            "        25.8167, 27.4610, 19.0370, 19.4718, 21.1664, 15.4354, 22.1272, 18.1436,\n",
            "        26.6735, 23.4486, 21.8865, 29.0992, 23.6826, 23.8092, 21.4064, 19.6101,\n",
            "        17.4990, 29.7108, 20.1069, 25.7341, 16.3975, 16.1182, 23.5346, 16.7828,\n",
            "        23.2711, 24.0984, 22.7676, 22.4990, 25.0448, 15.6801, 18.3468, 22.2666,\n",
            "        15.3861, 19.1603, 16.7018, 18.2190, 20.8960, 23.9806, 24.1961, 21.2144,\n",
            "        23.8585, 18.4385, 35.7290, 20.0520, 19.3645, 20.3736, 10.6624, 18.5864,\n",
            "        20.5835, 19.2172, 15.1869, 29.1866, 23.1596, 15.2447, 30.9925, 19.8621],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 1.6435245275497437\n",
            "mini Batch Loss: 0.9452824592590332\n",
            "mini Batch Loss: 1.8920528888702393\n",
            "mini Batch Loss: 1.264510154724121\n",
            "mini Batch Loss: 0.5618551969528198\n",
            "mini Batch Loss: 0.9431625604629517\n",
            "mini Batch Loss: 1.7373976707458496\n",
            "mini Batch Loss: 1.3092817068099976\n",
            "mini Batch Loss: 4.134234428405762\n",
            "mini Batch Loss: 0.19088098406791687\n",
            "mini Batch Loss: 0.8036822080612183\n",
            "mini Batch Loss: 1.3335628509521484\n",
            "mini Batch Loss: 1.4395527839660645\n",
            "mini Batch Loss: 1.701723575592041\n",
            "mini Batch Loss: 1.5329108238220215\n",
            "mini Batch Loss: 1.8446881771087646\n",
            "mini Batch Loss: 1.4578886032104492\n",
            "mini Batch Loss: 2.4271883964538574\n",
            "mini Batch Loss: 1.2751619815826416\n",
            "mini Batch Loss: 1.0757945775985718\n",
            "mini Batch Loss: 0.8868427276611328\n",
            "mini Batch Loss: 1.1991283893585205\n",
            "mini Batch Loss: 1.474189281463623\n",
            "mini Batch Loss: 2.467052936553955\n",
            "mini Batch Loss: 1.434011697769165\n",
            "mini Batch Loss: 0.7589559555053711\n",
            "mini Batch Loss: 0.9377397298812866\n",
            "mini Batch Loss: 1.6435319185256958\n",
            "mini Batch Loss: 1.5363013744354248\n",
            "mini Batch Loss: 0.8864789009094238\n",
            "Training Batch: 301 | Training Loss: 0.8864789009094238\n",
            "Training Batch: 301 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_301.pt\n",
            "dist a: tensor([11.0506, 15.4512, 12.9627, 11.3523, 14.2688, 14.8526, 11.8218, 14.2293,\n",
            "        10.8981, 15.3858, 13.3480, 14.4473, 18.4061,  9.7633, 14.3226, 15.0206,\n",
            "        24.2055,  8.9451, 11.0386, 15.4516, 18.1868, 10.0916, 11.0335, 12.9956,\n",
            "        21.1083, 24.7431, 10.7724, 15.3556, 21.1349, 12.6319, 11.5528, 11.4828,\n",
            "        15.5256, 16.4108, 18.4696, 15.0673, 13.8651,  9.9885, 11.6373, 13.2782,\n",
            "        13.1454, 10.3441, 13.1875, 12.5604, 25.3197, 11.9773, 17.3550, 21.0976,\n",
            "        14.2557, 11.0883, 21.8228, 13.6197, 15.6301, 14.0986, 23.1703, 14.8193,\n",
            "        15.6404, 18.0113, 16.7290, 15.0220, 16.6199, 18.3325, 14.6750, 16.5695],\n",
            "       device='cuda:0'), dist b: tensor([25.1182, 20.1314, 24.2785, 11.7682, 18.0194, 20.6494, 18.0793, 18.0664,\n",
            "        20.5973, 25.0084, 15.3566, 17.1904, 19.3993, 13.9781, 21.3762, 14.9119,\n",
            "        23.1106, 20.9702, 19.1495, 23.6752, 18.7250, 22.5537, 18.7614, 16.9804,\n",
            "        17.3015, 25.2530, 14.3782, 23.8329, 15.2818, 13.1011, 23.8346, 16.6973,\n",
            "        22.0235, 20.4105, 23.5906, 19.1477, 23.2445, 13.7484, 17.9574, 20.9346,\n",
            "        13.5367, 14.9214, 15.6377, 18.3773, 17.3955, 19.8812, 24.2150, 20.6617,\n",
            "        22.1633, 17.9997, 31.5781, 15.8399, 19.3238, 18.9162,  9.6567, 16.7347,\n",
            "        17.1808, 16.8195, 13.1201, 24.9920, 22.7309, 16.2377, 26.0179, 20.4780],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.8694072961807251\n",
            "mini Batch Loss: 0.9695295095443726\n",
            "mini Batch Loss: 0.8872649669647217\n",
            "mini Batch Loss: 0.6502820253372192\n",
            "mini Batch Loss: 1.9888415336608887\n",
            "mini Batch Loss: 0.7095214128494263\n",
            "mini Batch Loss: 0.8883886337280273\n",
            "mini Batch Loss: 0.8335763812065125\n",
            "mini Batch Loss: 1.497143030166626\n",
            "mini Batch Loss: 2.5419716835021973\n",
            "mini Batch Loss: 1.8231573104858398\n",
            "mini Batch Loss: 1.045121192932129\n",
            "mini Batch Loss: 0.9861389398574829\n",
            "mini Batch Loss: 1.45417320728302\n",
            "mini Batch Loss: 0.2140345424413681\n",
            "mini Batch Loss: 0.8001658916473389\n",
            "mini Batch Loss: 1.0567634105682373\n",
            "mini Batch Loss: 1.4412318468093872\n",
            "mini Batch Loss: 1.833268404006958\n",
            "mini Batch Loss: 0.9679092168807983\n",
            "mini Batch Loss: 1.7501418590545654\n",
            "mini Batch Loss: 3.254331588745117\n",
            "mini Batch Loss: 2.105111598968506\n",
            "mini Batch Loss: 1.7465426921844482\n",
            "mini Batch Loss: 0.7948366403579712\n",
            "mini Batch Loss: 0.7189217805862427\n",
            "mini Batch Loss: 1.4524145126342773\n",
            "mini Batch Loss: 0.8831132650375366\n",
            "mini Batch Loss: 2.3663296699523926\n",
            "mini Batch Loss: 0.6853337287902832\n",
            "Training Batch: 331 | Training Loss: 0.6853337287902832\n",
            "Training Batch: 331 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_331.pt\n",
            "dist a: tensor([13.5252, 20.2193, 16.6042, 14.5162, 17.0179, 18.7597, 16.0597, 16.0081,\n",
            "        13.2034, 18.3743, 18.5584, 19.3472, 24.8419, 13.7954, 18.0424, 18.1670,\n",
            "        26.6135, 11.4075, 13.8828, 19.8408, 23.9019, 11.4828, 14.6399, 15.1492,\n",
            "        23.6220, 29.8166, 12.2938, 20.3795, 27.5115, 13.4606, 13.9008, 14.9344,\n",
            "        18.4382, 18.3327, 22.2536, 20.7322, 15.5607, 14.9939, 14.0327, 17.4608,\n",
            "        14.9130, 14.1201, 17.1266, 15.6815, 31.5806, 14.4499, 19.6450, 25.3446,\n",
            "        20.0176, 10.9106, 27.6334, 17.2938, 20.9265, 17.9501, 26.6914, 18.0020,\n",
            "        19.5935, 23.2828, 21.4671, 16.0427, 20.5315, 22.0901, 18.8640, 18.9265],\n",
            "       device='cuda:0'), dist b: tensor([31.4522, 26.1516, 28.1449, 14.9015, 18.3935, 24.2103, 23.1008, 24.2935,\n",
            "        27.8392, 30.6226, 16.7972, 22.0715, 27.2307, 17.5359, 28.9376, 20.3221,\n",
            "        26.5698, 24.1839, 24.2685, 31.6230, 27.4352, 26.6169, 23.1059, 22.9388,\n",
            "        22.3361, 29.7446, 19.9275, 25.7768, 20.5408, 18.3253, 25.7928, 22.5043,\n",
            "        27.5139, 22.8704, 24.5149, 23.3624, 26.6454, 18.3486, 22.5535, 22.2693,\n",
            "        18.1957, 18.4236, 20.8424, 19.4235, 21.6128, 23.7778, 31.4477, 22.2001,\n",
            "        26.5219, 19.5642, 35.5217, 22.9025, 24.9748, 24.7704, 15.3538, 20.3941,\n",
            "        20.6337, 21.5960, 16.7590, 32.5485, 28.3563, 18.7129, 31.3153, 21.8862],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 1.9139337539672852\n",
            "mini Batch Loss: 1.4029964208602905\n",
            "mini Batch Loss: 0.10679066181182861\n",
            "mini Batch Loss: 0.28724977374076843\n",
            "mini Batch Loss: 0.559352695941925\n",
            "mini Batch Loss: 0.552699089050293\n",
            "mini Batch Loss: 2.2642102241516113\n",
            "mini Batch Loss: 0.6601428985595703\n",
            "mini Batch Loss: 2.0607872009277344\n",
            "mini Batch Loss: 2.0074212551116943\n",
            "mini Batch Loss: 0.5927432775497437\n",
            "mini Batch Loss: 0.3823580741882324\n",
            "mini Batch Loss: 0.2449176013469696\n",
            "mini Batch Loss: 0.5016012191772461\n",
            "mini Batch Loss: 0.68166184425354\n",
            "mini Batch Loss: 0.04405280947685242\n",
            "mini Batch Loss: 0.035429447889328\n",
            "mini Batch Loss: 0.039544180035591125\n",
            "mini Batch Loss: 1.596498966217041\n",
            "mini Batch Loss: 1.2189655303955078\n",
            "mini Batch Loss: 1.048110008239746\n",
            "mini Batch Loss: 1.5938713550567627\n",
            "mini Batch Loss: 2.7847275733947754\n",
            "mini Batch Loss: 1.6841851472854614\n",
            "mini Batch Loss: 2.1578426361083984\n",
            "mini Batch Loss: 2.2232441902160645\n",
            "mini Batch Loss: 1.5937628746032715\n",
            "mini Batch Loss: 0.8338043689727783\n",
            "mini Batch Loss: 1.3351151943206787\n",
            "mini Batch Loss: 2.019190788269043\n",
            "Training Batch: 361 | Training Loss: 2.019190788269043\n",
            "Training Batch: 361 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_361.pt\n",
            "dist a: tensor([17.5966, 23.7398, 19.9125, 16.5123, 18.2841, 22.6601, 20.3975, 18.9315,\n",
            "        17.5879, 21.0669, 23.3387, 20.2252, 29.2578, 20.3039, 18.7756, 21.1006,\n",
            "        28.7160, 16.6387, 16.9502, 23.6963, 28.2415, 13.3489, 20.1111, 17.5115,\n",
            "        25.2379, 35.7876, 15.5143, 24.0760, 32.8114, 14.5090, 16.5912, 20.3548,\n",
            "        19.0141, 23.0307, 24.7587, 22.8136, 16.8056, 18.8744, 16.3666, 20.5895,\n",
            "        17.4578, 17.7247, 17.8769, 17.6453, 35.5676, 17.8745, 26.2482, 30.5439,\n",
            "        16.9721, 12.5524, 27.6650, 20.5477, 28.0712, 19.8670, 28.6142, 19.4138,\n",
            "        20.3222, 25.2501, 26.2956, 19.6345, 23.0602, 22.0858, 24.2346, 21.8535],\n",
            "       device='cuda:0'), dist b: tensor([34.2751, 27.3372, 32.2834, 15.2727, 22.1866, 29.6698, 28.1157, 28.4833,\n",
            "        31.5909, 36.9423, 21.5064, 21.4620, 30.3809, 19.8865, 32.1144, 22.4216,\n",
            "        29.9629, 29.7223, 26.0927, 38.5965, 33.5391, 31.2730, 24.8501, 27.0325,\n",
            "        23.5099, 37.1342, 26.4710, 32.4953, 23.8997, 23.7761, 31.8015, 27.0670,\n",
            "        31.5735, 27.5698, 27.4096, 23.9323, 30.9011, 20.6416, 23.2766, 26.1163,\n",
            "        20.3452, 19.9474, 25.5854, 23.9523, 26.6976, 25.8784, 34.9799, 27.0934,\n",
            "        26.3602, 24.1418, 35.4707, 28.3485, 27.0391, 28.6121, 16.7973, 23.1722,\n",
            "        24.2649, 22.6436, 17.4041, 41.7224, 29.6958, 22.2853, 36.9963, 24.4207],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.8205611705780029\n",
            "mini Batch Loss: 1.3302221298217773\n",
            "mini Batch Loss: 2.1262753009796143\n",
            "mini Batch Loss: 0.8435075283050537\n",
            "mini Batch Loss: 1.2812895774841309\n",
            "mini Batch Loss: 1.0621200799942017\n",
            "mini Batch Loss: 2.196659564971924\n",
            "mini Batch Loss: 1.1556456089019775\n",
            "mini Batch Loss: 0.867225170135498\n",
            "mini Batch Loss: 0.9070325493812561\n",
            "mini Batch Loss: 0.6690660119056702\n",
            "mini Batch Loss: 1.7872586250305176\n",
            "mini Batch Loss: 0.5078601241111755\n",
            "mini Batch Loss: 1.4229336977005005\n",
            "mini Batch Loss: 0.6737589240074158\n",
            "mini Batch Loss: 0.27665698528289795\n",
            "mini Batch Loss: 0.5255621671676636\n",
            "mini Batch Loss: 3.268763542175293\n",
            "mini Batch Loss: 0.9491645693778992\n",
            "mini Batch Loss: 2.012953996658325\n",
            "mini Batch Loss: 2.358518362045288\n",
            "mini Batch Loss: 2.3260226249694824\n",
            "mini Batch Loss: 0.9579653739929199\n",
            "mini Batch Loss: 1.429943561553955\n",
            "mini Batch Loss: 1.6764602661132812\n",
            "mini Batch Loss: 1.5660055875778198\n",
            "mini Batch Loss: 1.5351216793060303\n",
            "mini Batch Loss: 1.1782225370407104\n",
            "mini Batch Loss: 1.3499023914337158\n",
            "mini Batch Loss: 1.3509211540222168\n",
            "Training Batch: 391 | Training Loss: 1.3509211540222168\n",
            "Training Batch: 391 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_391.pt\n",
            "dist a: tensor([17.1154, 23.3957, 20.0985, 16.6463, 17.9156, 20.5559, 21.4942, 16.9239,\n",
            "        17.1109, 20.9222, 20.6657, 20.1828, 29.6981, 18.5456, 20.6179, 20.3190,\n",
            "        28.5940, 13.8227, 17.6358, 23.1729, 26.9485, 14.4389, 20.7529, 16.1265,\n",
            "        23.3475, 34.4885, 14.0710, 21.3378, 30.6975, 14.2745, 16.0716, 17.5715,\n",
            "        21.1499, 21.7618, 26.2326, 20.3510, 17.4810, 16.8629, 17.1564, 19.4146,\n",
            "        14.8774, 18.4216, 17.4652, 16.1285, 33.5617, 16.7143, 26.0656, 26.8719,\n",
            "        16.1520, 11.9903, 30.2838, 18.7907, 29.2409, 18.3016, 30.2285, 17.6579,\n",
            "        20.4646, 21.8474, 25.0206, 20.6937, 21.5479, 25.5467, 24.0510, 25.0031],\n",
            "       device='cuda:0'), dist b: tensor([39.3547, 26.3565, 33.6445, 14.7614, 21.7020, 28.4151, 31.6042, 25.0280,\n",
            "        34.9403, 38.4216, 21.0494, 22.2212, 29.9565, 20.9803, 34.0329, 23.1801,\n",
            "        31.1760, 32.2068, 28.5122, 37.3635, 30.2755, 31.4979, 26.6583, 27.6909,\n",
            "        22.4115, 32.5601, 28.9662, 31.4882, 23.0355, 22.5614, 31.0139, 24.1340,\n",
            "        32.3487, 31.0564, 29.2025, 24.9968, 27.8380, 19.0716, 25.2114, 28.5309,\n",
            "        22.8337, 24.1191, 24.5137, 24.8763, 26.6523, 28.6942, 36.0844, 30.6366,\n",
            "        25.8022, 25.1367, 37.8612, 25.7326, 24.1241, 25.9000, 14.7973, 24.9845,\n",
            "        23.9081, 20.7064, 16.6061, 42.6506, 28.4793, 21.5491, 39.3509, 26.5486],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 1.2948848009109497\n",
            "mini Batch Loss: 1.5059583187103271\n",
            "mini Batch Loss: 0.6023040413856506\n",
            "mini Batch Loss: 1.8847628831863403\n",
            "mini Batch Loss: 0.36166542768478394\n",
            "mini Batch Loss: 0.8882812857627869\n",
            "mini Batch Loss: 2.152904748916626\n",
            "mini Batch Loss: 0.08494901657104492\n",
            "mini Batch Loss: 1.6889761686325073\n",
            "mini Batch Loss: 2.2590181827545166\n",
            "mini Batch Loss: 0.6152141094207764\n",
            "mini Batch Loss: 1.2183834314346313\n",
            "mini Batch Loss: 0.5512240529060364\n",
            "mini Batch Loss: 0.556943416595459\n",
            "mini Batch Loss: 1.257732629776001\n",
            "mini Batch Loss: 1.012915849685669\n",
            "mini Batch Loss: 1.557510256767273\n",
            "mini Batch Loss: 0.37862515449523926\n",
            "mini Batch Loss: 0.45027104020118713\n",
            "mini Batch Loss: 0.08126118779182434\n",
            "mini Batch Loss: 1.5129961967468262\n",
            "mini Batch Loss: 1.3726767301559448\n",
            "mini Batch Loss: 0.6183072328567505\n",
            "mini Batch Loss: 2.240170478820801\n",
            "mini Batch Loss: 1.2657228708267212\n",
            "mini Batch Loss: 0.9124594926834106\n",
            "mini Batch Loss: 1.1945502758026123\n",
            "mini Batch Loss: 2.0727763175964355\n",
            "mini Batch Loss: 1.7534422874450684\n",
            "mini Batch Loss: 0.7939053177833557\n",
            "Training Batch: 421 | Training Loss: 0.7939053177833557\n",
            "Training Batch: 421 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_421.pt\n",
            "dist a: tensor([15.0704, 19.9352, 19.1150, 15.1541, 14.9747, 17.5816, 19.7900, 14.8359,\n",
            "        15.1750, 17.7346, 18.5565, 18.9200, 26.0772, 15.6478, 18.6656, 18.4637,\n",
            "        25.1724, 11.5872, 14.8259, 19.8880, 20.6736, 16.2581, 19.0087, 16.2007,\n",
            "        21.1172, 31.3551, 12.1865, 19.7463, 27.8395, 12.2535, 14.4839, 17.5203,\n",
            "        18.8718, 21.4490, 24.8764, 19.2530, 16.2825, 14.9598, 13.8550, 19.0198,\n",
            "        14.7220, 15.6748, 17.0966, 15.0356, 30.2219, 12.9408, 20.7148, 22.1234,\n",
            "        14.3238, 10.6326, 27.0491, 17.5774, 23.8481, 16.4590, 30.0431, 16.4738,\n",
            "        20.3968, 21.2217, 24.6263, 20.3321, 21.6641, 22.7177, 21.2801, 22.0592],\n",
            "       device='cuda:0'), dist b: tensor([31.2800, 24.0055, 27.6307, 12.8248, 22.0561, 21.9349, 27.4639, 22.1531,\n",
            "        27.8263, 30.1236, 18.6600, 18.4333, 23.9393, 17.8736, 27.8449, 20.0046,\n",
            "        24.2930, 27.9344, 22.4702, 32.1579, 25.1242, 24.5203, 21.6784, 25.7894,\n",
            "        21.1501, 29.6017, 24.0301, 25.3588, 20.0775, 19.2956, 27.9889, 21.9116,\n",
            "        27.8942, 24.9614, 28.3670, 20.6989, 24.2155, 16.7719, 20.0179, 24.8363,\n",
            "        19.0444, 19.3859, 22.7357, 24.1896, 21.8210, 24.2997, 30.5638, 25.0738,\n",
            "        19.1840, 19.7388, 32.4641, 23.6008, 20.9760, 22.9548, 12.1262, 18.4023,\n",
            "        18.2104, 21.2001, 14.7623, 37.9042, 26.0119, 16.1884, 31.6110, 22.5999],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "mini Batch Loss: 0.43562522530555725\n",
            "mini Batch Loss: 0.15023669600486755\n",
            "mini Batch Loss: 2.8102831840515137\n",
            "mini Batch Loss: 0.37305667996406555\n",
            "mini Batch Loss: 2.1203246116638184\n",
            "mini Batch Loss: 2.844146490097046\n",
            "mini Batch Loss: 0.5994811058044434\n",
            "mini Batch Loss: 1.5392322540283203\n",
            "mini Batch Loss: 1.1699540615081787\n",
            "mini Batch Loss: 1.2373464107513428\n",
            "mini Batch Loss: 1.0738074779510498\n",
            "mini Batch Loss: 0.4707457721233368\n",
            "mini Batch Loss: 0.05134071409702301\n",
            "mini Batch Loss: 1.1120420694351196\n",
            "mini Batch Loss: 0.6435032486915588\n",
            "mini Batch Loss: 0.8793900012969971\n",
            "mini Batch Loss: 0.4743613600730896\n",
            "mini Batch Loss: 0.6531525254249573\n",
            "mini Batch Loss: 1.4687671661376953\n",
            "mini Batch Loss: 0.7735466361045837\n",
            "mini Batch Loss: 1.230360746383667\n",
            "mini Batch Loss: 1.1499077081680298\n",
            "mini Batch Loss: 1.265213131904602\n",
            "mini Batch Loss: 0.869685709476471\n",
            "mini Batch Loss: 0.3538742661476135\n",
            "mini Batch Loss: 0.7611340284347534\n",
            "mini Batch Loss: 2.91709041595459\n",
            "mini Batch Loss: 1.4276522397994995\n",
            "mini Batch Loss: 1.9809074401855469\n",
            "mini Batch Loss: 0.6770819425582886\n",
            "Training Batch: 451 | Training Loss: 0.6770819425582886\n",
            "Training Batch: 451 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_451.pt\n",
            "dist a: tensor([11.3726, 14.7819, 14.4535, 11.7337, 11.4657, 11.8610, 15.4145, 12.9646,\n",
            "        11.5215, 14.0151, 15.0866, 13.8136, 17.2370, 12.3037, 15.5997, 13.1835,\n",
            "        22.3808,  7.3403, 11.7595, 14.7457, 16.1502, 11.0045, 15.1130, 12.7297,\n",
            "        16.3184, 23.3377, 10.4697, 14.2973, 21.8544, 10.1592, 11.1065, 12.8953,\n",
            "        13.7940, 16.7654, 18.0593, 15.1435, 13.1640, 11.0516, 10.6688, 14.0959,\n",
            "        11.3249, 13.7317, 13.0770, 11.5834, 25.4029, 12.6423, 15.7906, 19.7934,\n",
            "        12.7091,  8.5299, 21.4117, 13.4719, 17.5695, 13.0435, 21.5932, 12.3176,\n",
            "        13.6911, 16.5561, 17.1974, 15.4230, 15.6075, 18.3689, 17.3346, 17.0712],\n",
            "       device='cuda:0'), dist b: tensor([25.9644, 17.0022, 21.3737, 10.2297, 16.6252, 16.6775, 21.4068, 18.9902,\n",
            "        21.2072, 24.9337, 15.2900, 14.7387, 17.3443, 15.5348, 21.2980, 14.8820,\n",
            "        20.7884, 21.5768, 21.2872, 21.6361, 21.4612, 20.6846, 16.2810, 19.7132,\n",
            "        15.6687, 23.7377, 18.5267, 17.6019, 16.0000, 12.8118, 18.1650, 14.9815,\n",
            "        21.2485, 21.4827, 19.2922, 16.4450, 18.3604, 12.1578, 16.4014, 20.7915,\n",
            "        13.8191, 15.7862, 16.6162, 17.3286, 16.8152, 21.9176, 23.9698, 22.3939,\n",
            "        15.7120, 17.8206, 28.7222, 15.7273, 16.8207, 17.2408, 10.1181, 14.8827,\n",
            "        14.8707, 17.3234, 12.1124, 27.5448, 21.7435, 12.3325, 24.1274, 16.9339],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.7564862370491028\n",
            "mini Batch Loss: 1.2662379741668701\n",
            "mini Batch Loss: 0.7647477388381958\n",
            "mini Batch Loss: 0.6951185464859009\n",
            "mini Batch Loss: 0.4369010925292969\n",
            "mini Batch Loss: 0.5400868058204651\n",
            "mini Batch Loss: 3.218255043029785\n",
            "mini Batch Loss: 1.3556780815124512\n",
            "mini Batch Loss: 0.7237491607666016\n",
            "mini Batch Loss: 2.2524232864379883\n",
            "mini Batch Loss: 0.6067384481430054\n",
            "mini Batch Loss: 0.4944832921028137\n",
            "mini Batch Loss: 0.34122779965400696\n",
            "mini Batch Loss: 0.4999798536300659\n",
            "mini Batch Loss: 0.110783651471138\n",
            "mini Batch Loss: 2.4938244819641113\n",
            "mini Batch Loss: 0.8324969410896301\n",
            "mini Batch Loss: 1.3927017450332642\n",
            "mini Batch Loss: 0.8097573518753052\n",
            "mini Batch Loss: 0.6764293313026428\n",
            "mini Batch Loss: 1.3020215034484863\n",
            "mini Batch Loss: 4.66375732421875\n",
            "mini Batch Loss: 1.76901113986969\n",
            "mini Batch Loss: 1.63796067237854\n",
            "mini Batch Loss: 1.722706913948059\n",
            "mini Batch Loss: 1.8482170104980469\n",
            "mini Batch Loss: 2.024751901626587\n",
            "mini Batch Loss: 1.964507818222046\n",
            "mini Batch Loss: 1.4246217012405396\n",
            "mini Batch Loss: 2.052102565765381\n",
            "Training Batch: 481 | Training Loss: 2.052102565765381\n",
            "Training Batch: 481 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_481.pt\n",
            "dist a: tensor([16.7698, 23.6652, 21.7332, 18.8617, 15.8950, 20.6561, 22.5125, 19.9552,\n",
            "        16.7686, 19.8517, 23.7884, 18.7736, 28.1120, 17.7137, 19.5522, 20.7398,\n",
            "        29.6158, 13.0028, 15.8040, 22.6427, 24.7277, 15.8842, 21.5521, 17.6341,\n",
            "        25.3863, 30.9820, 17.6004, 23.4107, 35.5423, 14.7874, 18.6774, 18.5493,\n",
            "        19.9644, 22.3642, 29.0473, 25.4281, 18.9629, 16.5885, 16.1556, 24.1182,\n",
            "        16.6618, 20.9125, 19.2616, 17.3029, 35.9459, 18.6961, 25.6586, 31.2588,\n",
            "        16.0835, 11.8012, 30.3818, 19.7939, 26.5082, 16.8876, 26.2300, 19.3249,\n",
            "        19.4671, 27.5992, 30.6728, 22.7177, 24.6167, 21.9049, 29.4677, 25.6615],\n",
            "       device='cuda:0'), dist b: tensor([39.5262, 23.6078, 30.7365, 16.0785, 24.2609, 27.8668, 33.5725, 28.6669,\n",
            "        32.4413, 36.4049, 25.6342, 21.4816, 26.6581, 22.0677, 30.4352, 20.2988,\n",
            "        28.9677, 31.2486, 28.6933, 35.5709, 31.8042, 32.2980, 25.4764, 31.1183,\n",
            "        23.0226, 32.3447, 25.7249, 30.1679, 25.3667, 23.1948, 28.3713, 25.1225,\n",
            "        31.5805, 28.0799, 29.3743, 21.7035, 29.1472, 19.3801, 22.4349, 30.5662,\n",
            "        18.7037, 22.5816, 25.4431, 24.3224, 29.2724, 31.0292, 35.3979, 30.3082,\n",
            "        21.7931, 27.7617, 33.5952, 22.5840, 22.1217, 26.5301, 14.5942, 21.4952,\n",
            "        21.1639, 25.8265, 18.0985, 41.3753, 30.7138, 20.5317, 37.6167, 25.6106],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "mini Batch Loss: 1.9198658466339111\n",
            "mini Batch Loss: 1.8425251245498657\n",
            "mini Batch Loss: 2.314700126647949\n",
            "mini Batch Loss: 1.8690314292907715\n",
            "mini Batch Loss: 1.7998497486114502\n",
            "mini Batch Loss: 0.8222965002059937\n",
            "mini Batch Loss: 1.0883159637451172\n",
            "mini Batch Loss: 1.4411139488220215\n",
            "mini Batch Loss: 1.887977123260498\n",
            "mini Batch Loss: 1.6140506267547607\n",
            "mini Batch Loss: 1.5839520692825317\n",
            "mini Batch Loss: 0.9688622951507568\n",
            "mini Batch Loss: 2.5190012454986572\n",
            "mini Batch Loss: 1.4560073614120483\n",
            "mini Batch Loss: 2.4109644889831543\n",
            "mini Batch Loss: 2.3542585372924805\n",
            "mini Batch Loss: 1.7393264770507812\n",
            "mini Batch Loss: 1.4315741062164307\n",
            "mini Batch Loss: 1.0288543701171875\n",
            "mini Batch Loss: 1.0681698322296143\n",
            "mini Batch Loss: 2.0293736457824707\n",
            "mini Batch Loss: 1.3226876258850098\n",
            "mini Batch Loss: 1.532712697982788\n",
            "mini Batch Loss: 1.6796271800994873\n",
            "mini Batch Loss: 2.3490450382232666\n",
            "mini Batch Loss: 1.4033282995224\n",
            "mini Batch Loss: 1.045583724975586\n",
            "mini Batch Loss: 1.0207339525222778\n",
            "mini Batch Loss: 1.9935235977172852\n",
            "mini Batch Loss: 1.800362229347229\n",
            "Training Batch: 511 | Training Loss: 1.800362229347229\n",
            "Training Batch: 511 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_511.pt\n",
            "dist a: tensor([16.1769, 24.3402, 19.5298, 16.0091, 18.6015, 19.2187, 20.2569, 15.9930,\n",
            "        16.7167, 18.1560, 22.4151, 18.6711, 24.3182, 17.7008, 17.5061, 22.4793,\n",
            "        24.3525, 10.2721, 15.8533, 20.5140, 24.3775, 19.6368, 19.3209, 16.4481,\n",
            "        23.5548, 31.1888, 14.1426, 21.0745, 27.6090, 14.7025, 14.7981, 17.0996,\n",
            "        20.0069, 22.6394, 25.5876, 25.7922, 19.2776, 15.4476, 15.9219, 24.5346,\n",
            "        17.1465, 17.5873, 19.8047, 17.7319, 33.4358, 15.9690, 23.6278, 28.1678,\n",
            "        18.3687, 10.7174, 27.4804, 18.9779, 23.7551, 15.5987, 27.6890, 18.0940,\n",
            "        16.9976, 24.0884, 25.4512, 20.5891, 25.3677, 20.6718, 23.7266, 24.8737],\n",
            "       device='cuda:0'), dist b: tensor([34.7890, 23.9132, 32.0615, 14.7536, 22.1247, 27.4229, 29.3930, 25.0891,\n",
            "        32.6387, 35.5981, 21.1917, 23.9571, 23.9224, 19.3882, 32.0040, 20.5281,\n",
            "        26.7247, 31.4303, 24.2947, 32.6756, 32.1967, 29.5941, 22.4591, 23.6106,\n",
            "        23.0970, 31.7041, 23.4986, 32.0949, 21.6457, 18.6941, 25.5334, 24.7798,\n",
            "        34.8447, 26.8288, 25.3594, 24.0528, 29.3322, 18.1596, 23.6683, 28.9797,\n",
            "        21.4615, 19.1690, 26.7785, 24.1688, 23.1830, 31.7156, 33.6551, 29.1313,\n",
            "        18.5139, 25.8485, 34.3371, 21.7479, 23.9753, 27.7438, 16.5903, 22.7587,\n",
            "        17.9026, 27.6085, 17.4794, 36.2977, 29.8313, 23.8583, 38.9274, 27.5300],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.8125 \n",
            "tensor(0.8125)\n",
            "mini Batch Loss: 1.5178561210632324\n",
            "mini Batch Loss: 2.0941152572631836\n",
            "mini Batch Loss: 1.58356773853302\n",
            "mini Batch Loss: 2.191342830657959\n",
            "mini Batch Loss: 1.123935580253601\n",
            "mini Batch Loss: 1.7282402515411377\n",
            "mini Batch Loss: 0.7699719667434692\n",
            "mini Batch Loss: 1.763529658317566\n",
            "mini Batch Loss: 1.32845139503479\n",
            "mini Batch Loss: 1.2864569425582886\n",
            "mini Batch Loss: 1.7590841054916382\n",
            "mini Batch Loss: 0.5054690837860107\n",
            "mini Batch Loss: 1.0546976327896118\n",
            "mini Batch Loss: 0.8395964503288269\n",
            "mini Batch Loss: 0.15590621531009674\n",
            "mini Batch Loss: 2.247627019882202\n",
            "mini Batch Loss: 1.3171929121017456\n",
            "mini Batch Loss: 0.594912052154541\n",
            "mini Batch Loss: 0.7628191709518433\n",
            "mini Batch Loss: 0.49284887313842773\n",
            "mini Batch Loss: 1.320388913154602\n",
            "mini Batch Loss: 0.4332485795021057\n",
            "mini Batch Loss: 2.9170687198638916\n",
            "mini Batch Loss: 0.928604006767273\n",
            "mini Batch Loss: 0.3404150605201721\n",
            "mini Batch Loss: 0.0483953058719635\n",
            "mini Batch Loss: 2.09592342376709\n",
            "mini Batch Loss: 0.605158805847168\n",
            "mini Batch Loss: 0.42630189657211304\n",
            "mini Batch Loss: 0.848543107509613\n",
            "Training Batch: 541 | Training Loss: 0.848543107509613\n",
            "Training Batch: 541 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_541.pt\n",
            "dist a: tensor([ 8.9936, 14.3901, 10.9286, 10.0370, 11.5355, 10.9914, 11.8783, 10.5261,\n",
            "        10.7375, 10.0557, 14.4396, 12.4766, 16.1449, 11.4805, 11.2763, 12.1971,\n",
            "        18.4978,  6.5656, 10.2067, 11.4444, 15.9452, 11.6307,  9.8633, 10.5292,\n",
            "        14.5877, 21.6659,  8.8307, 11.1042, 15.3245,  9.4256,  9.7475,  9.4531,\n",
            "        14.7397, 16.2398, 16.1058, 16.2712, 10.7993,  8.6117,  9.6575, 13.1296,\n",
            "        11.2443, 11.4927, 10.1571, 11.3131, 20.9386,  8.6997, 15.2316, 16.4113,\n",
            "        11.4916,  6.5607, 20.2635, 11.5098, 16.0722, 11.8157, 19.4301,  9.7536,\n",
            "        10.9213, 14.2105, 12.8060, 13.3719, 16.6745, 16.2642, 15.1622, 15.5197],\n",
            "       device='cuda:0'), dist b: tensor([23.8694, 14.4124, 21.8244,  8.9377, 15.6431, 15.9994, 16.8641, 16.5631,\n",
            "        18.5574, 20.6688, 12.4039, 14.8567, 16.4475, 13.9259, 20.7007, 12.3368,\n",
            "        19.1117, 22.5081, 17.5857, 18.9995, 18.0895, 18.9396, 15.5570, 14.5272,\n",
            "        15.2124, 25.7570, 13.6846, 18.2219, 13.8287, 11.4983, 15.1412, 14.2440,\n",
            "        22.9863, 17.6961, 17.3823, 16.8766, 20.0071, 10.8533, 14.8735, 18.9302,\n",
            "        13.2961, 14.5259, 14.3948, 17.1200, 13.4052, 20.7826, 20.8785, 22.9349,\n",
            "        13.9873, 15.7511, 25.2376, 13.3310, 14.0605, 16.8743, 10.2741, 14.9240,\n",
            "        12.2342, 16.1222, 10.4594, 27.1171, 18.6757, 12.2053, 23.9321, 18.7315],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 1.5133709907531738\n",
            "mini Batch Loss: 2.3783249855041504\n",
            "mini Batch Loss: 0.5545167922973633\n",
            "mini Batch Loss: 1.6671490669250488\n",
            "mini Batch Loss: 2.0845110416412354\n",
            "mini Batch Loss: 1.8466626405715942\n",
            "mini Batch Loss: 1.5512268543243408\n",
            "mini Batch Loss: 1.8537471294403076\n",
            "mini Batch Loss: 1.5017943382263184\n",
            "mini Batch Loss: 1.3569509983062744\n",
            "mini Batch Loss: 0.629814624786377\n",
            "mini Batch Loss: 2.073669910430908\n",
            "mini Batch Loss: 0.7268907427787781\n",
            "mini Batch Loss: 1.9358093738555908\n",
            "mini Batch Loss: 1.5409181118011475\n",
            "mini Batch Loss: 0.9592625498771667\n",
            "mini Batch Loss: 1.3309178352355957\n",
            "mini Batch Loss: 1.063768982887268\n",
            "mini Batch Loss: 2.9726874828338623\n",
            "mini Batch Loss: 1.6942437887191772\n",
            "mini Batch Loss: 0.7650713920593262\n",
            "mini Batch Loss: 0.8915719389915466\n",
            "mini Batch Loss: 0.3109132647514343\n",
            "mini Batch Loss: 1.9910366535186768\n",
            "mini Batch Loss: 0.7948876023292542\n",
            "mini Batch Loss: 0.20378227531909943\n",
            "mini Batch Loss: 0.3733850121498108\n",
            "mini Batch Loss: 2.052661418914795\n",
            "mini Batch Loss: 0.4306028187274933\n",
            "mini Batch Loss: 2.203099012374878\n",
            "Training Batch: 571 | Training Loss: 2.203099012374878\n",
            "Training Batch: 571 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_571.pt\n",
            "dist a: tensor([ 9.8424, 13.8789, 13.1474, 12.0013, 12.3254, 14.8844, 13.6356, 11.9446,\n",
            "        11.4510, 14.0129, 14.8921, 15.0141, 18.3266, 12.8460, 12.6444, 13.3881,\n",
            "        20.7433,  8.4716, 12.2480, 13.5838, 17.5546, 11.5624, 14.3807, 12.5019,\n",
            "        17.0037, 25.6280,  9.7228, 12.8388, 15.0726, 10.4351,  9.6123, 12.1274,\n",
            "        16.9348, 15.5930, 17.8875, 17.1802, 12.9665, 10.3627, 10.0165, 16.1426,\n",
            "        11.3529, 11.6620, 10.9325, 11.1784, 22.6326, 10.6553, 17.2227, 17.5320,\n",
            "        11.8125,  8.4883, 22.9592, 13.2328, 20.0965, 11.6711, 22.7483, 11.0103,\n",
            "        12.9792, 15.3961, 15.1550, 13.2003, 17.4918, 16.7080, 18.4686, 19.1425],\n",
            "       device='cuda:0'), dist b: tensor([26.2018, 19.0432, 24.4980, 11.9708, 17.6379, 18.2063, 20.6697, 18.1932,\n",
            "        22.1363, 26.2162, 16.0584, 18.9860, 16.9614, 16.2128, 20.9969, 13.4422,\n",
            "        19.7883, 25.5550, 21.1463, 19.2837, 21.2493, 22.0837, 16.6694, 18.2433,\n",
            "        17.9543, 28.3928, 15.4394, 21.4835, 15.0227, 14.4815, 17.0839, 15.9509,\n",
            "        26.3435, 21.7033, 20.6452, 19.9304, 19.9220, 14.4713, 13.5014, 22.5304,\n",
            "        13.0409, 16.3317, 14.4348, 16.8571, 16.7029, 22.1421, 21.2093, 26.6899,\n",
            "        15.0268, 18.0122, 31.4117, 14.6131, 13.9589, 16.7481, 11.9776, 17.7193,\n",
            "        16.1639, 16.8359, 11.6827, 26.9609, 19.4236, 15.1139, 25.4240, 19.1613],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.6731194257736206\n",
            "mini Batch Loss: 0.7390720248222351\n",
            "mini Batch Loss: 1.6732652187347412\n",
            "mini Batch Loss: 0.9117767810821533\n",
            "mini Batch Loss: 0.7842504978179932\n",
            "mini Batch Loss: 1.3321940898895264\n",
            "mini Batch Loss: 0.2352634221315384\n",
            "mini Batch Loss: 0.1098298728466034\n",
            "mini Batch Loss: 0.16483542323112488\n",
            "mini Batch Loss: 2.615872859954834\n",
            "mini Batch Loss: 1.949188470840454\n",
            "mini Batch Loss: 0.8715949058532715\n",
            "mini Batch Loss: 0.9944028854370117\n",
            "mini Batch Loss: 0.6973751783370972\n",
            "mini Batch Loss: 0.18960753083229065\n",
            "mini Batch Loss: 1.3871636390686035\n",
            "mini Batch Loss: 0.2989620864391327\n",
            "mini Batch Loss: 0.6376702189445496\n",
            "mini Batch Loss: 5.019755840301514\n",
            "mini Batch Loss: 2.040794610977173\n",
            "mini Batch Loss: 1.284260630607605\n",
            "mini Batch Loss: 1.2564239501953125\n",
            "mini Batch Loss: 0.8721694946289062\n",
            "mini Batch Loss: 1.402889370918274\n",
            "mini Batch Loss: 0.9534657001495361\n",
            "mini Batch Loss: 2.4405441284179688\n",
            "mini Batch Loss: 0.8379659652709961\n",
            "mini Batch Loss: 1.218017816543579\n",
            "mini Batch Loss: 0.2559482455253601\n",
            "mini Batch Loss: 1.1097286939620972\n",
            "Training Batch: 601 | Training Loss: 1.1097286939620972\n",
            "Training Batch: 601 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_601.pt\n",
            "dist a: tensor([11.9896, 16.0982, 14.2818, 12.1933, 12.0910, 16.9106, 14.8883, 13.1404,\n",
            "        11.6895, 14.3349, 16.5582, 14.4584, 19.1290, 13.0368, 12.6225, 13.1042,\n",
            "        19.1144,  8.8244, 13.1988, 14.5763, 18.0333, 12.1835, 14.3190, 12.6293,\n",
            "        18.5491, 25.6690, 10.3510, 13.7756, 14.6691, 12.3456, 10.0157, 12.9281,\n",
            "        18.4143, 15.2698, 20.1493, 17.1530, 13.9789, 12.4580, 11.1054, 17.6335,\n",
            "        11.5643, 12.8294, 12.0722, 11.2836, 22.7475, 11.9235, 17.6434, 18.6150,\n",
            "        12.8546,  8.9846, 23.7743, 13.3412, 18.5557, 12.5263, 22.2231, 12.6622,\n",
            "        12.7780, 16.2231, 17.7022, 13.7814, 16.7329, 17.2637, 19.3976, 20.5185],\n",
            "       device='cuda:0'), dist b: tensor([27.4590, 18.2242, 24.6763, 11.5966, 18.6761, 18.5331, 21.3786, 18.2983,\n",
            "        25.3686, 25.6312, 17.7508, 18.6119, 17.6530, 16.0315, 22.4283, 12.8058,\n",
            "        19.7223, 25.3946, 20.2100, 19.5430, 21.6934, 22.6507, 17.3977, 18.9349,\n",
            "        19.1548, 24.9858, 15.7876, 22.7496, 15.7089, 15.0122, 19.1230, 19.1134,\n",
            "        28.8247, 19.5547, 21.8376, 21.1672, 20.4990, 14.9769, 12.1653, 23.1123,\n",
            "        13.4381, 17.2566, 16.8586, 16.3064, 17.2040, 23.7422, 23.1287, 26.6276,\n",
            "        15.0522, 18.9682, 29.8841, 16.3196, 16.1962, 17.2271, 13.4287, 18.1347,\n",
            "        16.7871, 18.4922, 12.9098, 29.1616, 19.4605, 15.5471, 27.2431, 19.8765],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 1.9541547298431396\n",
            "mini Batch Loss: 3.155520439147949\n",
            "mini Batch Loss: 1.6134989261627197\n",
            "mini Batch Loss: 0.7362274527549744\n",
            "mini Batch Loss: 1.3290122747421265\n",
            "mini Batch Loss: 1.1607987880706787\n",
            "mini Batch Loss: 1.2633824348449707\n",
            "mini Batch Loss: 1.5819802284240723\n",
            "mini Batch Loss: 1.8106234073638916\n",
            "mini Batch Loss: 0.716759443283081\n",
            "mini Batch Loss: 0.7506179213523865\n",
            "mini Batch Loss: 1.2004988193511963\n",
            "mini Batch Loss: 0.01041547954082489\n",
            "mini Batch Loss: 0.4848017692565918\n",
            "mini Batch Loss: 1.2701454162597656\n",
            "mini Batch Loss: 1.7602850198745728\n",
            "mini Batch Loss: 1.2525641918182373\n",
            "mini Batch Loss: 0.9077212810516357\n",
            "mini Batch Loss: 2.2635576725006104\n",
            "mini Batch Loss: 0.3108760714530945\n",
            "mini Batch Loss: 1.1384830474853516\n",
            "mini Batch Loss: 0.7985281944274902\n",
            "mini Batch Loss: 1.5463097095489502\n",
            "mini Batch Loss: 0.9747617840766907\n",
            "mini Batch Loss: 0.9973765015602112\n",
            "mini Batch Loss: 0.4544403553009033\n",
            "mini Batch Loss: 0.3012066185474396\n",
            "mini Batch Loss: 0.4042889177799225\n",
            "mini Batch Loss: 1.3001494407653809\n",
            "mini Batch Loss: 2.044921636581421\n",
            "Training Batch: 631 | Training Loss: 2.044921636581421\n",
            "Training Batch: 631 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_631.pt\n",
            "dist a: tensor([13.1354, 14.5905, 13.4143, 11.3535, 10.3020, 15.5075, 14.0089, 12.0031,\n",
            "        10.1246, 13.7278, 13.9182, 13.0571, 18.7371, 11.6472, 12.8826, 13.8407,\n",
            "        19.2897,  7.7767, 12.0829, 14.2700, 16.9957, 12.6436, 13.9922, 12.2377,\n",
            "        16.3491, 24.9766, 10.1136, 12.9980, 13.2165, 11.2802, 10.9315, 12.1366,\n",
            "        19.0584, 14.8728, 16.6332, 15.7709, 13.2365, 11.4199, 10.7743, 16.0132,\n",
            "        11.9250, 13.2507, 12.4211, 10.9298, 23.9928, 12.0610, 17.0065, 20.3425,\n",
            "        11.4106,  7.7690, 20.5780, 13.6363, 15.3115, 12.1135, 20.7330, 12.3029,\n",
            "        11.2410, 16.3009, 16.7962, 15.7718, 17.8669, 17.6451, 18.7848, 18.2547],\n",
            "       device='cuda:0'), dist b: tensor([25.0351, 15.4714, 21.0204, 11.0491, 18.1009, 18.9343, 20.9153, 16.4280,\n",
            "        22.7120, 25.6279, 15.1456, 16.0912, 16.1911, 15.5658, 18.8181, 12.4874,\n",
            "        18.1792, 24.3475, 18.8343, 20.4160, 19.5335, 19.7029, 15.5720, 18.6373,\n",
            "        19.0399, 26.0985, 13.4811, 20.6965, 15.0215, 12.6152, 17.6014, 18.9801,\n",
            "        27.2305, 19.4685, 18.6619, 17.9880, 20.5664, 14.0521, 11.1955, 21.3279,\n",
            "        13.4770, 16.1868, 16.9371, 19.1496, 16.6551, 22.6144, 22.3587, 24.8479,\n",
            "        14.6518, 15.7841, 26.1672, 16.1705, 15.2174, 16.2461, 10.3546, 16.5433,\n",
            "        15.6632, 18.5301, 12.1369, 31.5918, 20.4804, 15.2601, 26.5061, 17.9604],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 1.1651220321655273\n",
            "mini Batch Loss: 0.6401556730270386\n",
            "mini Batch Loss: 0.8041802644729614\n",
            "mini Batch Loss: 1.305872917175293\n",
            "mini Batch Loss: 1.1415044069290161\n",
            "mini Batch Loss: 2.4233784675598145\n",
            "mini Batch Loss: 1.0050921440124512\n",
            "mini Batch Loss: 1.1839243173599243\n",
            "mini Batch Loss: 2.7654881477355957\n",
            "mini Batch Loss: 0.7717850208282471\n",
            "mini Batch Loss: 0.15234878659248352\n",
            "mini Batch Loss: 0.3624834418296814\n",
            "mini Batch Loss: 0.0\n",
            "mini Batch Loss: 0.41987597942352295\n",
            "mini Batch Loss: 0.008129075169563293\n",
            "mini Batch Loss: 2.449937582015991\n",
            "mini Batch Loss: 0.5067437887191772\n",
            "mini Batch Loss: 0.4584085941314697\n",
            "mini Batch Loss: 0.462552547454834\n",
            "mini Batch Loss: 1.6290323734283447\n",
            "mini Batch Loss: 1.255856990814209\n",
            "mini Batch Loss: 0.2784075438976288\n",
            "mini Batch Loss: 1.876537561416626\n",
            "mini Batch Loss: 2.5166797637939453\n",
            "mini Batch Loss: 2.0782241821289062\n",
            "mini Batch Loss: 0.6905950307846069\n",
            "mini Batch Loss: 0.8131090402603149\n",
            "mini Batch Loss: 1.2171965837478638\n",
            "mini Batch Loss: 1.3990135192871094\n",
            "mini Batch Loss: 0.10662628710269928\n",
            "Training Batch: 661 | Training Loss: 0.10662628710269928\n",
            "Training Batch: 661 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_661.pt\n",
            "dist a: tensor([13.7967, 16.9978, 16.4967, 13.5961, 13.4724, 16.2906, 16.5072, 15.6396,\n",
            "        11.3558, 15.6332, 17.1365, 15.1589, 20.8576, 14.1668, 14.5289, 14.1529,\n",
            "        20.3174,  8.0782, 14.8522, 16.3379, 22.3972, 11.9579, 14.6194, 11.8126,\n",
            "        19.3148, 29.1996, 11.9171, 15.5533, 16.0474, 13.1758, 12.2277, 12.6445,\n",
            "        19.1607, 18.7070, 22.5537, 19.1083, 14.0573, 12.4679, 11.9909, 19.5000,\n",
            "        14.3185, 16.1096, 13.9856, 11.6440, 24.4538, 12.5536, 18.0662, 19.5576,\n",
            "        13.0834,  8.1726, 24.3262, 15.1067, 21.7961, 14.6215, 23.4676, 14.1379,\n",
            "        15.3114, 20.0633, 18.7202, 16.2985, 20.9084, 16.9205, 20.5651, 21.1613],\n",
            "       device='cuda:0'), dist b: tensor([29.3539, 21.2935, 26.8402, 12.5772, 18.8686, 20.6635, 25.7041, 19.6165,\n",
            "        27.1207, 28.2930, 19.3333, 19.0286, 20.7758, 15.1854, 25.9821, 14.7380,\n",
            "        20.5294, 24.3334, 21.6518, 25.1964, 25.2424, 24.3751, 17.3462, 20.9804,\n",
            "        21.1148, 31.0462, 18.0725, 24.4831, 17.5558, 16.5247, 19.1342, 19.4695,\n",
            "        29.9422, 22.0890, 21.4476, 18.7751, 23.5281, 14.7560, 14.2207, 25.5688,\n",
            "        14.9456, 18.3339, 18.2604, 20.1740, 17.9386, 25.6751, 27.0198, 27.5618,\n",
            "        19.3121, 20.3560, 29.8909, 21.0965, 15.3384, 19.0789, 13.4696, 19.7987,\n",
            "        18.8732, 23.3765, 12.7357, 32.4923, 21.3044, 18.0263, 30.5365, 20.6768],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.7994855642318726\n",
            "mini Batch Loss: 1.569649338722229\n",
            "mini Batch Loss: 1.1636195182800293\n",
            "mini Batch Loss: 1.312383770942688\n",
            "mini Batch Loss: 0.8231515884399414\n",
            "mini Batch Loss: 3.9259400367736816\n",
            "mini Batch Loss: 1.085871934890747\n",
            "mini Batch Loss: 2.1849496364593506\n",
            "mini Batch Loss: 0.6873384714126587\n",
            "mini Batch Loss: 0.3044641613960266\n",
            "mini Batch Loss: 0.7891684770584106\n",
            "mini Batch Loss: 1.2524429559707642\n",
            "mini Batch Loss: 0.7178976535797119\n",
            "mini Batch Loss: 0.45646241307258606\n",
            "mini Batch Loss: 0.4220849275588989\n",
            "mini Batch Loss: 0.37018445134162903\n",
            "mini Batch Loss: 0.5959909558296204\n",
            "mini Batch Loss: 0.5032503604888916\n",
            "mini Batch Loss: 1.491173267364502\n",
            "mini Batch Loss: 1.3328089714050293\n",
            "mini Batch Loss: 1.0622491836547852\n",
            "mini Batch Loss: 1.4109479188919067\n",
            "mini Batch Loss: 2.6941895484924316\n",
            "mini Batch Loss: 1.0492745637893677\n",
            "mini Batch Loss: 0.9674916863441467\n",
            "mini Batch Loss: 0.2837868332862854\n",
            "mini Batch Loss: 2.105739116668701\n",
            "mini Batch Loss: 1.002545714378357\n",
            "mini Batch Loss: 0.4169917702674866\n",
            "mini Batch Loss: 4.540183067321777\n",
            "Training Batch: 691 | Training Loss: 4.540183067321777\n",
            "Training Batch: 691 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_691.pt\n",
            "dist a: tensor([11.4667, 12.4929, 14.2057, 11.6352, 13.9848, 12.4924, 15.0852, 13.6970,\n",
            "        10.2425, 13.5005, 14.1637, 11.0369, 18.6335, 12.3554, 12.7510, 10.8746,\n",
            "        19.3592,  6.7327, 13.1014, 14.7102, 18.9742, 12.2379, 13.6837,  9.9830,\n",
            "        15.2443, 23.3420, 10.4825, 13.2987, 12.9802, 11.2585, 10.4274, 11.0908,\n",
            "        15.5239, 14.5001, 17.8569, 16.4068, 12.4570,  9.3954, 12.0587, 14.7530,\n",
            "        12.9576, 12.8379, 12.9360, 10.2555, 23.4522, 12.1564, 14.4534, 17.7943,\n",
            "        11.5106,  7.7145, 20.7619, 13.5368, 18.5333, 12.4322, 20.0884, 10.2834,\n",
            "        13.9971, 15.3042, 17.6407, 14.4138, 18.0720, 12.2492, 19.2664, 18.0620],\n",
            "       device='cuda:0'), dist b: tensor([23.8066, 17.1880, 23.8363, 11.8282, 14.5465, 16.7022, 20.4509, 17.3757,\n",
            "        22.1915, 25.5617, 17.7857, 15.3074, 19.7673, 12.0278, 21.6613, 11.9322,\n",
            "        19.4322, 20.4966, 18.1437, 21.9148, 19.6126, 18.2206, 14.5913, 18.9840,\n",
            "        16.5614, 26.4249, 14.9056, 21.5496, 14.9651, 13.0086, 17.9861, 15.4452,\n",
            "        21.9303, 21.7082, 20.1426, 18.9709, 18.4904, 11.6105, 14.1759, 20.4381,\n",
            "        13.2015, 16.0966, 16.2065, 17.0846, 16.2711, 21.3170, 23.6816, 22.6415,\n",
            "        19.5113, 17.4704, 29.1954, 15.6502, 13.4292, 15.7391, 10.7802, 15.9106,\n",
            "        14.1636, 17.4897, 11.5097, 24.3989, 19.8799, 18.7634, 25.5173, 18.2353],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.921875 \n",
            "tensor(0.9219)\n",
            "mini Batch Loss: 0.6434698700904846\n",
            "mini Batch Loss: 0.008715242147445679\n",
            "mini Batch Loss: 2.856180191040039\n",
            "mini Batch Loss: 1.6866860389709473\n",
            "mini Batch Loss: 0.9710325002670288\n",
            "mini Batch Loss: 0.2393433302640915\n",
            "mini Batch Loss: 2.1426901817321777\n",
            "mini Batch Loss: 1.9875859022140503\n",
            "mini Batch Loss: 1.425779104232788\n",
            "mini Batch Loss: 1.348395824432373\n",
            "mini Batch Loss: 1.0485671758651733\n",
            "mini Batch Loss: 0.9851201772689819\n",
            "mini Batch Loss: 1.8700429201126099\n",
            "mini Batch Loss: 0.14786964654922485\n",
            "mini Batch Loss: 0.5906882882118225\n",
            "mini Batch Loss: 0.19600138068199158\n",
            "mini Batch Loss: 0.9994827508926392\n",
            "mini Batch Loss: 0.6778100728988647\n",
            "mini Batch Loss: 0.010476723313331604\n",
            "mini Batch Loss: 0.7305727005004883\n",
            "mini Batch Loss: 1.7267271280288696\n",
            "mini Batch Loss: 0.3724748492240906\n",
            "mini Batch Loss: 0.602986216545105\n",
            "mini Batch Loss: 0.5170378684997559\n",
            "mini Batch Loss: 2.458977460861206\n",
            "mini Batch Loss: 1.4212695360183716\n",
            "mini Batch Loss: 1.9272428750991821\n",
            "mini Batch Loss: 1.8091055154800415\n",
            "mini Batch Loss: 0.5294158458709717\n",
            "mini Batch Loss: 0.98160719871521\n",
            "Training Batch: 721 | Training Loss: 0.98160719871521\n",
            "Training Batch: 721 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_721.pt\n",
            "dist a: tensor([15.6061, 17.3656, 18.2282, 15.6078, 14.3344, 17.4364, 19.8714, 16.1187,\n",
            "        13.7013, 17.1790, 16.7763, 14.3684, 23.9696, 15.5558, 16.4242, 14.6035,\n",
            "        23.9741, 12.1291, 16.9918, 16.1359, 23.4619, 11.2325, 17.7603, 12.2398,\n",
            "        17.3990, 32.6052, 12.7850, 18.5857, 18.8312, 13.0733, 14.9342, 14.1212,\n",
            "        21.7706, 18.9163, 21.5592, 20.3812, 16.1353, 14.7057, 13.1463, 17.2093,\n",
            "        14.4491, 18.1822, 15.1211, 13.6119, 25.8643, 15.7040, 19.5534, 22.9354,\n",
            "        14.3836,  9.4026, 26.3828, 16.1864, 23.2787, 14.3575, 27.7399, 12.2277,\n",
            "        14.7328, 19.8055, 21.9428, 20.4754, 22.8265, 15.2958, 25.3003, 22.0855],\n",
            "       device='cuda:0'), dist b: tensor([30.4243, 24.0614, 29.3296, 14.7926, 23.0290, 20.7008, 26.5312, 19.4922,\n",
            "        28.6391, 29.8720, 19.6019, 18.3411, 23.9607, 17.9895, 25.5015, 16.3845,\n",
            "        22.8658, 28.0934, 23.3064, 24.4828, 26.4266, 25.2620, 19.3222, 21.5626,\n",
            "        22.7594, 36.7019, 20.8432, 26.2493, 17.9027, 15.0188, 21.8032, 21.6582,\n",
            "        31.9513, 22.6394, 24.2319, 20.9771, 23.0414, 17.1975, 15.0409, 26.1482,\n",
            "        16.6981, 19.8400, 21.5754, 22.9518, 21.6290, 25.4921, 29.8671, 27.4593,\n",
            "        21.0569, 19.0911, 32.4699, 20.6530, 19.0573, 18.7488, 14.7204, 22.3911,\n",
            "        17.1205, 24.0599, 16.9300, 34.8704, 23.6382, 26.4796, 31.9941, 22.9800],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 1.114426851272583\n",
            "mini Batch Loss: 0.693418025970459\n",
            "mini Batch Loss: 0.715843677520752\n",
            "mini Batch Loss: 1.2614343166351318\n",
            "mini Batch Loss: 0.7157922983169556\n",
            "mini Batch Loss: 0.7076117396354675\n",
            "mini Batch Loss: 0.6677089333534241\n",
            "mini Batch Loss: 0.4820293188095093\n",
            "mini Batch Loss: 0.5814248323440552\n",
            "mini Batch Loss: 0.48877209424972534\n",
            "mini Batch Loss: 2.0365171432495117\n",
            "mini Batch Loss: 2.4994003772735596\n",
            "mini Batch Loss: 1.037787675857544\n",
            "mini Batch Loss: 1.0751515626907349\n",
            "mini Batch Loss: 1.1448211669921875\n",
            "mini Batch Loss: 0.49360889196395874\n",
            "mini Batch Loss: 0.6512141227722168\n",
            "mini Batch Loss: 2.8176755905151367\n",
            "mini Batch Loss: 0.7622417211532593\n",
            "mini Batch Loss: 0.05853681266307831\n",
            "mini Batch Loss: 0.38299262523651123\n",
            "mini Batch Loss: 1.219260811805725\n",
            "mini Batch Loss: 0.514621913433075\n",
            "mini Batch Loss: 2.1687116622924805\n",
            "mini Batch Loss: 0.07155431807041168\n",
            "mini Batch Loss: 1.0985424518585205\n",
            "mini Batch Loss: 0.4053683876991272\n",
            "mini Batch Loss: 1.6269537210464478\n",
            "mini Batch Loss: 1.0618529319763184\n",
            "mini Batch Loss: 1.579122543334961\n",
            "Training Batch: 751 | Training Loss: 1.579122543334961\n",
            "Training Batch: 751 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_751.pt\n",
            "dist a: tensor([13.4676, 16.7546, 14.4119, 14.4280, 12.6273, 17.1691, 16.7300, 14.9809,\n",
            "        12.4031, 16.5321, 13.8999, 13.4129, 22.7844, 13.6710, 16.4613, 13.8373,\n",
            "        21.6660,  8.0073, 13.2715, 14.9655, 21.0450, 11.4983, 15.9310, 11.1090,\n",
            "        18.4695, 25.6132, 12.4496, 17.7902, 18.1674, 13.2156, 15.2250, 13.1732,\n",
            "        20.4184, 16.9474, 18.0533, 19.7664, 16.5368, 11.1114, 11.9594, 16.9103,\n",
            "        12.6231, 16.4058, 16.0032, 13.1286, 24.3980, 12.1156, 19.3789, 23.0571,\n",
            "        13.3852,  8.9927, 27.1016, 15.5154, 20.5230, 12.4339, 23.1356, 11.7407,\n",
            "        15.6788, 19.2014, 17.3123, 17.5669, 20.1527, 13.2249, 21.6464, 20.0064],\n",
            "       device='cuda:0'), dist b: tensor([28.8934, 23.5421, 26.0438, 13.7546, 18.6043, 20.3757, 23.3924, 18.1803,\n",
            "        27.5070, 27.8498, 15.7034, 17.7967, 23.5491, 15.1372, 24.2489, 15.2851,\n",
            "        20.8962, 23.6984, 21.2885, 27.0402, 22.0483, 23.1401, 23.5751, 20.5690,\n",
            "        22.0240, 28.7056, 17.8571, 22.0594, 16.9024, 14.6267, 20.3164, 17.6781,\n",
            "        29.7421, 19.2200, 22.4173, 20.3625, 20.2564, 12.8738, 15.4302, 24.3069,\n",
            "        17.4338, 18.6433, 20.1006, 19.4134, 20.1122, 23.8578, 28.5507, 24.6751,\n",
            "        20.0257, 17.9445, 31.0291, 18.6233, 18.3444, 18.6187, 13.4348, 19.5778,\n",
            "        14.6535, 21.8555, 16.1400, 32.2199, 21.0721, 26.3002, 26.6654, 21.1955],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 1.4353582859039307\n",
            "mini Batch Loss: 1.280150294303894\n",
            "mini Batch Loss: 2.856280565261841\n",
            "mini Batch Loss: 0.8510068655014038\n",
            "mini Batch Loss: 0.6383092403411865\n",
            "mini Batch Loss: 2.633683204650879\n",
            "mini Batch Loss: 2.748711109161377\n",
            "mini Batch Loss: 0.6658570170402527\n",
            "mini Batch Loss: 0.38576239347457886\n",
            "mini Batch Loss: 3.5500407218933105\n",
            "mini Batch Loss: 0.8494400978088379\n",
            "mini Batch Loss: 0.6732344627380371\n",
            "mini Batch Loss: 0.47019052505493164\n",
            "mini Batch Loss: 1.1334295272827148\n",
            "mini Batch Loss: 1.0238468647003174\n",
            "mini Batch Loss: 0.8434505462646484\n",
            "mini Batch Loss: 0.7083041667938232\n",
            "mini Batch Loss: 0.20455163717269897\n",
            "mini Batch Loss: 0.5964800119400024\n",
            "mini Batch Loss: 1.5059938430786133\n",
            "mini Batch Loss: 1.5564590692520142\n",
            "mini Batch Loss: 0.6775516271591187\n",
            "mini Batch Loss: 0.3103652596473694\n",
            "mini Batch Loss: 3.0187816619873047\n",
            "mini Batch Loss: 0.0\n",
            "mini Batch Loss: 0.2970725893974304\n",
            "mini Batch Loss: 1.2355787754058838\n",
            "mini Batch Loss: 0.4607451260089874\n",
            "mini Batch Loss: 0.6594396829605103\n",
            "mini Batch Loss: 0.7032890319824219\n",
            "Training Batch: 781 | Training Loss: 0.7032890319824219\n",
            "Training Batch: 781 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_781.pt\n",
            "dist a: tensor([12.2491, 13.9305, 15.2099, 13.8354, 14.2490, 14.9319, 15.2376, 13.0688,\n",
            "        11.2877, 14.1042, 12.9800, 13.6162, 18.9097, 12.8574, 15.5997, 15.0633,\n",
            "        20.8077,  9.1130, 13.0820, 13.2020, 20.5877, 11.4346, 14.4437, 11.1435,\n",
            "        18.0126, 27.5685, 12.3728, 15.6654, 16.9445, 13.6195, 12.7933, 12.5706,\n",
            "        19.6071, 16.0215, 19.0475, 17.7447, 15.2288, 11.7999, 10.8172, 14.8894,\n",
            "        12.7117, 15.9808, 15.3251, 11.7829, 21.4418, 13.8696, 17.1896, 23.0025,\n",
            "        12.7208,  9.4398, 25.5452, 14.6250, 21.9684, 12.7828, 24.2390, 13.5165,\n",
            "        15.2374, 19.0616, 18.1609, 17.3485, 21.0373, 14.4570, 18.2577, 20.3809],\n",
            "       device='cuda:0'), dist b: tensor([29.8253, 22.4175, 27.1601, 12.9571, 18.6447, 19.2812, 23.1971, 17.7328,\n",
            "        27.5355, 24.2279, 16.0488, 20.0148, 18.8066, 17.9346, 24.8140, 15.1392,\n",
            "        20.4003, 25.2055, 20.4069, 23.7567, 22.5043, 22.4341, 17.8743, 19.4022,\n",
            "        20.4393, 30.3853, 19.2270, 24.2861, 15.4701, 15.1288, 20.4827, 19.0958,\n",
            "        28.0980, 20.7079, 24.1512, 19.1618, 20.4916, 13.6119, 13.8338, 22.5416,\n",
            "        15.7042, 16.9425, 18.3571, 19.7002, 17.0517, 24.5494, 24.0423, 27.9699,\n",
            "        18.5453, 19.0677, 32.3062, 16.2221, 15.9800, 17.2944, 10.4867, 17.6613,\n",
            "        17.4291, 19.1267, 13.9520, 33.4361, 17.1813, 19.5751, 26.0835, 22.9799],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 1.4624346494674683\n",
            "mini Batch Loss: 0.633394718170166\n",
            "mini Batch Loss: 0.4988352954387665\n",
            "mini Batch Loss: 2.483304500579834\n",
            "mini Batch Loss: 0.37932032346725464\n",
            "mini Batch Loss: 0.571735143661499\n",
            "mini Batch Loss: 0.8225697875022888\n",
            "mini Batch Loss: 2.1808300018310547\n",
            "mini Batch Loss: 0.8325592875480652\n",
            "mini Batch Loss: 1.0238248109817505\n",
            "mini Batch Loss: 1.0234553813934326\n",
            "mini Batch Loss: 0.577394425868988\n",
            "mini Batch Loss: 0.17117327451705933\n",
            "mini Batch Loss: 0.7230827212333679\n",
            "mini Batch Loss: 0.3368957042694092\n",
            "mini Batch Loss: 1.1708403825759888\n",
            "mini Batch Loss: 2.70760178565979\n",
            "mini Batch Loss: 0.43599390983581543\n",
            "mini Batch Loss: 0.6504685282707214\n",
            "mini Batch Loss: 0.1441815048456192\n",
            "mini Batch Loss: 2.0011935234069824\n",
            "mini Batch Loss: 0.4386233389377594\n",
            "mini Batch Loss: 0.8578128814697266\n",
            "mini Batch Loss: 0.2967776954174042\n",
            "mini Batch Loss: 1.2238144874572754\n",
            "mini Batch Loss: 0.46042147278785706\n",
            "mini Batch Loss: 0.02238154411315918\n",
            "mini Batch Loss: 0.3970028758049011\n",
            "mini Batch Loss: 0.08297528326511383\n",
            "mini Batch Loss: 0.39169198274612427\n",
            "Training Batch: 811 | Training Loss: 0.39169198274612427\n",
            "Training Batch: 811 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_811.pt\n",
            "dist a: tensor([12.5815, 11.8408, 12.6377, 13.5146, 12.5883, 15.0040, 12.9452, 11.8725,\n",
            "         9.2077, 13.7084, 13.5115, 10.5320, 17.5576, 12.4388, 12.6315, 14.7340,\n",
            "        16.5901,  8.2270, 12.3752, 12.1346, 15.3416, 10.6197, 14.4167,  8.6711,\n",
            "        14.7999, 24.2176, 11.5428, 12.8051, 13.9516,  9.7474, 11.1744, 11.1580,\n",
            "        16.7684, 12.8579, 16.3950, 13.7742, 13.2936, 10.1676,  9.3886, 12.5690,\n",
            "        10.3382, 11.0890, 13.6777, 10.5204, 19.1712, 11.6440, 17.3490, 18.7971,\n",
            "        11.8996,  8.1107, 20.4498, 12.5712, 17.4814, 12.1233, 20.9237, 10.1993,\n",
            "        12.0104, 17.7607, 14.4957, 14.9224, 17.7959, 11.4659, 15.2791, 17.6597],\n",
            "       device='cuda:0'), dist b: tensor([26.3793, 17.5797, 23.7416, 10.9445, 14.7498, 16.4292, 19.7423, 15.3060,\n",
            "        20.8266, 21.4420, 14.5551, 17.8656, 17.1424, 15.2659, 22.9188, 12.3583,\n",
            "        15.8838, 22.2373, 16.4127, 20.8281, 20.3127, 19.1365, 16.2576, 17.9555,\n",
            "        16.6524, 25.4461, 16.2345, 19.9780, 14.3418, 13.0355, 17.1675, 17.8283,\n",
            "        22.3112, 16.1160, 19.4709, 16.7388, 18.5108, 12.9006, 11.8052, 20.4781,\n",
            "        13.4172, 13.4305, 16.6334, 17.5450, 15.7645, 21.6073, 20.7525, 20.7490,\n",
            "        15.6744, 15.7373, 26.8799, 14.4457, 14.3175, 15.9106, 10.1962, 15.2631,\n",
            "        15.6435, 15.7408, 11.3141, 28.6512, 15.8875, 17.5970, 21.8848, 18.9385],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.6245107054710388\n",
            "mini Batch Loss: 0.632672905921936\n",
            "mini Batch Loss: 1.0477042198181152\n",
            "mini Batch Loss: 0.11279146373271942\n",
            "mini Batch Loss: 0.6281722784042358\n",
            "mini Batch Loss: 0.6976721882820129\n",
            "mini Batch Loss: 0.1756003201007843\n",
            "mini Batch Loss: 0.22957547008991241\n",
            "mini Batch Loss: 3.331601619720459\n",
            "mini Batch Loss: 0.23190270364284515\n",
            "mini Batch Loss: 0.0708332359790802\n",
            "mini Batch Loss: 1.5489245653152466\n",
            "mini Batch Loss: 1.1466059684753418\n",
            "mini Batch Loss: 1.0735687017440796\n",
            "mini Batch Loss: 1.6201975345611572\n",
            "mini Batch Loss: 0.5553194284439087\n",
            "mini Batch Loss: 1.280562162399292\n",
            "mini Batch Loss: 2.2276110649108887\n",
            "mini Batch Loss: 0.9191885590553284\n",
            "mini Batch Loss: 0.6746719479560852\n",
            "mini Batch Loss: 0.40070703625679016\n",
            "mini Batch Loss: 1.5177618265151978\n",
            "mini Batch Loss: 2.8101329803466797\n",
            "mini Batch Loss: 1.5827186107635498\n",
            "mini Batch Loss: 1.9642164707183838\n",
            "mini Batch Loss: 0.4525003135204315\n",
            "mini Batch Loss: 0.26885420083999634\n",
            "mini Batch Loss: 1.071333408355713\n",
            "mini Batch Loss: 2.893517017364502\n",
            "mini Batch Loss: 2.094302177429199\n",
            "Training Batch: 841 | Training Loss: 2.094302177429199\n",
            "Training Batch: 841 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_841.pt\n",
            "dist a: tensor([10.3068,  9.5180, 10.1485, 11.6933, 11.2551, 12.4453, 10.4997, 10.1306,\n",
            "         9.9597, 12.4178, 10.1800,  9.6712, 13.7546,  9.2416, 11.9868, 11.8602,\n",
            "        14.0907,  7.4899, 13.5542, 11.6139, 14.1481,  8.8309, 14.4809,  8.4128,\n",
            "        11.0505, 21.0627,  9.3457, 12.5773, 13.2915,  7.6334,  9.9226, 10.0325,\n",
            "        15.7500, 12.6008, 13.8728, 12.5580, 11.9143,  8.7519,  8.3952, 11.8738,\n",
            "        10.0430, 10.8305, 11.0707,  9.9969, 18.3189, 10.2278, 13.4166, 17.7901,\n",
            "        11.4263,  6.5603, 17.2568, 11.7517, 14.8326, 11.4374, 16.3316,  9.5359,\n",
            "        11.1622, 15.2294, 10.1639, 12.8311, 15.7578, 12.1493, 13.9081, 15.3345],\n",
            "       device='cuda:0'), dist b: tensor([21.0919, 14.1580, 21.9185,  9.2106, 14.5120, 14.9987, 16.6493, 14.1901,\n",
            "        18.4810, 20.6612, 10.1068, 16.6193, 15.4890, 14.4455, 20.2499, 10.8211,\n",
            "        13.1354, 21.1067, 13.7159, 16.9079, 18.2984, 16.7021, 11.4874, 12.9661,\n",
            "        13.4564, 22.0656, 14.5758, 17.8404, 13.3303,  9.6171, 17.1394, 14.6443,\n",
            "        19.8043, 18.2540, 19.6030, 14.3281, 15.3549, 11.0473, 11.9495, 18.1917,\n",
            "        10.6353, 13.5641, 14.8783, 15.3484, 15.0856, 18.4399, 18.0872, 19.5917,\n",
            "        14.6777, 15.1400, 23.4484, 13.4484, 13.0906, 13.9411,  9.2738, 14.0962,\n",
            "        13.8552, 14.2883,  9.4623, 24.0800, 14.7658, 13.4939, 19.3546, 16.9290],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.26283931732177734\n",
            "mini Batch Loss: 4.290245056152344\n",
            "mini Batch Loss: 2.1548447608947754\n",
            "mini Batch Loss: 1.7118029594421387\n",
            "mini Batch Loss: 2.3365790843963623\n",
            "mini Batch Loss: 1.9121067523956299\n",
            "mini Batch Loss: 1.9777073860168457\n",
            "mini Batch Loss: 1.3302067518234253\n",
            "mini Batch Loss: 1.5675395727157593\n",
            "mini Batch Loss: 1.0487143993377686\n",
            "mini Batch Loss: 1.792248249053955\n",
            "mini Batch Loss: 1.4679635763168335\n",
            "mini Batch Loss: 0.2987724542617798\n",
            "mini Batch Loss: 0.6466119885444641\n",
            "mini Batch Loss: 1.2367244958877563\n",
            "mini Batch Loss: 1.236901044845581\n",
            "mini Batch Loss: 0.45641863346099854\n",
            "mini Batch Loss: 0.5194980502128601\n",
            "mini Batch Loss: 1.5572489500045776\n",
            "mini Batch Loss: 1.4298009872436523\n",
            "mini Batch Loss: 0.7188162803649902\n",
            "mini Batch Loss: 0.5310819745063782\n",
            "mini Batch Loss: 1.5835084915161133\n",
            "mini Batch Loss: 0.13021770119667053\n",
            "mini Batch Loss: 1.9933297634124756\n",
            "mini Batch Loss: 1.8784273862838745\n",
            "mini Batch Loss: 1.6197187900543213\n",
            "mini Batch Loss: 0.5912187695503235\n",
            "mini Batch Loss: 0.7718085050582886\n",
            "mini Batch Loss: 0.9674330353736877\n",
            "Training Batch: 871 | Training Loss: 0.9674330353736877\n",
            "Training Batch: 871 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_871.pt\n",
            "dist a: tensor([14.3754, 15.4372, 15.6787, 17.4976, 14.7934, 15.9986, 15.9382, 17.4427,\n",
            "        14.1496, 19.9562, 17.3934, 15.8654, 22.2685, 16.4951, 18.8921, 18.2322,\n",
            "        23.0260, 12.1184, 19.4486, 18.8348, 25.2592, 17.9744, 18.9135, 13.4263,\n",
            "        21.2509, 30.0284, 14.9989, 22.5934, 20.0383, 12.6998, 13.5097, 14.5164,\n",
            "        20.1657, 21.0425, 24.3274, 19.7067, 14.8131, 13.4697, 11.5748, 18.0675,\n",
            "        12.8944, 15.6564, 15.4291, 14.6385, 24.4732, 14.4713, 15.7664, 22.6270,\n",
            "        16.5697,  8.6213, 28.5994, 15.4348, 23.0676, 18.2374, 23.4747, 11.8337,\n",
            "        15.7565, 23.2760, 15.4403, 16.3877, 24.9948, 14.3834, 20.9979, 22.2127],\n",
            "       device='cuda:0'), dist b: tensor([29.2061, 24.1364, 28.4924, 16.5040, 19.5193, 20.8243, 24.4235, 21.1841,\n",
            "        26.4993, 33.1372, 14.5719, 17.1961, 24.1522, 19.3996, 30.0520, 16.6248,\n",
            "        18.7107, 27.7642, 22.2145, 28.1780, 28.7299, 23.0523, 20.6091, 22.8047,\n",
            "        23.6086, 31.0309, 23.3176, 23.8732, 23.0839, 16.9625, 23.7298, 18.1730,\n",
            "        28.5913, 29.2732, 28.7618, 19.2576, 20.1063, 16.1849, 17.3669, 29.0135,\n",
            "        18.5285, 19.1074, 20.1757, 25.0619, 22.8946, 25.3348, 28.0307, 27.5960,\n",
            "        23.6508, 21.6913, 32.0653, 22.0063, 18.7282, 20.1235, 13.8444, 24.6247,\n",
            "        21.8958, 18.7942, 12.5526, 32.3876, 24.2258, 26.8008, 32.9959, 25.8964],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 1.6526561975479126\n",
            "mini Batch Loss: 0.3666152358055115\n",
            "mini Batch Loss: 0.30189213156700134\n",
            "mini Batch Loss: 2.4395740032196045\n",
            "mini Batch Loss: 2.4448671340942383\n",
            "mini Batch Loss: 0.3029913306236267\n",
            "mini Batch Loss: 1.039687156677246\n",
            "mini Batch Loss: 0.37768876552581787\n",
            "mini Batch Loss: 1.9118813276290894\n",
            "mini Batch Loss: 1.2868032455444336\n",
            "mini Batch Loss: 1.991909146308899\n",
            "mini Batch Loss: 0.014348328113555908\n",
            "mini Batch Loss: 1.0146379470825195\n",
            "mini Batch Loss: 0.745354950428009\n",
            "mini Batch Loss: 3.2738451957702637\n",
            "mini Batch Loss: 0.655177116394043\n",
            "mini Batch Loss: 0.8483952283859253\n",
            "mini Batch Loss: 0.23742376267910004\n",
            "mini Batch Loss: 0.07995103299617767\n",
            "mini Batch Loss: 1.2344493865966797\n",
            "mini Batch Loss: 1.3070404529571533\n",
            "mini Batch Loss: 2.245837450027466\n",
            "mini Batch Loss: 0.6143680214881897\n",
            "mini Batch Loss: 1.4223692417144775\n",
            "mini Batch Loss: 0.61220782995224\n",
            "mini Batch Loss: 2.5409231185913086\n",
            "mini Batch Loss: 0.26861298084259033\n",
            "mini Batch Loss: 0.21403127908706665\n",
            "mini Batch Loss: 1.0541372299194336\n",
            "mini Batch Loss: 1.659525990486145\n",
            "Training Batch: 901 | Training Loss: 1.659525990486145\n",
            "Training Batch: 901 | Model saved to: /content/drive/My Drive/test4/model_epoch_4_batch_901.pt\n",
            "dist a: tensor([12.0694, 11.4500, 10.9520, 11.4375, 11.4931, 11.8278, 11.1911, 11.9625,\n",
            "        11.6218, 14.9280, 12.5665, 12.7835, 16.0976, 13.0194, 12.8087, 14.6169,\n",
            "        17.3419,  8.1170, 12.4071, 12.9897, 17.8664, 11.2683, 14.3724, 10.2015,\n",
            "        15.3146, 20.3915, 12.4969, 16.5326, 15.5872, 10.1760,  9.9706, 10.2702,\n",
            "        14.7702, 14.3324, 17.2837, 15.0993, 11.4796,  9.7911,  9.1491, 13.5559,\n",
            "        10.4344, 11.3089, 11.4277,  9.8025, 18.3857, 11.2146, 12.5347, 16.7093,\n",
            "        11.8437,  7.8360, 19.4880, 12.5420, 14.9889, 12.8618, 17.4842,  9.4723,\n",
            "        11.9092, 15.0797, 10.9444, 10.7002, 16.8679, 11.3655, 17.0485, 14.5620],\n",
            "       device='cuda:0'), dist b: tensor([21.9644, 17.8364, 23.1279, 10.6517, 14.0418, 16.9951, 18.7684, 14.9010,\n",
            "        20.9559, 23.6563, 11.4625, 14.7752, 15.8580, 13.3023, 20.8915, 13.4840,\n",
            "        14.7985, 21.6593, 16.6919, 21.7855, 20.8888, 15.8755, 16.0372, 17.4605,\n",
            "        15.3972, 24.4224, 15.0957, 19.6611, 15.9833, 13.6030, 15.5219, 13.5824,\n",
            "        21.1502, 18.7965, 20.8757, 16.2563, 16.4468, 12.5508, 13.3295, 19.4926,\n",
            "        13.7591, 14.2126, 14.5659, 17.8565, 15.2526, 21.0253, 21.1277, 19.9361,\n",
            "        17.0111, 15.6691, 25.3084, 13.6268, 13.6078, 15.4137,  9.7666, 15.7380,\n",
            "        14.1721, 13.7988,  9.6732, 23.9306, 16.6909, 22.2222, 24.8673, 17.1701],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.5764607191085815\n",
            "mini Batch Loss: 1.7806923389434814\n",
            "mini Batch Loss: 1.2193862199783325\n",
            "mini Batch Loss: 1.1951029300689697\n",
            "mini Batch Loss: 1.985465407371521\n",
            "mini Batch Loss: 1.1541390419006348\n",
            "mini Batch Loss: 1.5444520711898804\n",
            "mini Batch Loss: 1.6202608346939087\n",
            "mini Batch Loss: 1.5857019424438477\n",
            "mini Batch Loss: 0.15043696761131287\n",
            "mini Batch Loss: 1.666515827178955\n",
            "mini Batch Loss: 4.8815202713012695\n",
            "mini Batch Loss: 0.7265649437904358\n",
            "mini Batch Loss: 0.7505165934562683\n",
            "mini Batch Loss: 0.5146961212158203\n",
            "mini Batch Loss: 0.5464980006217957\n",
            "mini Batch Loss: 0.9829854369163513\n",
            "mini Batch Loss: 2.2094545364379883\n",
            "mini Batch Loss: 0.48804616928100586\n",
            "mini Batch Loss: 0.20426659286022186\n",
            "[4] average loss per epoch: 1.259\n",
            "Saved model checkpoint to /content/drive/My Drive/test4/model_epoch_4.pt\n",
            "dist a: tensor([12.7772, 12.5324, 12.7257, 12.5280, 11.0927, 12.4085, 13.1137, 12.6232,\n",
            "        12.5286, 15.5675, 12.4204, 13.5084, 18.7249, 12.7045, 14.5702, 17.1404,\n",
            "        20.1076,  7.9625, 14.4144, 15.3963, 19.2182, 10.7908, 16.3024, 12.4501,\n",
            "        15.6863, 22.5552, 13.1690, 18.7200, 16.4763, 12.4451, 10.1513, 11.5212,\n",
            "        18.7881, 16.0089, 19.6190, 15.9317, 12.1283, 10.5615,  9.9051, 14.1385,\n",
            "        12.6591, 14.4258, 12.4377, 10.0354, 19.5233, 14.9830, 14.0939, 18.2038,\n",
            "        13.7518,  8.0792, 24.5706, 13.1215, 19.1921, 14.5940, 20.4320, 10.0867,\n",
            "        13.6922, 16.6285, 14.2559, 12.6999, 17.5215, 13.6271, 18.2094, 17.2233],\n",
            "       device='cuda:0'), dist b: tensor([27.9158, 16.8560, 26.1762, 11.4174, 16.8215, 19.0248, 22.7662, 15.4401,\n",
            "        21.9660, 26.6318, 11.5849, 17.7535, 18.4549, 19.9312, 19.0548, 15.8401,\n",
            "        18.3959, 23.8921, 18.8335, 21.3154, 21.1850, 19.3679, 17.8303, 17.2799,\n",
            "        17.4020, 28.0586, 17.5854, 24.3580, 16.7697, 14.1982, 14.0973, 13.7072,\n",
            "        24.9398, 22.9018, 22.9784, 18.7971, 18.9479, 14.2096, 14.7077, 22.6134,\n",
            "        13.8307, 17.8992, 15.4409, 22.6471, 18.7495, 27.2737, 23.4511, 24.7321,\n",
            "        20.4764, 18.4101, 30.9054, 14.8017, 15.4297, 15.4422, 11.5316, 19.7229,\n",
            "        17.2469, 14.3847, 10.7287, 26.1201, 20.9209, 21.7672, 25.5936, 18.6939],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "================== START PREDICTION ==================\n",
            "batch:  0\n",
            "batch:  1\n",
            "batch:  2\n",
            "batch:  3\n",
            "batch:  4\n",
            "batch:  5\n",
            "batch:  6\n",
            "batch:  7\n",
            "batch:  8\n",
            "batch:  9\n",
            "batch:  10\n",
            "batch:  11\n",
            "batch:  12\n",
            "batch:  13\n",
            "batch:  14\n",
            "batch:  15\n",
            "batch:  16\n",
            "batch:  17\n",
            "batch:  18\n",
            "batch:  19\n",
            "batch:  20\n",
            "batch:  21\n",
            "batch:  22\n",
            "batch:  23\n",
            "batch:  24\n",
            "batch:  25\n",
            "batch:  26\n",
            "batch:  27\n",
            "batch:  28\n",
            "batch:  29\n",
            "batch:  30\n",
            "batch:  31\n",
            "batch:  32\n",
            "batch:  33\n",
            "batch:  34\n",
            "batch:  35\n",
            "batch:  36\n",
            "batch:  37\n",
            "batch:  38\n",
            "batch:  39\n",
            "batch:  40\n",
            "batch:  41\n",
            "batch:  42\n",
            "batch:  43\n",
            "batch:  44\n",
            "batch:  45\n",
            "batch:  46\n",
            "batch:  47\n",
            "batch:  48\n",
            "batch:  49\n",
            "batch:  50\n",
            "batch:  51\n",
            "batch:  52\n",
            "batch:  53\n",
            "batch:  54\n",
            "batch:  55\n",
            "batch:  56\n",
            "batch:  57\n",
            "batch:  58\n",
            "batch:  59\n",
            "batch:  60\n",
            "batch:  61\n",
            "batch:  62\n",
            "batch:  63\n",
            "batch:  64\n",
            "batch:  65\n",
            "batch:  66\n",
            "batch:  67\n",
            "batch:  68\n",
            "batch:  69\n",
            "batch:  70\n",
            "batch:  71\n",
            "batch:  72\n",
            "batch:  73\n",
            "batch:  74\n",
            "batch:  75\n",
            "batch:  76\n",
            "batch:  77\n",
            "batch:  78\n",
            "batch:  79\n",
            "batch:  80\n",
            "batch:  81\n",
            "batch:  82\n",
            "batch:  83\n",
            "batch:  84\n",
            "batch:  85\n",
            "batch:  86\n",
            "batch:  87\n",
            "batch:  88\n",
            "batch:  89\n",
            "batch:  90\n",
            "batch:  91\n",
            "batch:  92\n",
            "batch:  93\n",
            "batch:  94\n",
            "batch:  95\n",
            "batch:  96\n",
            "batch:  97\n",
            "batch:  98\n",
            "batch:  99\n",
            "batch:  100\n",
            "batch:  101\n",
            "batch:  102\n",
            "batch:  103\n",
            "batch:  104\n",
            "batch:  105\n",
            "batch:  106\n",
            "batch:  107\n",
            "batch:  108\n",
            "batch:  109\n",
            "batch:  110\n",
            "batch:  111\n",
            "batch:  112\n",
            "batch:  113\n",
            "batch:  114\n",
            "batch:  115\n",
            "batch:  116\n",
            "batch:  117\n",
            "batch:  118\n",
            "batch:  119\n",
            "batch:  120\n",
            "batch:  121\n",
            "batch:  122\n",
            "batch:  123\n",
            "batch:  124\n",
            "batch:  125\n",
            "batch:  126\n",
            "batch:  127\n",
            "batch:  128\n",
            "batch:  129\n",
            "batch:  130\n",
            "batch:  131\n",
            "batch:  132\n",
            "batch:  133\n",
            "batch:  134\n",
            "batch:  135\n",
            "batch:  136\n",
            "batch:  137\n",
            "batch:  138\n",
            "batch:  139\n",
            "batch:  140\n",
            "batch:  141\n",
            "batch:  142\n",
            "batch:  143\n",
            "batch:  144\n",
            "batch:  145\n",
            "batch:  146\n",
            "batch:  147\n",
            "batch:  148\n",
            "batch:  149\n",
            "batch:  150\n",
            "batch:  151\n",
            "batch:  152\n",
            "batch:  153\n",
            "batch:  154\n",
            "batch:  155\n",
            "batch:  156\n",
            "batch:  157\n",
            "batch:  158\n",
            "batch:  159\n",
            "batch:  160\n",
            "batch:  161\n",
            "batch:  162\n",
            "batch:  163\n",
            "batch:  164\n",
            "batch:  165\n",
            "batch:  166\n",
            "batch:  167\n",
            "batch:  168\n",
            "batch:  169\n",
            "batch:  170\n",
            "batch:  171\n",
            "batch:  172\n",
            "batch:  173\n",
            "batch:  174\n",
            "batch:  175\n",
            "batch:  176\n",
            "batch:  177\n",
            "batch:  178\n",
            "batch:  179\n",
            "batch:  180\n",
            "batch:  181\n",
            "batch:  182\n",
            "batch:  183\n",
            "batch:  184\n",
            "batch:  185\n",
            "batch:  186\n",
            "batch:  187\n",
            "batch:  188\n",
            "batch:  189\n",
            "batch:  190\n",
            "batch:  191\n",
            "batch:  192\n",
            "batch:  193\n",
            "batch:  194\n",
            "batch:  195\n",
            "batch:  196\n",
            "batch:  197\n",
            "batch:  198\n",
            "batch:  199\n",
            "batch:  200\n",
            "batch:  201\n",
            "batch:  202\n",
            "batch:  203\n",
            "batch:  204\n",
            "batch:  205\n",
            "batch:  206\n",
            "batch:  207\n",
            "batch:  208\n",
            "batch:  209\n",
            "batch:  210\n",
            "batch:  211\n",
            "batch:  212\n",
            "batch:  213\n",
            "batch:  214\n",
            "batch:  215\n",
            "batch:  216\n",
            "batch:  217\n",
            "batch:  218\n",
            "batch:  219\n",
            "batch:  220\n",
            "batch:  221\n",
            "batch:  222\n",
            "batch:  223\n",
            "batch:  224\n",
            "batch:  225\n",
            "batch:  226\n",
            "batch:  227\n",
            "batch:  228\n",
            "batch:  229\n",
            "batch:  230\n",
            "batch:  231\n",
            "batch:  232\n",
            "batch:  233\n",
            "batch:  234\n",
            "batch:  235\n",
            "batch:  236\n",
            "batch:  237\n",
            "batch:  238\n",
            "batch:  239\n",
            "batch:  240\n",
            "batch:  241\n",
            "batch:  242\n",
            "batch:  243\n",
            "batch:  244\n",
            "batch:  245\n",
            "batch:  246\n",
            "batch:  247\n",
            "batch:  248\n",
            "batch:  249\n",
            "batch:  250\n",
            "batch:  251\n",
            "batch:  252\n",
            "batch:  253\n",
            "batch:  254\n",
            "batch:  255\n",
            "batch:  256\n",
            "batch:  257\n",
            "batch:  258\n",
            "batch:  259\n",
            "batch:  260\n",
            "batch:  261\n",
            "batch:  262\n",
            "batch:  263\n",
            "batch:  264\n",
            "batch:  265\n",
            "batch:  266\n",
            "batch:  267\n",
            "batch:  268\n",
            "batch:  269\n",
            "batch:  270\n",
            "batch:  271\n",
            "batch:  272\n",
            "batch:  273\n",
            "batch:  274\n",
            "batch:  275\n",
            "batch:  276\n",
            "batch:  277\n",
            "batch:  278\n",
            "batch:  279\n",
            "batch:  280\n",
            "batch:  281\n",
            "batch:  282\n",
            "batch:  283\n",
            "batch:  284\n",
            "batch:  285\n",
            "batch:  286\n",
            "batch:  287\n",
            "batch:  288\n",
            "batch:  289\n",
            "batch:  290\n",
            "batch:  291\n",
            "batch:  292\n",
            "batch:  293\n",
            "batch:  294\n",
            "batch:  295\n",
            "batch:  296\n",
            "batch:  297\n",
            "batch:  298\n",
            "batch:  299\n",
            "batch:  300\n",
            "batch:  301\n",
            "batch:  302\n",
            "batch:  303\n",
            "batch:  304\n",
            "batch:  305\n",
            "batch:  306\n",
            "batch:  307\n",
            "batch:  308\n",
            "batch:  309\n",
            "batch:  310\n",
            "batch:  311\n",
            "batch:  312\n",
            "batch:  313\n",
            "batch:  314\n",
            "batch:  315\n",
            "batch:  316\n",
            "batch:  317\n",
            "batch:  318\n",
            "batch:  319\n",
            "batch:  320\n",
            "batch:  321\n",
            "batch:  322\n",
            "batch:  323\n",
            "batch:  324\n",
            "batch:  325\n",
            "batch:  326\n",
            "batch:  327\n",
            "batch:  328\n",
            "batch:  329\n",
            "batch:  330\n",
            "batch:  331\n",
            "batch:  332\n",
            "batch:  333\n",
            "batch:  334\n",
            "batch:  335\n",
            "batch:  336\n",
            "batch:  337\n",
            "batch:  338\n",
            "batch:  339\n",
            "batch:  340\n",
            "batch:  341\n",
            "batch:  342\n",
            "batch:  343\n",
            "batch:  344\n",
            "batch:  345\n",
            "batch:  346\n",
            "batch:  347\n",
            "batch:  348\n",
            "batch:  349\n",
            "batch:  350\n",
            "batch:  351\n",
            "batch:  352\n",
            "batch:  353\n",
            "batch:  354\n",
            "batch:  355\n",
            "batch:  356\n",
            "batch:  357\n",
            "batch:  358\n",
            "batch:  359\n",
            "batch:  360\n",
            "batch:  361\n",
            "batch:  362\n",
            "batch:  363\n",
            "batch:  364\n",
            "batch:  365\n",
            "batch:  366\n",
            "batch:  367\n",
            "batch:  368\n",
            "batch:  369\n",
            "batch:  370\n",
            "batch:  371\n",
            "batch:  372\n",
            "batch:  373\n",
            "batch:  374\n",
            "batch:  375\n",
            "batch:  376\n",
            "batch:  377\n",
            "batch:  378\n",
            "batch:  379\n",
            "batch:  380\n",
            "batch:  381\n",
            "batch:  382\n",
            "batch:  383\n",
            "batch:  384\n",
            "batch:  385\n",
            "batch:  386\n",
            "batch:  387\n",
            "batch:  388\n",
            "batch:  389\n",
            "batch:  390\n",
            "batch:  391\n",
            "batch:  392\n",
            "batch:  393\n",
            "batch:  394\n",
            "batch:  395\n",
            "batch:  396\n",
            "batch:  397\n",
            "batch:  398\n",
            "batch:  399\n",
            "batch:  400\n",
            "batch:  401\n",
            "batch:  402\n",
            "batch:  403\n",
            "batch:  404\n",
            "batch:  405\n",
            "batch:  406\n",
            "batch:  407\n",
            "batch:  408\n",
            "batch:  409\n",
            "batch:  410\n",
            "batch:  411\n",
            "batch:  412\n",
            "batch:  413\n",
            "batch:  414\n",
            "batch:  415\n",
            "batch:  416\n",
            "batch:  417\n",
            "batch:  418\n",
            "batch:  419\n",
            "batch:  420\n",
            "batch:  421\n",
            "batch:  422\n",
            "batch:  423\n",
            "batch:  424\n",
            "batch:  425\n",
            "batch:  426\n",
            "batch:  427\n",
            "batch:  428\n",
            "batch:  429\n",
            "batch:  430\n",
            "batch:  431\n",
            "batch:  432\n",
            "batch:  433\n",
            "batch:  434\n",
            "batch:  435\n",
            "batch:  436\n",
            "batch:  437\n",
            "batch:  438\n",
            "batch:  439\n",
            "batch:  440\n",
            "batch:  441\n",
            "batch:  442\n",
            "batch:  443\n",
            "batch:  444\n",
            "batch:  445\n",
            "batch:  446\n",
            "batch:  447\n",
            "batch:  448\n",
            "batch:  449\n",
            "batch:  450\n",
            "batch:  451\n",
            "batch:  452\n",
            "batch:  453\n",
            "batch:  454\n",
            "batch:  455\n",
            "batch:  456\n",
            "batch:  457\n",
            "batch:  458\n",
            "batch:  459\n",
            "batch:  460\n",
            "batch:  461\n",
            "batch:  462\n",
            "batch:  463\n",
            "batch:  464\n",
            "batch:  465\n",
            "batch:  466\n",
            "batch:  467\n",
            "batch:  468\n",
            "batch:  469\n",
            "batch:  470\n",
            "batch:  471\n",
            "batch:  472\n",
            "batch:  473\n",
            "batch:  474\n",
            "batch:  475\n",
            "batch:  476\n",
            "batch:  477\n",
            "batch:  478\n",
            "batch:  479\n",
            "batch:  480\n",
            "batch:  481\n",
            "batch:  482\n",
            "batch:  483\n",
            "batch:  484\n",
            "batch:  485\n",
            "batch:  486\n",
            "batch:  487\n",
            "batch:  488\n",
            "batch:  489\n",
            "batch:  490\n",
            "batch:  491\n",
            "batch:  492\n",
            "batch:  493\n",
            "batch:  494\n",
            "batch:  495\n",
            "batch:  496\n",
            "batch:  497\n",
            "batch:  498\n",
            "batch:  499\n",
            "batch:  500\n",
            "batch:  501\n",
            "batch:  502\n",
            "batch:  503\n",
            "batch:  504\n",
            "batch:  505\n",
            "batch:  506\n",
            "batch:  507\n",
            "batch:  508\n",
            "batch:  509\n",
            "batch:  510\n",
            "batch:  511\n",
            "batch:  512\n",
            "batch:  513\n",
            "batch:  514\n",
            "batch:  515\n",
            "batch:  516\n",
            "batch:  517\n",
            "batch:  518\n",
            "batch:  519\n",
            "batch:  520\n",
            "batch:  521\n",
            "batch:  522\n",
            "batch:  523\n",
            "batch:  524\n",
            "batch:  525\n",
            "batch:  526\n",
            "batch:  527\n",
            "batch:  528\n",
            "batch:  529\n",
            "batch:  530\n",
            "batch:  531\n",
            "batch:  532\n",
            "batch:  533\n",
            "batch:  534\n",
            "batch:  535\n",
            "batch:  536\n",
            "batch:  537\n",
            "batch:  538\n",
            "batch:  539\n",
            "batch:  540\n",
            "batch:  541\n",
            "batch:  542\n",
            "batch:  543\n",
            "batch:  544\n",
            "batch:  545\n",
            "batch:  546\n",
            "batch:  547\n",
            "batch:  548\n",
            "batch:  549\n",
            "batch:  550\n",
            "batch:  551\n",
            "batch:  552\n",
            "batch:  553\n",
            "batch:  554\n",
            "batch:  555\n",
            "batch:  556\n",
            "batch:  557\n",
            "batch:  558\n",
            "batch:  559\n",
            "batch:  560\n",
            "batch:  561\n",
            "batch:  562\n",
            "batch:  563\n",
            "batch:  564\n",
            "batch:  565\n",
            "batch:  566\n",
            "batch:  567\n",
            "batch:  568\n",
            "batch:  569\n",
            "batch:  570\n",
            "batch:  571\n",
            "batch:  572\n",
            "batch:  573\n",
            "batch:  574\n",
            "batch:  575\n",
            "batch:  576\n",
            "batch:  577\n",
            "batch:  578\n",
            "batch:  579\n",
            "batch:  580\n",
            "batch:  581\n",
            "batch:  582\n",
            "batch:  583\n",
            "batch:  584\n",
            "batch:  585\n",
            "batch:  586\n",
            "batch:  587\n",
            "batch:  588\n",
            "batch:  589\n",
            "batch:  590\n",
            "batch:  591\n",
            "batch:  592\n",
            "batch:  593\n",
            "batch:  594\n",
            "batch:  595\n",
            "batch:  596\n",
            "batch:  597\n",
            "batch:  598\n",
            "batch:  599\n",
            "batch:  600\n",
            "batch:  601\n",
            "batch:  602\n",
            "batch:  603\n",
            "batch:  604\n",
            "batch:  605\n",
            "batch:  606\n",
            "batch:  607\n",
            "batch:  608\n",
            "batch:  609\n",
            "batch:  610\n",
            "batch:  611\n",
            "batch:  612\n",
            "batch:  613\n",
            "batch:  614\n",
            "batch:  615\n",
            "batch:  616\n",
            "batch:  617\n",
            "batch:  618\n",
            "batch:  619\n",
            "batch:  620\n",
            "batch:  621\n",
            "batch:  622\n",
            "batch:  623\n",
            "batch:  624\n",
            "batch:  625\n",
            "batch:  626\n",
            "batch:  627\n",
            "batch:  628\n",
            "batch:  629\n",
            "batch:  630\n",
            "batch:  631\n",
            "batch:  632\n",
            "batch:  633\n",
            "batch:  634\n",
            "batch:  635\n",
            "batch:  636\n",
            "batch:  637\n",
            "batch:  638\n",
            "batch:  639\n",
            "batch:  640\n",
            "batch:  641\n",
            "batch:  642\n",
            "batch:  643\n",
            "batch:  644\n",
            "batch:  645\n",
            "batch:  646\n",
            "batch:  647\n",
            "batch:  648\n",
            "batch:  649\n",
            "batch:  650\n",
            "batch:  651\n",
            "batch:  652\n",
            "batch:  653\n",
            "batch:  654\n",
            "batch:  655\n",
            "batch:  656\n",
            "batch:  657\n",
            "batch:  658\n",
            "batch:  659\n",
            "batch:  660\n",
            "batch:  661\n",
            "batch:  662\n",
            "batch:  663\n",
            "batch:  664\n",
            "batch:  665\n",
            "batch:  666\n",
            "batch:  667\n",
            "batch:  668\n",
            "batch:  669\n",
            "batch:  670\n",
            "batch:  671\n",
            "batch:  672\n",
            "batch:  673\n",
            "batch:  674\n",
            "batch:  675\n",
            "batch:  676\n",
            "batch:  677\n",
            "batch:  678\n",
            "batch:  679\n",
            "batch:  680\n",
            "batch:  681\n",
            "batch:  682\n",
            "batch:  683\n",
            "batch:  684\n",
            "batch:  685\n",
            "batch:  686\n",
            "batch:  687\n",
            "batch:  688\n",
            "batch:  689\n",
            "batch:  690\n",
            "batch:  691\n",
            "batch:  692\n",
            "batch:  693\n",
            "batch:  694\n",
            "batch:  695\n",
            "batch:  696\n",
            "batch:  697\n",
            "batch:  698\n",
            "batch:  699\n",
            "batch:  700\n",
            "batch:  701\n",
            "batch:  702\n",
            "batch:  703\n",
            "batch:  704\n",
            "batch:  705\n",
            "batch:  706\n",
            "batch:  707\n",
            "batch:  708\n",
            "batch:  709\n",
            "batch:  710\n",
            "batch:  711\n",
            "batch:  712\n",
            "batch:  713\n",
            "batch:  714\n",
            "batch:  715\n",
            "batch:  716\n",
            "batch:  717\n",
            "batch:  718\n",
            "batch:  719\n",
            "batch:  720\n",
            "batch:  721\n",
            "batch:  722\n",
            "batch:  723\n",
            "batch:  724\n",
            "batch:  725\n",
            "batch:  726\n",
            "batch:  727\n",
            "batch:  728\n",
            "batch:  729\n",
            "batch:  730\n",
            "batch:  731\n",
            "batch:  732\n",
            "batch:  733\n",
            "batch:  734\n",
            "batch:  735\n",
            "batch:  736\n",
            "batch:  737\n",
            "batch:  738\n",
            "batch:  739\n",
            "batch:  740\n",
            "batch:  741\n",
            "batch:  742\n",
            "batch:  743\n",
            "batch:  744\n",
            "batch:  745\n",
            "batch:  746\n",
            "batch:  747\n",
            "batch:  748\n",
            "batch:  749\n",
            "batch:  750\n",
            "batch:  751\n",
            "batch:  752\n",
            "batch:  753\n",
            "batch:  754\n",
            "batch:  755\n",
            "batch:  756\n",
            "batch:  757\n",
            "batch:  758\n",
            "batch:  759\n",
            "batch:  760\n",
            "batch:  761\n",
            "batch:  762\n",
            "batch:  763\n",
            "batch:  764\n",
            "batch:  765\n",
            "batch:  766\n",
            "batch:  767\n",
            "batch:  768\n",
            "batch:  769\n",
            "batch:  770\n",
            "batch:  771\n",
            "batch:  772\n",
            "batch:  773\n",
            "batch:  774\n",
            "batch:  775\n",
            "batch:  776\n",
            "batch:  777\n",
            "batch:  778\n",
            "batch:  779\n",
            "batch:  780\n",
            "batch:  781\n",
            "batch:  782\n",
            "batch:  783\n",
            "batch:  784\n",
            "batch:  785\n",
            "batch:  786\n",
            "batch:  787\n",
            "batch:  788\n",
            "batch:  789\n",
            "batch:  790\n",
            "batch:  791\n",
            "batch:  792\n",
            "batch:  793\n",
            "batch:  794\n",
            "batch:  795\n",
            "batch:  796\n",
            "batch:  797\n",
            "batch:  798\n",
            "batch:  799\n",
            "batch:  800\n",
            "batch:  801\n",
            "batch:  802\n",
            "batch:  803\n",
            "batch:  804\n",
            "batch:  805\n",
            "batch:  806\n",
            "batch:  807\n",
            "batch:  808\n",
            "batch:  809\n",
            "batch:  810\n",
            "batch:  811\n",
            "batch:  812\n",
            "batch:  813\n",
            "batch:  814\n",
            "batch:  815\n",
            "batch:  816\n",
            "batch:  817\n",
            "batch:  818\n",
            "batch:  819\n",
            "batch:  820\n",
            "batch:  821\n",
            "batch:  822\n",
            "batch:  823\n",
            "batch:  824\n",
            "batch:  825\n",
            "batch:  826\n",
            "batch:  827\n",
            "batch:  828\n",
            "batch:  829\n",
            "batch:  830\n",
            "batch:  831\n",
            "batch:  832\n",
            "batch:  833\n",
            "batch:  834\n",
            "batch:  835\n",
            "batch:  836\n",
            "batch:  837\n",
            "batch:  838\n",
            "batch:  839\n",
            "batch:  840\n",
            "batch:  841\n",
            "batch:  842\n",
            "batch:  843\n",
            "batch:  844\n",
            "batch:  845\n",
            "batch:  846\n",
            "batch:  847\n",
            "batch:  848\n",
            "batch:  849\n",
            "batch:  850\n",
            "batch:  851\n",
            "batch:  852\n",
            "batch:  853\n",
            "batch:  854\n",
            "batch:  855\n",
            "batch:  856\n",
            "batch:  857\n",
            "batch:  858\n",
            "batch:  859\n",
            "batch:  860\n",
            "batch:  861\n",
            "batch:  862\n",
            "batch:  863\n",
            "batch:  864\n",
            "batch:  865\n",
            "batch:  866\n",
            "batch:  867\n",
            "batch:  868\n",
            "batch:  869\n",
            "batch:  870\n",
            "batch:  871\n",
            "batch:  872\n",
            "batch:  873\n",
            "batch:  874\n",
            "batch:  875\n",
            "batch:  876\n",
            "batch:  877\n",
            "batch:  878\n",
            "batch:  879\n",
            "batch:  880\n",
            "batch:  881\n",
            "batch:  882\n",
            "batch:  883\n",
            "batch:  884\n",
            "batch:  885\n",
            "batch:  886\n",
            "batch:  887\n",
            "batch:  888\n",
            "batch:  889\n",
            "batch:  890\n",
            "batch:  891\n",
            "batch:  892\n",
            "batch:  893\n",
            "batch:  894\n",
            "batch:  895\n",
            "batch:  896\n",
            "batch:  897\n",
            "batch:  898\n",
            "batch:  899\n",
            "batch:  900\n",
            "batch:  901\n",
            "batch:  902\n",
            "batch:  903\n",
            "batch:  904\n",
            "batch:  905\n",
            "batch:  906\n",
            "batch:  907\n",
            "batch:  908\n",
            "batch:  909\n",
            "batch:  910\n",
            "batch:  911\n",
            "batch:  912\n",
            "batch:  913\n",
            "batch:  914\n",
            "batch:  915\n",
            "batch:  916\n",
            "batch:  917\n",
            "batch:  918\n",
            "batch:  919\n",
            "batch:  920\n",
            "batch:  921\n",
            "batch:  922\n",
            "batch:  923\n",
            "batch:  924\n",
            "batch:  925\n",
            "batch:  926\n",
            "batch:  927\n",
            "batch:  928\n",
            "batch:  929\n",
            "batch:  930\n",
            "[1 0 1 ... 1 1 1]\n",
            "mini Batch Loss: 0.8952100872993469\n",
            "Training Batch: 1 | Training Loss: 0.8952100872993469\n",
            "Training Batch: 1 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_1.pt\n",
            "dist a: tensor([12.7939, 12.5621, 12.8443, 12.7042, 11.1578, 12.5195, 13.1104, 12.6261,\n",
            "        12.6420, 15.7157, 12.4715, 13.7111, 18.8219, 12.8316, 14.6376, 17.2068,\n",
            "        20.0147,  7.9439, 14.3331, 15.5221, 19.2916, 10.7780, 16.3793, 12.6141,\n",
            "        15.8229, 22.4805, 13.1714, 19.0534, 16.5946, 12.4363, 10.0907, 11.6473,\n",
            "        18.8373, 16.1654, 19.8005, 15.8925, 12.3356, 10.6745,  9.9726, 14.1703,\n",
            "        12.5739, 14.4704, 12.5370, 10.0806, 19.4796, 15.0164, 14.1119, 18.1851,\n",
            "        13.8497,  8.1864, 24.7742, 13.2661, 19.0514, 14.7169, 20.5505, 10.2085,\n",
            "        13.9769, 16.9104, 14.6108, 12.8474, 17.4832, 13.6021, 18.3674, 17.0793],\n",
            "       device='cuda:0'), dist b: tensor([27.9355, 16.6789, 26.3687, 11.5147, 16.9408, 18.9741, 23.0370, 15.4624,\n",
            "        21.9633, 26.7125, 11.5735, 17.9519, 18.6528, 20.2616, 19.1802, 15.9721,\n",
            "        18.2753, 23.7870, 18.6839, 21.1530, 21.2301, 19.3334, 18.0689, 17.4742,\n",
            "        17.5068, 27.8275, 17.6453, 24.5035, 16.8359, 14.2946, 14.3273, 13.7025,\n",
            "        24.8570, 23.0304, 23.0443, 18.9429, 19.3622, 14.3652, 14.7449, 22.4166,\n",
            "        13.9761, 18.0650, 15.5284, 22.8188, 18.7773, 27.2685, 23.5872, 24.6509,\n",
            "        20.4588, 18.5012, 30.9473, 14.8533, 15.5293, 15.5757, 11.4833, 19.8290,\n",
            "        17.0833, 14.4101, 10.7608, 26.0280, 20.9708, 22.0544, 25.6506, 18.6410],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.7383068799972534\n",
            "mini Batch Loss: 0.7629366517066956\n",
            "mini Batch Loss: 0.2613546848297119\n",
            "mini Batch Loss: 1.3754546642303467\n",
            "mini Batch Loss: 0.012800469994544983\n",
            "mini Batch Loss: 0.19297415018081665\n",
            "mini Batch Loss: 0.8193473815917969\n",
            "mini Batch Loss: 0.877960205078125\n",
            "mini Batch Loss: 0.8676429986953735\n",
            "mini Batch Loss: 0.3482632637023926\n",
            "mini Batch Loss: 1.1633189916610718\n",
            "mini Batch Loss: 0.03499135375022888\n",
            "mini Batch Loss: 0.8743652105331421\n",
            "mini Batch Loss: 1.865817904472351\n",
            "mini Batch Loss: 0.9062812328338623\n",
            "mini Batch Loss: 1.9687858819961548\n",
            "mini Batch Loss: 1.6260590553283691\n",
            "mini Batch Loss: 0.9649659395217896\n",
            "mini Batch Loss: 1.2965853214263916\n",
            "mini Batch Loss: 1.1128218173980713\n",
            "mini Batch Loss: 0.9488404989242554\n",
            "mini Batch Loss: 0.8563317060470581\n",
            "mini Batch Loss: 0.7360647320747375\n",
            "mini Batch Loss: 0.7257550954818726\n",
            "mini Batch Loss: 0.3250546157360077\n",
            "mini Batch Loss: 0.6476467847824097\n",
            "mini Batch Loss: 2.3321056365966797\n",
            "mini Batch Loss: 0.5990909934043884\n",
            "mini Batch Loss: 0.5425231456756592\n",
            "mini Batch Loss: 0.5513629913330078\n",
            "Training Batch: 31 | Training Loss: 0.5513629913330078\n",
            "Training Batch: 31 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_31.pt\n",
            "dist a: tensor([12.2874, 13.8740, 12.2364, 14.0366, 11.9471, 12.0576, 13.1370, 12.8974,\n",
            "        11.0793, 14.7348, 13.2697, 13.7810, 17.4802, 14.2120, 14.4454, 17.3857,\n",
            "        20.2613,  8.5402, 13.3371, 14.8440, 18.2043, 10.1670, 14.8856, 11.7892,\n",
            "        17.8716, 20.4033, 12.4058, 15.3800, 17.4402, 11.9106, 11.5105, 10.8778,\n",
            "        15.8681, 16.1064, 17.4252, 18.6348, 13.1156, 11.4611,  8.7404, 14.6560,\n",
            "        13.9609, 12.4669, 12.2295, 11.1334, 20.9459, 11.8280, 13.0578, 17.7461,\n",
            "        11.4865,  8.5075, 21.2096, 12.3382, 17.9131, 15.5530, 19.1751, 10.3621,\n",
            "        17.2084, 16.9716, 14.9196, 12.5698, 18.2952, 13.5177, 19.9665, 16.1888],\n",
            "       device='cuda:0'), dist b: tensor([24.0759, 20.0672, 23.9736, 11.5348, 16.3375, 18.4094, 19.9665, 16.9361,\n",
            "        23.8681, 25.3195, 13.2162, 15.7031, 19.0615, 18.2567, 19.1217, 15.8852,\n",
            "        18.3210, 20.1784, 18.0313, 22.5885, 20.4232, 18.1002, 18.0096, 16.0175,\n",
            "        17.7924, 26.5815, 16.3556, 21.1564, 15.5302, 13.3721, 16.8467, 15.5920,\n",
            "        21.0293, 16.9735, 21.6174, 17.7462, 18.3739, 13.5387, 13.2863, 21.4425,\n",
            "        14.3255, 15.8951, 16.3583, 21.8363, 18.9961, 23.8630, 22.9408, 18.8915,\n",
            "        18.1136, 16.7396, 27.1707, 14.6819, 15.3108, 16.8121, 10.9778, 18.3193,\n",
            "        14.0334, 15.4993, 11.3630, 24.8728, 21.8425, 22.1931, 24.8404, 16.7978],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "mini Batch Loss: 0.32322782278060913\n",
            "mini Batch Loss: 0.48899176716804504\n",
            "mini Batch Loss: 0.19631263613700867\n",
            "mini Batch Loss: 0.21595574915409088\n",
            "mini Batch Loss: 2.2756752967834473\n",
            "mini Batch Loss: 2.7110824584960938\n",
            "mini Batch Loss: 0.4399316906929016\n",
            "mini Batch Loss: 2.6480917930603027\n",
            "mini Batch Loss: 1.0584161281585693\n",
            "mini Batch Loss: 1.87095308303833\n",
            "mini Batch Loss: 0.7326692938804626\n",
            "mini Batch Loss: 1.8357670307159424\n",
            "mini Batch Loss: 1.999781847000122\n",
            "mini Batch Loss: 1.550646185874939\n",
            "mini Batch Loss: 2.0902626514434814\n",
            "mini Batch Loss: 1.6854579448699951\n",
            "mini Batch Loss: 2.3675291538238525\n",
            "mini Batch Loss: 1.2322720289230347\n",
            "mini Batch Loss: 1.3952890634536743\n",
            "mini Batch Loss: 1.4405685663223267\n",
            "mini Batch Loss: 1.4678360223770142\n",
            "mini Batch Loss: 2.3441224098205566\n",
            "mini Batch Loss: 0.9876578450202942\n",
            "mini Batch Loss: 1.4316011667251587\n",
            "mini Batch Loss: 1.4783802032470703\n",
            "mini Batch Loss: 0.7212203741073608\n",
            "mini Batch Loss: 2.7175536155700684\n",
            "mini Batch Loss: 1.454230546951294\n",
            "mini Batch Loss: 1.3666809797286987\n",
            "mini Batch Loss: 1.4932341575622559\n",
            "Training Batch: 61 | Training Loss: 1.4932341575622559\n",
            "Training Batch: 61 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_61.pt\n",
            "dist a: tensor([19.2322, 23.9204, 18.5950, 20.3343, 17.9465, 19.1587, 15.5941, 21.6728,\n",
            "        20.2032, 24.0562, 21.0047, 19.3039, 30.0724, 21.5962, 22.9241, 27.0190,\n",
            "        27.6211, 11.5810, 20.2652, 26.1503, 27.7002, 15.2072, 21.9054, 18.1455,\n",
            "        23.3856, 35.4284, 18.8381, 28.2904, 25.5693, 20.6053, 19.2295, 16.3799,\n",
            "        25.2891, 21.6681, 27.8997, 29.2414, 21.3214, 18.5996, 13.5485, 25.4059,\n",
            "        18.9690, 17.6195, 21.8436, 17.8088, 32.5978, 17.8525, 21.3091, 30.6445,\n",
            "        20.9711, 11.4989, 33.1956, 17.6538, 31.5671, 21.0516, 27.1960, 15.5721,\n",
            "        25.3925, 26.7630, 26.1522, 19.1269, 30.1040, 19.4665, 30.1358, 27.6896],\n",
            "       device='cuda:0'), dist b: tensor([37.1038, 33.5535, 34.9190, 17.3159, 27.2422, 31.6397, 32.7478, 25.0641,\n",
            "        35.4241, 38.7236, 21.3270, 29.6231, 34.2199, 23.4480, 30.6623, 22.5696,\n",
            "        24.2252, 29.1556, 26.3423, 35.8752, 30.1505, 31.1086, 28.0387, 24.4376,\n",
            "        23.2041, 40.2750, 26.8671, 31.9828, 23.7241, 23.2865, 27.6178, 26.7810,\n",
            "        35.8914, 27.9446, 33.3520, 24.6556, 32.2955, 21.8217, 23.4033, 35.3510,\n",
            "        25.9627, 25.4672, 28.3551, 28.5964, 32.3579, 35.3179, 34.8953, 31.8955,\n",
            "        30.9469, 27.1846, 39.7641, 25.7155, 24.5097, 26.7364, 17.4927, 31.2889,\n",
            "        23.2081, 24.6298, 18.2610, 38.7745, 30.8734, 32.4762, 40.7112, 27.7002],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.8125 \n",
            "tensor(0.8125)\n",
            "mini Batch Loss: 0.5341781377792358\n",
            "mini Batch Loss: 1.324688196182251\n",
            "mini Batch Loss: 1.257561206817627\n",
            "mini Batch Loss: 1.169101357460022\n",
            "mini Batch Loss: 1.387865662574768\n",
            "mini Batch Loss: 0.7035931944847107\n",
            "mini Batch Loss: 0.6150736212730408\n",
            "mini Batch Loss: 1.3317906856536865\n",
            "mini Batch Loss: 0.21024301648139954\n",
            "mini Batch Loss: 2.0105764865875244\n",
            "mini Batch Loss: 1.065970778465271\n",
            "mini Batch Loss: 0.4206481873989105\n",
            "mini Batch Loss: 0.9208099842071533\n",
            "mini Batch Loss: 0.5015228986740112\n",
            "mini Batch Loss: 0.29219937324523926\n",
            "mini Batch Loss: 0.4073631167411804\n",
            "mini Batch Loss: 1.4492169618606567\n",
            "mini Batch Loss: 0.34610477089881897\n",
            "mini Batch Loss: 0.787975013256073\n",
            "mini Batch Loss: 2.1575260162353516\n",
            "mini Batch Loss: 1.3830561637878418\n",
            "mini Batch Loss: 0.8853267431259155\n",
            "mini Batch Loss: 0.0925552248954773\n",
            "mini Batch Loss: 1.7508642673492432\n",
            "mini Batch Loss: 1.399385929107666\n",
            "mini Batch Loss: 0.7884835004806519\n",
            "mini Batch Loss: 0.8593822717666626\n",
            "mini Batch Loss: 5.13491678237915\n",
            "mini Batch Loss: 1.3127161264419556\n",
            "mini Batch Loss: 1.6921852827072144\n",
            "Training Batch: 91 | Training Loss: 1.6921852827072144\n",
            "Training Batch: 91 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_91.pt\n",
            "dist a: tensor([13.8640, 20.5532, 14.4730, 15.3844, 11.2955, 15.9297, 13.6998, 15.0839,\n",
            "        14.9424, 16.6045, 17.0673, 15.1326, 23.5005, 16.1215, 14.7777, 18.8621,\n",
            "        24.7682,  9.2612, 16.2659, 18.8495, 17.6158,  9.4564, 17.7485, 13.5805,\n",
            "        21.0753, 25.8864, 13.2577, 18.9103, 16.6337, 15.2434, 13.2629, 11.6248,\n",
            "        18.3761, 14.2152, 19.9817, 16.9372, 15.3507, 15.1134, 10.7180, 18.0975,\n",
            "        13.7543, 15.4132, 14.8809, 12.8459, 24.4314, 14.1114, 19.1093, 22.4782,\n",
            "        16.3055, 10.6179, 26.7071, 16.6889, 23.0540, 15.2181, 22.8107, 12.7551,\n",
            "        17.5384, 19.1079, 19.4736, 13.7734, 20.7927, 14.4983, 22.2772, 22.1793],\n",
            "       device='cuda:0'), dist b: tensor([28.3934, 20.1762, 27.3126, 13.2196, 20.0809, 23.4761, 23.9717, 19.0939,\n",
            "        26.7417, 29.7181, 18.3785, 22.4509, 26.7649, 19.6297, 25.4947, 14.9841,\n",
            "        20.1310, 25.7421, 19.5422, 25.7823, 24.6225, 27.7390, 19.6829, 18.3693,\n",
            "        18.4990, 29.9860, 22.6869, 28.1284, 16.8133, 17.9052, 22.4815, 20.0469,\n",
            "        26.9416, 25.8925, 27.6913, 20.0371, 25.9567, 16.4964, 17.0436, 29.0703,\n",
            "        21.4438, 20.9923, 19.6613, 21.5719, 25.5474, 31.3110, 24.7355, 25.6098,\n",
            "        26.3504, 25.3162, 31.7105, 20.2719, 18.1957, 18.6233, 15.0236, 22.9385,\n",
            "        22.1702, 17.9302, 13.8853, 32.5117, 19.4618, 24.9887, 27.4802, 21.2456],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 1.7501144409179688\n",
            "mini Batch Loss: 1.5763635635375977\n",
            "mini Batch Loss: 1.927668571472168\n",
            "mini Batch Loss: 0.8324594497680664\n",
            "mini Batch Loss: 1.8811378479003906\n",
            "mini Batch Loss: 0.8354191780090332\n",
            "mini Batch Loss: 1.6735758781433105\n",
            "mini Batch Loss: 1.9554393291473389\n",
            "mini Batch Loss: 1.1422266960144043\n",
            "mini Batch Loss: 1.3610594272613525\n",
            "mini Batch Loss: 1.3715693950653076\n",
            "mini Batch Loss: 0.942558765411377\n",
            "mini Batch Loss: 1.4766244888305664\n",
            "mini Batch Loss: 1.2659215927124023\n",
            "mini Batch Loss: 2.1837642192840576\n",
            "mini Batch Loss: 1.5533453226089478\n",
            "mini Batch Loss: 1.6776838302612305\n",
            "mini Batch Loss: 1.1290112733840942\n",
            "mini Batch Loss: 0.6545337438583374\n",
            "mini Batch Loss: 1.2550833225250244\n",
            "mini Batch Loss: 0.9229437112808228\n",
            "mini Batch Loss: 1.1565485000610352\n",
            "mini Batch Loss: 0.8513012528419495\n",
            "mini Batch Loss: 0.3314628303050995\n",
            "mini Batch Loss: 0.921923041343689\n",
            "mini Batch Loss: 1.1815663576126099\n",
            "mini Batch Loss: 1.052443027496338\n",
            "mini Batch Loss: 1.7143126726150513\n",
            "mini Batch Loss: 0.18616464734077454\n",
            "mini Batch Loss: 1.1436233520507812\n",
            "Training Batch: 121 | Training Loss: 1.1436233520507812\n",
            "Training Batch: 121 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_121.pt\n",
            "dist a: tensor([14.3586, 15.9414, 14.0201, 13.9852, 11.2920, 13.5277, 13.1860, 13.5715,\n",
            "        13.0466, 17.6747, 14.4471, 14.3149, 16.9975, 14.0084, 14.2565, 19.8784,\n",
            "        23.7902,  9.7814, 14.7483, 17.2618, 14.9445,  9.7383, 18.7852, 13.5064,\n",
            "        18.3791, 22.9801, 11.4929, 17.3451, 16.2652, 13.2118, 15.3765, 12.7165,\n",
            "        18.3278, 13.7842, 17.1796, 17.8786, 15.9293, 16.3273, 12.7429, 19.8591,\n",
            "        16.7034, 13.6274, 14.8887, 13.1354, 23.3944, 13.7664, 15.2322, 19.7035,\n",
            "        14.8674, 10.9148, 24.1605, 14.8145, 19.0136, 14.9630, 22.7488, 13.0533,\n",
            "        14.4088, 17.1489, 15.5170, 12.4429, 18.4065, 13.1930, 18.0378, 17.7549],\n",
            "       device='cuda:0'), dist b: tensor([26.3265, 23.3064, 27.1764, 13.3166, 15.8963, 18.0507, 21.0845, 19.2822,\n",
            "        23.7117, 25.5722, 14.8800, 17.9254, 22.8714, 16.5732, 21.0399, 16.9746,\n",
            "        21.6595, 23.7102, 15.9203, 26.4189, 23.1685, 22.9044, 17.3173, 16.6737,\n",
            "        17.6813, 25.5602, 20.9984, 26.9488, 15.3907, 13.5133, 23.3590, 19.6031,\n",
            "        21.7451, 17.6636, 26.6357, 19.7408, 21.5197, 16.1203, 17.5176, 25.6087,\n",
            "        17.2950, 17.1086, 19.7965, 24.6093, 20.6208, 24.7076, 22.7502, 20.6404,\n",
            "        22.3940, 20.3528, 29.9225, 18.2204, 17.8946, 18.0832, 12.8237, 19.4203,\n",
            "        16.3051, 17.5011, 12.8865, 29.5221, 24.1931, 17.5809, 25.7736, 18.8553],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.2706281542778015\n",
            "mini Batch Loss: 1.5979372262954712\n",
            "mini Batch Loss: 0.956412136554718\n",
            "mini Batch Loss: 0.46993085741996765\n",
            "mini Batch Loss: 2.8093109130859375\n",
            "mini Batch Loss: 0.5869324803352356\n",
            "mini Batch Loss: 1.369959831237793\n",
            "mini Batch Loss: 0.3657063841819763\n",
            "mini Batch Loss: 2.135185956954956\n",
            "mini Batch Loss: 1.4292182922363281\n",
            "mini Batch Loss: 0.7759108543395996\n",
            "mini Batch Loss: 1.5371547937393188\n",
            "mini Batch Loss: 2.206294298171997\n",
            "mini Batch Loss: 1.2367000579833984\n",
            "mini Batch Loss: 1.6392439603805542\n",
            "mini Batch Loss: 1.4809203147888184\n",
            "mini Batch Loss: 1.7831722497940063\n",
            "mini Batch Loss: 0.6112386584281921\n",
            "mini Batch Loss: 1.5463495254516602\n",
            "mini Batch Loss: 0.5655395984649658\n",
            "mini Batch Loss: 0.7920498847961426\n",
            "mini Batch Loss: 0.6384350657463074\n",
            "mini Batch Loss: 0.6412445306777954\n",
            "mini Batch Loss: 0.8460303544998169\n",
            "mini Batch Loss: 1.4236146211624146\n",
            "mini Batch Loss: 1.5285425186157227\n",
            "mini Batch Loss: 0.2709933817386627\n",
            "mini Batch Loss: 1.1748768091201782\n",
            "mini Batch Loss: 0.29231134057044983\n",
            "mini Batch Loss: 2.529390811920166\n",
            "Training Batch: 151 | Training Loss: 2.529390811920166\n",
            "Training Batch: 151 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_151.pt\n",
            "dist a: tensor([14.3743, 14.9466, 14.3278, 12.8068, 12.2946, 12.5426, 12.7687, 12.6721,\n",
            "        10.9903, 15.9000, 13.5179, 14.0398, 18.2059, 13.8075, 13.6676, 15.0931,\n",
            "        21.3557,  9.3098, 13.3020, 14.0065, 15.4466,  8.8987, 15.3973, 13.1813,\n",
            "        16.0236, 23.9435, 11.9754, 13.6373, 16.1581, 14.9870, 10.8425,  9.8955,\n",
            "        16.2907, 11.4132, 16.7961, 16.7441, 13.2360, 14.0210,  9.5173, 16.5922,\n",
            "        13.1298, 13.7661, 11.8376, 10.8748, 21.6581, 12.4665, 14.3430, 17.4516,\n",
            "        10.5953,  9.4728, 27.7391, 12.9417, 21.3978, 11.7177, 19.8314, 10.5643,\n",
            "        13.7831, 15.8261, 13.6182, 12.7538, 19.2501, 12.4110, 18.0868, 18.8863],\n",
            "       device='cuda:0'), dist b: tensor([25.0876, 21.2699, 23.9977, 12.5594, 15.7328, 18.4673, 21.4561, 19.4074,\n",
            "        23.2061, 24.4559, 14.1127, 15.4091, 21.7432, 16.5292, 22.1331, 12.3537,\n",
            "        19.8887, 24.2637, 20.9700, 24.3832, 21.2301, 21.3269, 18.7974, 15.4964,\n",
            "        16.3948, 28.6095, 16.4164, 21.9026, 16.4268, 14.2490, 16.9101, 15.7817,\n",
            "        19.6772, 17.3346, 20.7809, 19.6758, 21.3980, 15.8413, 13.4169, 25.1623,\n",
            "        16.5909, 18.2692, 15.0086, 19.9349, 19.3036, 23.0763, 22.1300, 22.2189,\n",
            "        17.9616, 16.7893, 31.0950, 16.4398, 15.7941, 14.7114, 10.8168, 19.4635,\n",
            "        17.5142, 17.4779, 11.6151, 30.3764, 24.4521, 15.7280, 24.4198, 18.1922],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.4179457426071167\n",
            "mini Batch Loss: 0.26173722743988037\n",
            "mini Batch Loss: 1.659663200378418\n",
            "mini Batch Loss: 1.1212049722671509\n",
            "mini Batch Loss: 0.7558335661888123\n",
            "mini Batch Loss: 0.5197843313217163\n",
            "mini Batch Loss: 0.7096688747406006\n",
            "mini Batch Loss: 2.2229201793670654\n",
            "mini Batch Loss: 0.3701460063457489\n",
            "mini Batch Loss: 0.83738112449646\n",
            "mini Batch Loss: 0.8897696137428284\n",
            "mini Batch Loss: 0.6007958650588989\n",
            "mini Batch Loss: 2.714329481124878\n",
            "mini Batch Loss: 0.07910507917404175\n",
            "mini Batch Loss: 3.033846139907837\n",
            "mini Batch Loss: 0.9750661849975586\n",
            "mini Batch Loss: 1.1701669692993164\n",
            "mini Batch Loss: 0.29709112644195557\n",
            "mini Batch Loss: 1.303755521774292\n",
            "mini Batch Loss: 1.380574107170105\n",
            "mini Batch Loss: 0.18137937784194946\n",
            "mini Batch Loss: 0.2573356032371521\n",
            "mini Batch Loss: 0.3471491038799286\n",
            "mini Batch Loss: 0.4844874143600464\n",
            "mini Batch Loss: 1.5112494230270386\n",
            "mini Batch Loss: 0.600530207157135\n",
            "mini Batch Loss: 0.6164751648902893\n",
            "mini Batch Loss: 0.6409239768981934\n",
            "mini Batch Loss: 0.4067213833332062\n",
            "mini Batch Loss: 0.3413201570510864\n",
            "Training Batch: 181 | Training Loss: 0.3413201570510864\n",
            "Training Batch: 181 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_181.pt\n",
            "dist a: tensor([16.4022, 15.4460, 14.1153, 14.0159, 10.1234, 14.7905, 12.4499, 12.7667,\n",
            "        11.7865, 17.5524, 15.4222, 14.1813, 23.5853, 13.6144, 13.8342, 16.4826,\n",
            "        21.9290,  9.2046, 14.7155, 16.1640, 16.1276, 10.8169, 16.7795, 14.7498,\n",
            "        17.0863, 23.5505, 12.5492, 15.5624, 16.1971, 15.1710, 11.4514, 10.3524,\n",
            "        16.8407, 13.5377, 19.7321, 15.6257, 14.9650, 14.4496,  9.7824, 20.3742,\n",
            "        14.0766, 14.7928, 12.2224, 11.4058, 23.6931, 15.0928, 16.9049, 20.5246,\n",
            "        12.2302,  9.6842, 25.3837, 13.8101, 20.6967, 12.2756, 22.5174, 11.5172,\n",
            "        14.1312, 19.5696, 14.5765, 14.8940, 18.7305, 12.1157, 18.2818, 20.1493],\n",
            "       device='cuda:0'), dist b: tensor([26.0629, 21.4144, 25.5536, 13.1343, 16.3178, 19.2514, 21.4020, 20.3399,\n",
            "        26.2717, 29.1092, 13.9536, 16.1295, 25.8099, 17.7157, 23.6815, 14.9840,\n",
            "        20.3312, 25.0643, 18.2869, 30.3556, 23.2172, 21.6375, 22.4756, 19.2060,\n",
            "        16.2463, 29.7782, 15.8964, 24.1547, 17.0101, 15.3943, 17.4794, 18.7383,\n",
            "        24.1923, 22.9865, 24.7819, 19.2934, 23.2284, 16.4492, 15.0607, 24.2965,\n",
            "        19.3326, 19.5593, 16.6444, 20.2905, 20.5522, 27.4312, 25.6295, 21.5676,\n",
            "        22.7824, 18.4442, 29.7106, 18.4447, 16.4995, 15.8647, 11.4039, 21.6745,\n",
            "        18.9172, 18.3225, 12.6050, 28.2422, 25.7008, 19.9748, 27.5269, 19.1579],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.22152318060398102\n",
            "mini Batch Loss: 1.1394858360290527\n",
            "mini Batch Loss: 0.8367481827735901\n",
            "mini Batch Loss: 0.6035858988761902\n",
            "mini Batch Loss: 1.0713375806808472\n",
            "mini Batch Loss: 1.243013858795166\n",
            "mini Batch Loss: 0.3934367001056671\n",
            "mini Batch Loss: 0.6150864362716675\n",
            "mini Batch Loss: 1.6328544616699219\n",
            "mini Batch Loss: 1.7307374477386475\n",
            "mini Batch Loss: 0.7512407302856445\n",
            "mini Batch Loss: 1.0078014135360718\n",
            "mini Batch Loss: 0.6021959781646729\n",
            "mini Batch Loss: 2.075188159942627\n",
            "mini Batch Loss: 1.6503591537475586\n",
            "mini Batch Loss: 0.2074890434741974\n",
            "mini Batch Loss: 0.8309247493743896\n",
            "mini Batch Loss: 1.0194406509399414\n",
            "mini Batch Loss: 3.2898969650268555\n",
            "mini Batch Loss: 0.3384082019329071\n",
            "mini Batch Loss: 1.4669066667556763\n",
            "mini Batch Loss: 0.19801300764083862\n",
            "mini Batch Loss: 0.6590839624404907\n",
            "mini Batch Loss: 0.0704563558101654\n",
            "mini Batch Loss: 0.15328651666641235\n",
            "mini Batch Loss: 0.055313482880592346\n",
            "mini Batch Loss: 0.7598873376846313\n",
            "mini Batch Loss: 0.04063741862773895\n",
            "mini Batch Loss: 0.37777411937713623\n",
            "mini Batch Loss: 0.203987255692482\n",
            "Training Batch: 211 | Training Loss: 0.203987255692482\n",
            "Training Batch: 211 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_211.pt\n",
            "dist a: tensor([13.8524, 12.4437, 12.6689, 11.7917,  9.5215, 11.5253, 10.6842, 11.0065,\n",
            "         9.4599, 14.4617, 12.9699, 12.4733, 15.4638, 11.5609, 12.3772, 14.4270,\n",
            "        16.5780,  7.5821, 12.2892, 12.2638, 13.9429,  8.9288, 14.4405, 12.7005,\n",
            "        15.2194, 21.2085,  9.3517, 12.8524, 13.4732, 12.5718, 10.3657,  9.8272,\n",
            "        14.0120, 10.9843, 17.5114, 13.5635, 12.6582, 11.5703,  8.6372, 17.6075,\n",
            "        12.4619, 12.6992, 10.7318,  9.5542, 16.2419, 13.3059, 13.9584, 16.5342,\n",
            "        10.9840,  8.1250, 19.5982, 13.6263, 17.0176, 10.1534, 17.4484, 10.0901,\n",
            "        11.2349, 15.4699, 12.9770, 12.2579, 15.2325,  9.5618, 17.0205, 17.3505],\n",
            "       device='cuda:0'), dist b: tensor([21.1612, 17.7398, 21.3032, 10.6518, 13.3637, 16.8940, 18.9175, 17.2019,\n",
            "        23.8611, 21.4518, 11.8334, 13.3889, 18.9365, 12.2923, 20.7706, 13.6056,\n",
            "        15.2003, 19.2806, 16.3715, 22.7092, 21.1151, 16.4592, 17.4704, 16.4437,\n",
            "        14.5032, 22.9795, 13.0519, 19.3374, 14.1030, 12.0355, 13.4660, 15.3087,\n",
            "        18.4670, 17.2852, 19.2365, 14.9551, 16.6172, 13.0335, 11.4668, 18.9288,\n",
            "        17.2062, 14.7889, 15.2681, 17.7072, 17.3471, 23.3397, 22.2308, 16.8102,\n",
            "        18.3075, 15.5456, 23.9124, 16.3480, 14.0759, 13.1181,  8.8759, 17.7004,\n",
            "        15.0006, 16.2139, 10.4306, 21.5770, 19.5394, 18.4390, 23.5346, 17.6543],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.2258903682231903\n",
            "mini Batch Loss: 0.7542396187782288\n",
            "mini Batch Loss: 0.7838225364685059\n",
            "mini Batch Loss: 0.31565728783607483\n",
            "mini Batch Loss: 1.1408621072769165\n",
            "mini Batch Loss: 0.9576878547668457\n",
            "mini Batch Loss: 0.9713319540023804\n",
            "mini Batch Loss: 2.1090941429138184\n",
            "mini Batch Loss: 2.2066385746002197\n",
            "mini Batch Loss: 0.8037117123603821\n",
            "mini Batch Loss: 0.3505633771419525\n",
            "mini Batch Loss: 0.3916032314300537\n",
            "mini Batch Loss: 0.7066147327423096\n",
            "mini Batch Loss: 0.12817569077014923\n",
            "mini Batch Loss: 0.0067023783922195435\n",
            "mini Batch Loss: 1.8630657196044922\n",
            "mini Batch Loss: 0.20686748623847961\n",
            "mini Batch Loss: 0.49977079033851624\n",
            "mini Batch Loss: 1.6789478063583374\n",
            "mini Batch Loss: 1.4861598014831543\n",
            "mini Batch Loss: 1.5644612312316895\n",
            "mini Batch Loss: 1.4053478240966797\n",
            "mini Batch Loss: 1.674523115158081\n",
            "mini Batch Loss: 2.2415595054626465\n",
            "mini Batch Loss: 1.1684155464172363\n",
            "mini Batch Loss: 0.7783159017562866\n",
            "mini Batch Loss: 2.0302255153656006\n",
            "mini Batch Loss: 2.249218225479126\n",
            "mini Batch Loss: 0.8216749429702759\n",
            "mini Batch Loss: 2.642690658569336\n",
            "Training Batch: 241 | Training Loss: 2.642690658569336\n",
            "Training Batch: 241 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_241.pt\n",
            "dist a: tensor([23.3893, 19.6428, 21.3210, 18.7624, 14.2218, 21.2373, 16.8736, 21.7855,\n",
            "        17.4493, 23.2568, 20.7972, 18.2538, 30.5701, 19.3974, 20.7739, 23.1512,\n",
            "        25.1701, 12.3302, 19.5790, 23.9023, 28.7907, 13.1163, 23.1769, 19.1837,\n",
            "        23.3934, 34.0317, 14.4788, 25.1329, 21.6411, 18.5974, 16.4813, 16.6506,\n",
            "        26.8807, 21.7957, 26.7965, 23.3809, 19.7265, 17.7060, 13.8999, 27.8493,\n",
            "        20.7546, 21.3744, 18.6341, 16.9995, 27.7570, 22.7133, 23.8783, 26.2351,\n",
            "        18.5923, 11.4457, 32.5630, 19.2022, 29.2997, 17.8430, 28.6724, 16.3411,\n",
            "        18.8733, 23.7010, 22.6346, 21.2062, 25.5668, 15.5686, 26.0070, 26.2011],\n",
            "       device='cuda:0'), dist b: tensor([37.1509, 25.9763, 33.3136, 19.1554, 22.3648, 24.7195, 30.9227, 27.1315,\n",
            "        39.2248, 39.9947, 19.9377, 24.5016, 31.4507, 21.7292, 35.2965, 21.7868,\n",
            "        26.6401, 31.6536, 26.1773, 37.5707, 31.8667, 31.1120, 28.6744, 27.1466,\n",
            "        20.7751, 37.6794, 25.1180, 34.6878, 22.8079, 20.0498, 25.3654, 27.8462,\n",
            "        39.2267, 30.3864, 31.6218, 23.3906, 28.2035, 20.4208, 19.7609, 32.5575,\n",
            "        25.9814, 26.4747, 27.0536, 25.6852, 30.5434, 38.2223, 34.5720, 29.3925,\n",
            "        32.2166, 28.4662, 36.3959, 27.0530, 22.5968, 22.9399, 14.8779, 28.8120,\n",
            "        25.9327, 27.2928, 16.6457, 37.5927, 31.2764, 28.4435, 38.3560, 29.6983],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.90625 \n",
            "tensor(0.9062)\n",
            "mini Batch Loss: 0.42035728693008423\n",
            "mini Batch Loss: 1.621621012687683\n",
            "mini Batch Loss: 0.005497470498085022\n",
            "mini Batch Loss: 0.24497374892234802\n",
            "mini Batch Loss: 1.931452751159668\n",
            "mini Batch Loss: 3.3962340354919434\n",
            "mini Batch Loss: 1.353244662284851\n",
            "mini Batch Loss: 1.495023488998413\n",
            "mini Batch Loss: 0.40132126212120056\n",
            "mini Batch Loss: 1.470770239830017\n",
            "mini Batch Loss: 0.6318894624710083\n",
            "mini Batch Loss: 0.10576415061950684\n",
            "mini Batch Loss: 0.2610705494880676\n",
            "mini Batch Loss: 0.030621394515037537\n",
            "mini Batch Loss: 0.9903782606124878\n",
            "mini Batch Loss: 0.7832549214363098\n",
            "mini Batch Loss: 1.7790393829345703\n",
            "mini Batch Loss: 0.0\n",
            "mini Batch Loss: 0.7154033184051514\n",
            "mini Batch Loss: 0.4777892827987671\n",
            "mini Batch Loss: 0.35645294189453125\n",
            "mini Batch Loss: 0.5524535179138184\n",
            "mini Batch Loss: 0.08235286176204681\n",
            "mini Batch Loss: 0.04688723385334015\n",
            "mini Batch Loss: 1.7699209451675415\n",
            "mini Batch Loss: 2.165128707885742\n",
            "mini Batch Loss: 1.3332988023757935\n",
            "mini Batch Loss: 0.402853786945343\n",
            "mini Batch Loss: 0.7837328910827637\n",
            "mini Batch Loss: 0.6591578722000122\n",
            "Training Batch: 271 | Training Loss: 0.6591578722000122\n",
            "Training Batch: 271 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_271.pt\n",
            "dist a: tensor([15.1123, 14.1236, 16.4063, 14.8925, 13.8417, 16.1242, 13.7782, 14.7760,\n",
            "        13.2062, 19.2003, 14.6503, 12.8308, 22.4454, 14.5171, 16.0868, 17.8851,\n",
            "        19.7544,  8.7622, 13.2234, 14.8927, 19.4442, 10.5549, 19.5025, 14.7269,\n",
            "        17.1341, 25.2679, 13.3335, 20.6000, 16.9861, 14.1162, 13.7447, 11.1893,\n",
            "        21.8353, 17.5714, 19.9376, 17.7037, 15.5136, 15.1483,  9.1281, 21.2012,\n",
            "        16.4409, 17.1387, 13.5452, 12.0700, 23.9107, 16.5556, 17.9196, 21.0670,\n",
            "        13.1307,  9.7342, 26.2589, 15.5762, 23.1811, 12.7817, 23.9729, 12.4146,\n",
            "        13.2345, 19.0026, 18.1207, 16.1225, 21.2385, 13.8323, 20.9617, 21.1077],\n",
            "       device='cuda:0'), dist b: tensor([30.5656, 20.7138, 25.5110, 16.0308, 17.8272, 19.9349, 24.9849, 18.8140,\n",
            "        30.6620, 29.5817, 17.0087, 18.7708, 22.5519, 14.5136, 27.6750, 17.0764,\n",
            "        19.9138, 25.4136, 21.8984, 27.4166, 26.9618, 23.2487, 23.1687, 19.5469,\n",
            "        16.1253, 33.4703, 20.5422, 28.9522, 18.0281, 16.0844, 16.3320, 22.1490,\n",
            "        30.7929, 24.8412, 26.2018, 18.1305, 21.8718, 17.3311, 16.9573, 25.8090,\n",
            "        18.6164, 22.9870, 18.7307, 18.4460, 20.7371, 32.3745, 26.4354, 22.7477,\n",
            "        24.7356, 20.9130, 30.1607, 20.0472, 15.9284, 18.5176, 11.0894, 22.8153,\n",
            "        21.0085, 20.9376, 13.7613, 32.0807, 25.8846, 19.2230, 29.9393, 22.2689],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.890625 \n",
            "tensor(0.8906)\n",
            "mini Batch Loss: 0.8262289762496948\n",
            "mini Batch Loss: 1.3792906999588013\n",
            "mini Batch Loss: 1.030216097831726\n",
            "mini Batch Loss: 0.11573731899261475\n",
            "mini Batch Loss: 0.6323124766349792\n",
            "mini Batch Loss: 0.4405585527420044\n",
            "mini Batch Loss: 0.3399355113506317\n",
            "mini Batch Loss: 1.932376742362976\n",
            "mini Batch Loss: 1.7088273763656616\n",
            "mini Batch Loss: 0.32067471742630005\n",
            "mini Batch Loss: 1.5208728313446045\n",
            "mini Batch Loss: 0.2211158573627472\n",
            "mini Batch Loss: 0.5605708956718445\n",
            "mini Batch Loss: 1.191312313079834\n",
            "mini Batch Loss: 1.088813066482544\n",
            "mini Batch Loss: 1.0848257541656494\n",
            "mini Batch Loss: 0.591525137424469\n",
            "mini Batch Loss: 0.024952441453933716\n",
            "mini Batch Loss: 0.6634035706520081\n",
            "mini Batch Loss: 1.1455485820770264\n",
            "mini Batch Loss: 0.1331394612789154\n",
            "mini Batch Loss: 0.7618998289108276\n",
            "mini Batch Loss: 0.5487980842590332\n",
            "mini Batch Loss: 0.6237406730651855\n",
            "mini Batch Loss: 1.675865650177002\n",
            "mini Batch Loss: 0.6176420450210571\n",
            "mini Batch Loss: 0.19954542815685272\n",
            "mini Batch Loss: 0.3947097659111023\n",
            "mini Batch Loss: 0.23588204383850098\n",
            "mini Batch Loss: 0.713403046131134\n",
            "Training Batch: 301 | Training Loss: 0.713403046131134\n",
            "Training Batch: 301 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_301.pt\n",
            "dist a: tensor([15.9906, 14.3940, 16.1871, 15.3955, 11.5878, 16.0531, 15.0061, 14.8064,\n",
            "        13.3292, 17.7587, 16.0283, 13.8056, 23.6247, 13.0696, 18.1823, 17.7845,\n",
            "        23.0429,  9.4467, 13.4597, 15.3798, 20.9957, 11.0518, 20.1991, 16.0427,\n",
            "        18.2188, 23.4735, 12.2113, 19.1588, 17.2030, 12.6933, 13.4783, 11.9481,\n",
            "        23.3528, 19.8744, 19.9924, 19.1942, 16.1781, 15.2397, 10.2259, 19.6406,\n",
            "        16.8301, 16.3395, 14.8232, 11.3798, 24.6173, 13.4325, 18.3589, 19.5119,\n",
            "        13.8413, 10.2302, 26.8211, 16.0416, 21.9465, 12.7075, 23.6653, 12.7972,\n",
            "        15.7669, 18.4791, 17.9562, 18.6454, 22.4095, 15.5200, 21.4623, 19.1755],\n",
            "       device='cuda:0'), dist b: tensor([29.4100, 21.1009, 25.6436, 16.6598, 18.0082, 19.1507, 24.8835, 18.9373,\n",
            "        29.1585, 31.9960, 15.2455, 17.5317, 24.3781, 18.5650, 22.2891, 16.2063,\n",
            "        23.9229, 25.3074, 19.3971, 30.4543, 24.8440, 22.3657, 25.0845, 21.6512,\n",
            "        17.0532, 30.4677, 18.7899, 28.1474, 18.6896, 16.6298, 19.1895, 21.5835,\n",
            "        29.8684, 26.0139, 29.7153, 18.3132, 19.6963, 16.8954, 16.9602, 25.8149,\n",
            "        18.7036, 19.7806, 20.3191, 20.0146, 21.4571, 25.8920, 26.3858, 20.4305,\n",
            "        22.8636, 20.2229, 27.3819, 20.3433, 16.9780, 19.0206, 11.9427, 20.0450,\n",
            "        19.8420, 24.4575, 14.4793, 29.3966, 23.7785, 18.5669, 26.8404, 24.5462],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 0.24371932446956635\n",
            "mini Batch Loss: 2.310823917388916\n",
            "mini Batch Loss: 1.1917200088500977\n",
            "mini Batch Loss: 0.24652820825576782\n",
            "mini Batch Loss: 4.477754592895508\n",
            "mini Batch Loss: 0.23916786909103394\n",
            "mini Batch Loss: 0.1811537742614746\n",
            "mini Batch Loss: 0.8521041870117188\n",
            "mini Batch Loss: 0.7450566291809082\n",
            "mini Batch Loss: 1.1412371397018433\n",
            "mini Batch Loss: 2.4329819679260254\n",
            "mini Batch Loss: 1.2703816890716553\n",
            "mini Batch Loss: 0.8907892107963562\n",
            "mini Batch Loss: 0.17301185429096222\n",
            "mini Batch Loss: 1.323930263519287\n",
            "mini Batch Loss: 0.14119303226470947\n",
            "mini Batch Loss: 1.0941354036331177\n",
            "mini Batch Loss: 0.15847358107566833\n",
            "mini Batch Loss: 0.656930685043335\n",
            "mini Batch Loss: 2.349522113800049\n",
            "mini Batch Loss: 0.7170657515525818\n",
            "mini Batch Loss: 1.567068099975586\n",
            "mini Batch Loss: 0.38529157638549805\n",
            "mini Batch Loss: 0.4099457859992981\n",
            "mini Batch Loss: 0.5427296757698059\n",
            "mini Batch Loss: 0.5968926548957825\n",
            "mini Batch Loss: 0.4289449155330658\n",
            "mini Batch Loss: 0.494193971157074\n",
            "mini Batch Loss: 1.103299617767334\n",
            "mini Batch Loss: 0.5807490348815918\n",
            "Training Batch: 331 | Training Loss: 0.5807490348815918\n",
            "Training Batch: 331 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_331.pt\n",
            "dist a: tensor([12.9202, 13.1368, 14.2389, 13.2736, 11.0255, 13.3801, 14.2190, 13.1029,\n",
            "        12.7145, 15.7894, 12.3869, 10.7890, 20.2289, 13.0775, 15.2545, 16.4206,\n",
            "        18.1644,  9.2952, 12.4870, 14.8990, 18.2816, 10.9940, 17.1239, 12.3593,\n",
            "        15.7449, 22.6527, 10.4326, 16.2368, 14.9695, 12.4114, 10.9469,  9.9528,\n",
            "        19.0269, 15.8419, 16.3752, 15.8101, 15.0919, 12.2649,  8.3167, 17.0149,\n",
            "        14.5333, 15.4993, 12.8048, 10.7889, 21.1708, 12.6662, 16.6897, 17.4496,\n",
            "        12.3474,  8.5525, 26.0867, 14.4417, 18.8534, 11.1494, 21.0471, 11.0682,\n",
            "        16.4030, 18.1585, 15.6356, 14.6019, 18.3354, 14.4386, 20.5684, 18.4908],\n",
            "       device='cuda:0'), dist b: tensor([25.0954, 18.5549, 22.2718, 16.0032, 15.8278, 18.7673, 23.6306, 17.1251,\n",
            "        25.7600, 28.1268, 13.6135, 16.5169, 21.9440, 15.1075, 19.9157, 14.2961,\n",
            "        18.8244, 22.4352, 19.0686, 25.7613, 21.1939, 18.4627, 21.7653, 16.5010,\n",
            "        16.0554, 28.1472, 16.8972, 23.0759, 17.4010, 13.6435, 14.5617, 17.0685,\n",
            "        22.8654, 20.1393, 20.9399, 17.9143, 17.2349, 14.7023, 12.5614, 19.7846,\n",
            "        15.5032, 18.9420, 17.4662, 19.4162, 18.8449, 24.1706, 24.5960, 18.3431,\n",
            "        18.7409, 16.8356, 28.4889, 16.8985, 13.5134, 15.6272, 11.4537, 17.2353,\n",
            "        17.0301, 20.9174, 13.2707, 27.9073, 22.0373, 13.9748, 22.3989, 20.0303],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.90625 \n",
            "tensor(0.9062)\n",
            "mini Batch Loss: 0.05959498882293701\n",
            "mini Batch Loss: 0.6416027545928955\n",
            "mini Batch Loss: 1.0245742797851562\n",
            "mini Batch Loss: 2.9368066787719727\n",
            "mini Batch Loss: 0.32862019538879395\n",
            "mini Batch Loss: 0.41254380345344543\n",
            "mini Batch Loss: 0.16494053602218628\n",
            "mini Batch Loss: 0.23486517369747162\n",
            "mini Batch Loss: 0.052174076437950134\n",
            "mini Batch Loss: 0.444864422082901\n",
            "mini Batch Loss: 0.8050558567047119\n",
            "mini Batch Loss: 1.4806740283966064\n",
            "mini Batch Loss: 0.29901087284088135\n",
            "mini Batch Loss: 0.008776158094406128\n",
            "mini Batch Loss: 1.417494535446167\n",
            "mini Batch Loss: 4.43780517578125\n",
            "mini Batch Loss: 2.005342960357666\n",
            "mini Batch Loss: 1.2160820960998535\n",
            "mini Batch Loss: 1.4910860061645508\n",
            "mini Batch Loss: 2.009019136428833\n",
            "mini Batch Loss: 1.550544261932373\n",
            "mini Batch Loss: 1.1347112655639648\n",
            "mini Batch Loss: 1.663926362991333\n",
            "mini Batch Loss: 1.616403579711914\n",
            "mini Batch Loss: 1.2753558158874512\n",
            "mini Batch Loss: 1.7315709590911865\n",
            "mini Batch Loss: 1.322938084602356\n",
            "mini Batch Loss: 1.5592381954193115\n",
            "mini Batch Loss: 1.621823787689209\n",
            "mini Batch Loss: 1.3228708505630493\n",
            "Training Batch: 361 | Training Loss: 1.3228708505630493\n",
            "Training Batch: 361 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_361.pt\n",
            "dist a: tensor([18.4173, 19.8784, 22.5435, 19.4685, 17.0046, 22.0046, 25.1691, 26.7370,\n",
            "        20.9672, 24.5141, 25.7221, 17.9481, 33.3450, 23.7816, 19.5919, 23.9557,\n",
            "        21.6226, 13.7498, 19.3066, 27.9949, 29.8390, 16.4539, 27.4515, 19.3170,\n",
            "        27.8980, 35.9340, 16.9019, 26.7730, 26.7402, 16.0249, 17.4548, 16.8162,\n",
            "        25.9257, 20.3910, 27.1905, 22.8895, 22.8574, 18.8871, 15.3882, 25.8355,\n",
            "        15.2730, 30.7650, 21.9322, 18.6837, 33.4131, 16.9644, 26.5599, 31.0751,\n",
            "        17.1752, 11.7258, 37.4063, 19.5276, 30.3369, 19.2970, 27.6308, 18.0360,\n",
            "        19.7011, 31.5066, 25.4406, 20.1783, 27.8231, 19.8698, 35.0312, 27.4652],\n",
            "       device='cuda:0'), dist b: tensor([40.4945, 26.9452, 32.1753, 20.2993, 28.9869, 30.0224, 35.6191, 30.5704,\n",
            "        40.2062, 38.8221, 22.5890, 21.6194, 31.8913, 28.5844, 28.9863, 20.1515,\n",
            "        19.4107, 33.7937, 26.7304, 38.0039, 38.2344, 35.4588, 30.6503, 28.0792,\n",
            "        22.9205, 41.3694, 27.9381, 36.6603, 22.9196, 22.5469, 24.4611, 27.2782,\n",
            "        36.4069, 34.2300, 32.7307, 22.1398, 30.7272, 19.8764, 21.2589, 32.1141,\n",
            "        24.7698, 29.1635, 24.5439, 27.7851, 30.1931, 37.7675, 34.5838, 29.2120,\n",
            "        28.6306, 29.2922, 40.0186, 25.7127, 22.7067, 23.3510, 14.6951, 25.5355,\n",
            "        25.7825, 30.8441, 17.8522, 41.3841, 31.3197, 25.6555, 34.4660, 27.0056],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.75 \n",
            "tensor(0.7500)\n",
            "mini Batch Loss: 1.641361951828003\n",
            "mini Batch Loss: 2.243403434753418\n",
            "mini Batch Loss: 1.4504444599151611\n",
            "mini Batch Loss: 1.679020881652832\n",
            "mini Batch Loss: 2.680750846862793\n",
            "mini Batch Loss: 1.337454915046692\n",
            "mini Batch Loss: 1.8728915452957153\n",
            "mini Batch Loss: 2.233104705810547\n",
            "mini Batch Loss: 1.39931058883667\n",
            "mini Batch Loss: 2.023542881011963\n",
            "mini Batch Loss: 1.9380834102630615\n",
            "mini Batch Loss: 1.871351718902588\n",
            "mini Batch Loss: 1.614692211151123\n",
            "mini Batch Loss: 1.9392815828323364\n",
            "mini Batch Loss: 1.067683458328247\n",
            "mini Batch Loss: 1.2707860469818115\n",
            "mini Batch Loss: 2.3092901706695557\n",
            "mini Batch Loss: 2.078545093536377\n",
            "mini Batch Loss: 1.311661720275879\n",
            "mini Batch Loss: 2.357609748840332\n",
            "mini Batch Loss: 1.7891062498092651\n",
            "mini Batch Loss: 1.0836446285247803\n",
            "mini Batch Loss: 1.6953716278076172\n",
            "mini Batch Loss: 2.1703639030456543\n",
            "mini Batch Loss: 1.2080223560333252\n",
            "mini Batch Loss: 1.7974541187286377\n",
            "mini Batch Loss: 1.3077894449234009\n",
            "mini Batch Loss: 1.276220679283142\n",
            "mini Batch Loss: 1.5719420909881592\n",
            "mini Batch Loss: 1.7930948734283447\n",
            "Training Batch: 391 | Training Loss: 1.7930948734283447\n",
            "Training Batch: 391 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_391.pt\n",
            "dist a: tensor([19.5133, 20.9818, 18.2490, 19.6361, 18.3615, 19.5534, 22.8661, 21.2443,\n",
            "        21.4219, 23.8022, 20.0869, 16.2518, 25.3648, 26.9154, 18.6179, 26.4453,\n",
            "        32.7800, 15.4938, 20.6726, 28.4534, 29.0135, 16.6922, 23.2205, 16.6292,\n",
            "        22.4195, 36.8261, 16.7786, 28.8208, 22.6079, 14.2344, 17.0309, 17.0162,\n",
            "        24.2572, 22.4696, 26.9743, 20.4675, 19.9272, 18.2137, 19.1918, 22.0105,\n",
            "        20.6458, 23.5843, 20.1099, 19.9726, 25.1118, 18.4044, 27.9319, 30.3577,\n",
            "        18.4309, 12.1321, 30.4259, 18.7939, 27.1902, 16.1953, 26.5770, 15.7202,\n",
            "        18.3421, 29.7536, 23.8446, 20.7308, 29.2616, 17.3553, 28.2966, 24.5133],\n",
            "       device='cuda:0'), dist b: tensor([34.8207, 22.9896, 31.6828, 19.5052, 23.6165, 30.3173, 32.4474, 27.6452,\n",
            "        35.1635, 39.1485, 20.9623, 21.9165, 27.2703, 26.2056, 27.9203, 24.7434,\n",
            "        31.3743, 29.5916, 25.6524, 37.4077, 37.1343, 31.4823, 25.2313, 26.5318,\n",
            "        21.5629, 40.8902, 30.0871, 37.9342, 21.3712, 19.1973, 25.3594, 29.7797,\n",
            "        34.8558, 34.0311, 32.3883, 23.9416, 26.0643, 21.1674, 24.0112, 30.2622,\n",
            "        29.9062, 29.5372, 26.8906, 31.5714, 25.8228, 33.8093, 33.9876, 26.3812,\n",
            "        33.6864, 28.8708, 35.1998, 25.2924, 23.5618, 26.8608, 18.4909, 27.8217,\n",
            "        20.7945, 27.6719, 16.0072, 36.1572, 29.1917, 27.7682, 38.5643, 24.4308],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.796875 \n",
            "tensor(0.7969)\n",
            "mini Batch Loss: 1.4343128204345703\n",
            "mini Batch Loss: 1.679189682006836\n",
            "mini Batch Loss: 1.1819217205047607\n",
            "mini Batch Loss: 1.885141372680664\n",
            "mini Batch Loss: 0.9515300989151001\n",
            "mini Batch Loss: 1.8621748685836792\n",
            "mini Batch Loss: 1.4219404458999634\n",
            "mini Batch Loss: 2.768990993499756\n",
            "mini Batch Loss: 1.8950865268707275\n",
            "mini Batch Loss: 1.7523293495178223\n",
            "mini Batch Loss: 1.0560226440429688\n",
            "mini Batch Loss: 0.896676778793335\n",
            "mini Batch Loss: 1.625725269317627\n",
            "mini Batch Loss: 2.0150046348571777\n",
            "mini Batch Loss: 1.137921929359436\n",
            "mini Batch Loss: 1.4216432571411133\n",
            "mini Batch Loss: 1.3233251571655273\n",
            "mini Batch Loss: 1.639491081237793\n",
            "mini Batch Loss: 2.3441355228424072\n",
            "mini Batch Loss: 1.3730177879333496\n",
            "mini Batch Loss: 1.4408931732177734\n",
            "mini Batch Loss: 1.7795764207839966\n",
            "mini Batch Loss: 1.2697699069976807\n",
            "mini Batch Loss: 1.497514247894287\n",
            "mini Batch Loss: 1.0072394609451294\n",
            "mini Batch Loss: 2.272303581237793\n",
            "mini Batch Loss: 1.6883199214935303\n",
            "mini Batch Loss: 1.868596076965332\n",
            "mini Batch Loss: 1.7642604112625122\n",
            "mini Batch Loss: 1.891379952430725\n",
            "Training Batch: 421 | Training Loss: 1.891379952430725\n",
            "Training Batch: 421 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_421.pt\n",
            "dist a: tensor([18.1205, 19.6149, 18.9270, 19.6790, 15.8794, 19.3754, 21.4266, 18.8861,\n",
            "        18.7947, 21.6457, 20.8415, 18.6323, 26.4050, 21.7774, 17.2043, 23.4215,\n",
            "        33.3001, 12.3610, 19.4302, 24.1454, 30.7955, 16.9110, 20.5659, 16.9156,\n",
            "        23.3065, 35.7123, 16.4729, 25.3610, 24.5752, 17.5914, 15.2710, 15.6047,\n",
            "        26.0428, 20.5736, 26.4068, 23.0780, 24.8000, 17.2763, 17.9376, 21.0207,\n",
            "        20.7579, 20.9671, 22.0124, 17.7604, 31.1603, 19.6465, 24.5041, 27.4910,\n",
            "        22.2983, 11.5720, 32.3444, 20.3200, 27.0691, 16.9547, 26.8740, 16.5795,\n",
            "        19.8026, 27.9001, 23.3031, 20.9478, 30.5944, 17.8676, 25.6974, 25.3267],\n",
            "       device='cuda:0'), dist b: tensor([32.8672, 22.3747, 32.7932, 20.3571, 24.2062, 29.9852, 33.4056, 26.2264,\n",
            "        35.1873, 37.7207, 19.1782, 26.3616, 27.6567, 19.3995, 30.0971, 22.1764,\n",
            "        33.6052, 29.7693, 24.7050, 34.3996, 33.0070, 32.3283, 27.2769, 28.5670,\n",
            "        24.9940, 38.9290, 30.8641, 34.1728, 24.7108, 21.8186, 26.6303, 30.2948,\n",
            "        34.8169, 31.8657, 35.6365, 26.8778, 32.9781, 19.5405, 23.0995, 31.4998,\n",
            "        28.6574, 25.6978, 25.6481, 30.2140, 24.1945, 33.8656, 38.0015, 25.0355,\n",
            "        35.0542, 29.3287, 33.2840, 26.4213, 25.3534, 24.9890, 14.9372, 26.7023,\n",
            "        23.7511, 28.5388, 16.9473, 42.7980, 28.0848, 28.8183, 37.8647, 24.6832],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 2.038588523864746\n",
            "mini Batch Loss: 1.6660606861114502\n",
            "mini Batch Loss: 1.1952108144760132\n",
            "mini Batch Loss: 1.0702310800552368\n",
            "mini Batch Loss: 1.8149120807647705\n",
            "mini Batch Loss: 1.758805274963379\n",
            "mini Batch Loss: 1.3223929405212402\n",
            "mini Batch Loss: 0.992052435874939\n",
            "mini Batch Loss: 1.3751198053359985\n",
            "mini Batch Loss: 1.1573853492736816\n",
            "mini Batch Loss: 1.673132061958313\n",
            "mini Batch Loss: 1.162787675857544\n",
            "mini Batch Loss: 1.625051736831665\n",
            "mini Batch Loss: 1.3513150215148926\n",
            "mini Batch Loss: 1.564201831817627\n",
            "mini Batch Loss: 1.6840139627456665\n",
            "mini Batch Loss: 1.3636257648468018\n",
            "mini Batch Loss: 1.9506890773773193\n",
            "mini Batch Loss: 1.1820282936096191\n",
            "mini Batch Loss: 0.7737808227539062\n",
            "mini Batch Loss: 1.278083324432373\n",
            "mini Batch Loss: 0.6869321465492249\n",
            "mini Batch Loss: 2.577859878540039\n",
            "mini Batch Loss: 1.8521525859832764\n",
            "mini Batch Loss: 1.1559789180755615\n",
            "mini Batch Loss: 0.6839560270309448\n",
            "mini Batch Loss: 1.8968913555145264\n",
            "mini Batch Loss: 1.207167387008667\n",
            "mini Batch Loss: 0.7629305720329285\n",
            "mini Batch Loss: 0.9656873941421509\n",
            "Training Batch: 451 | Training Loss: 0.9656873941421509\n",
            "Training Batch: 451 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_451.pt\n",
            "dist a: tensor([16.1139, 19.1028, 15.6264, 16.9349, 16.0509, 18.5241, 18.9522, 16.4380,\n",
            "        17.1636, 22.7092, 16.8704, 17.8611, 24.5814, 16.9256, 17.5446, 19.8981,\n",
            "        31.7616, 13.7161, 15.6366, 19.7556, 27.2047, 13.7588, 22.8190, 17.4365,\n",
            "        21.7311, 31.2436, 16.2011, 21.9314, 22.1712, 17.7701, 15.2072, 16.8405,\n",
            "        24.0933, 15.6118, 25.3348, 22.6275, 20.4241, 20.6894, 15.9611, 21.3543,\n",
            "        15.5328, 19.3474, 20.4675, 16.5212, 28.7982, 16.2045, 21.9585, 24.0333,\n",
            "        17.5895, 11.8325, 33.0551, 19.4795, 28.2096, 16.6556, 26.2389, 15.4411,\n",
            "        17.8756, 23.9811, 20.1795, 22.8396, 28.8252, 18.4932, 20.6321, 24.9053],\n",
            "       device='cuda:0'), dist b: tensor([31.7400, 25.7380, 27.8417, 18.9637, 26.3631, 24.5391, 27.7493, 24.2878,\n",
            "        30.1936, 34.6105, 17.9456, 20.4196, 26.5053, 20.4233, 28.8537, 16.0417,\n",
            "        29.6468, 28.3052, 25.1184, 29.9222, 30.4067, 29.8120, 25.1043, 23.5088,\n",
            "        23.7495, 34.0376, 24.0974, 31.0638, 21.2864, 18.4435, 22.6339, 29.0055,\n",
            "        31.6921, 22.7945, 30.4007, 21.6205, 26.4505, 19.2464, 20.1153, 32.5941,\n",
            "        27.0594, 24.1897, 25.5912, 25.1562, 21.7430, 25.7151, 33.9333, 23.7560,\n",
            "        28.5990, 26.4511, 34.5341, 24.0497, 24.8205, 24.3255, 13.3609, 22.9248,\n",
            "        19.8038, 28.2747, 15.9745, 37.7614, 25.2080, 27.3351, 29.9405, 23.5362],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.8125 \n",
            "tensor(0.8125)\n",
            "mini Batch Loss: 0.7802219390869141\n",
            "mini Batch Loss: 0.696526288986206\n",
            "mini Batch Loss: 1.4859764575958252\n",
            "mini Batch Loss: 2.2227015495300293\n",
            "mini Batch Loss: 1.2194461822509766\n",
            "mini Batch Loss: 1.7370381355285645\n",
            "mini Batch Loss: 1.503432035446167\n",
            "mini Batch Loss: 1.6102876663208008\n",
            "mini Batch Loss: 0.9007053375244141\n",
            "mini Batch Loss: 0.7459842562675476\n",
            "mini Batch Loss: 1.811194896697998\n",
            "mini Batch Loss: 0.5775820016860962\n",
            "mini Batch Loss: 0.7186537981033325\n",
            "mini Batch Loss: 1.2638624906539917\n",
            "mini Batch Loss: 0.947011411190033\n",
            "mini Batch Loss: 1.253882884979248\n",
            "mini Batch Loss: 1.4943139553070068\n",
            "mini Batch Loss: 1.3063666820526123\n",
            "mini Batch Loss: 1.047853708267212\n",
            "mini Batch Loss: 0.9251378774642944\n",
            "mini Batch Loss: 0.8491030335426331\n",
            "mini Batch Loss: 1.085707426071167\n",
            "mini Batch Loss: 0.9983659982681274\n",
            "mini Batch Loss: 1.0702085494995117\n",
            "mini Batch Loss: 0.554570198059082\n",
            "mini Batch Loss: 1.0268864631652832\n",
            "mini Batch Loss: 0.7857283353805542\n",
            "mini Batch Loss: 0.6296564936637878\n",
            "mini Batch Loss: 0.2757510542869568\n",
            "mini Batch Loss: 2.372983455657959\n",
            "Training Batch: 481 | Training Loss: 2.372983455657959\n",
            "Training Batch: 481 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_481.pt\n",
            "dist a: tensor([11.7277, 13.6113, 10.7777, 12.4102, 13.4126, 14.0451, 13.6138, 13.4865,\n",
            "        11.6457, 17.9114, 14.0586, 13.2557, 20.4531, 11.5756, 16.1646, 15.7202,\n",
            "        23.9618,  9.9278, 12.5307, 13.3253, 20.6495, 11.6924, 15.1100, 12.2192,\n",
            "        15.4342, 21.0136, 12.0726, 13.6987, 16.2302, 12.9970, 11.0369, 11.5003,\n",
            "        19.2270, 11.9391, 19.8047, 17.5140, 12.7673, 13.4684, 12.9021, 18.5835,\n",
            "        11.0990, 15.2590, 15.3771, 11.5291, 24.3789, 14.6576, 16.6896, 20.3185,\n",
            "        15.2395,  9.2663, 26.0996, 15.2487, 20.3679, 10.8336, 19.1699, 11.2910,\n",
            "        12.5621, 16.8557, 12.8924, 18.0660, 20.2791, 16.3871, 13.8049, 18.6635],\n",
            "       device='cuda:0'), dist b: tensor([24.2532, 22.1258, 23.0736, 13.9889, 18.3892, 18.5910, 20.1878, 17.4081,\n",
            "        21.7407, 29.8471, 13.9942, 17.7639, 22.4645, 15.1593, 23.1566, 12.4962,\n",
            "        26.6704, 22.8195, 18.9030, 23.8231, 23.9723, 21.8574, 19.1956, 16.9163,\n",
            "        18.6781, 25.1252, 18.7121, 24.8567, 16.1051, 13.6209, 18.4442, 20.7664,\n",
            "        24.9673, 19.1078, 24.9534, 16.7216, 21.4793, 14.2745, 18.2034, 24.8827,\n",
            "        19.8733, 19.5349, 16.8565, 18.5618, 16.8372, 23.5545, 27.5415, 19.0350,\n",
            "        23.5992, 21.1943, 26.8030, 17.8024, 16.3616, 17.7490, 10.2700, 19.1107,\n",
            "        15.8011, 20.7461, 12.9183, 27.3262, 19.9454, 16.4988, 24.8305, 18.2943],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 1.032161831855774\n",
            "mini Batch Loss: 0.9684195518493652\n",
            "mini Batch Loss: 2.087775230407715\n",
            "mini Batch Loss: 2.3010542392730713\n",
            "mini Batch Loss: 0.44598057866096497\n",
            "mini Batch Loss: 0.7757843732833862\n",
            "mini Batch Loss: 0.7983358502388\n",
            "mini Batch Loss: 0.7839125394821167\n",
            "mini Batch Loss: 2.5179696083068848\n",
            "mini Batch Loss: 0.9863256216049194\n",
            "mini Batch Loss: 0.5611504316329956\n",
            "mini Batch Loss: 0.39911532402038574\n",
            "mini Batch Loss: 0.3780478835105896\n",
            "mini Batch Loss: 0.2448066622018814\n",
            "mini Batch Loss: 1.3899345397949219\n",
            "mini Batch Loss: 0.9354066848754883\n",
            "mini Batch Loss: 0.7408634424209595\n",
            "mini Batch Loss: 0.2689880132675171\n",
            "mini Batch Loss: 0.13556179404258728\n",
            "mini Batch Loss: 0.09928838908672333\n",
            "mini Batch Loss: 0.8161581754684448\n",
            "mini Batch Loss: 1.2264013290405273\n",
            "mini Batch Loss: 0.1365557312965393\n",
            "mini Batch Loss: 1.584991693496704\n",
            "mini Batch Loss: 0.3562585115432739\n",
            "mini Batch Loss: 0.7278260588645935\n",
            "mini Batch Loss: 0.8726409077644348\n",
            "mini Batch Loss: 0.09104017913341522\n",
            "mini Batch Loss: 0.48479288816452026\n",
            "mini Batch Loss: 1.3521347045898438\n",
            "Training Batch: 511 | Training Loss: 1.3521347045898438\n",
            "Training Batch: 511 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_511.pt\n",
            "dist a: tensor([10.2158, 12.7146, 10.5032, 12.2363, 13.1869, 12.4601, 12.9510, 13.0104,\n",
            "         9.5648, 14.6064, 12.1164, 12.4214, 17.7408, 10.7051, 14.4216, 14.3694,\n",
            "        19.6696,  8.9348, 11.2213, 11.3090, 17.4608, 10.3397, 13.8571, 12.1747,\n",
            "        15.6000, 21.9963, 11.9132, 12.1106, 16.2198, 11.9228, 12.7186, 10.1315,\n",
            "        16.2663, 13.2668, 17.8156, 14.6999, 11.3249, 10.4315, 10.9857, 16.0655,\n",
            "        10.6811, 13.5390, 12.9224, 10.7422, 20.6288, 12.1542, 15.2854, 16.0717,\n",
            "        11.8219,  8.2897, 22.0640, 13.1638, 17.1772,  9.8456, 18.2374, 10.7747,\n",
            "        11.7476, 16.2845, 13.0326, 16.1216, 17.8411, 13.3017, 15.6208, 14.8247],\n",
            "       device='cuda:0'), dist b: tensor([20.7057, 19.6253, 17.9249, 11.8535, 17.6116, 15.8658, 18.3089, 15.9569,\n",
            "        19.6577, 23.2912, 13.0946, 16.1847, 19.9836, 14.7135, 19.5093, 11.6404,\n",
            "        19.9107, 20.0934, 15.8047, 19.1091, 23.1397, 18.0554, 18.5408, 15.1770,\n",
            "        15.7569, 25.0772, 15.8153, 19.8243, 14.2468, 12.5633, 18.3237, 17.8846,\n",
            "        20.0339, 15.5876, 23.3716, 14.3353, 16.8934, 13.0407, 14.5832, 20.5993,\n",
            "        16.8829, 16.8365, 15.5318, 16.6962, 16.3065, 19.8478, 22.8112, 15.7890,\n",
            "        17.8934, 17.9620, 23.9732, 14.7923, 13.9052, 14.9982,  8.9184, 16.9422,\n",
            "        15.5839, 18.3637, 12.5619, 23.5660, 19.3844, 14.3898, 21.6451, 15.8074],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.2198227494955063\n",
            "mini Batch Loss: 2.427518844604492\n",
            "mini Batch Loss: 1.1723452806472778\n",
            "mini Batch Loss: 0.23779478669166565\n",
            "mini Batch Loss: 1.6357921361923218\n",
            "mini Batch Loss: 1.5885010957717896\n",
            "mini Batch Loss: 0.6497677564620972\n",
            "mini Batch Loss: 0.0746251791715622\n",
            "mini Batch Loss: 0.5788360834121704\n",
            "mini Batch Loss: 0.7231083512306213\n",
            "mini Batch Loss: 1.2435914278030396\n",
            "mini Batch Loss: 0.2090272605419159\n",
            "mini Batch Loss: 1.1060978174209595\n",
            "mini Batch Loss: 1.5170886516571045\n",
            "mini Batch Loss: 0.7978618741035461\n",
            "mini Batch Loss: 1.3090124130249023\n",
            "mini Batch Loss: 0.740013599395752\n",
            "mini Batch Loss: 1.199833631515503\n",
            "mini Batch Loss: 0.21733364462852478\n",
            "mini Batch Loss: 0.5656477808952332\n",
            "mini Batch Loss: 0.18966835737228394\n",
            "mini Batch Loss: 0.6326525211334229\n",
            "mini Batch Loss: 0.809890627861023\n",
            "mini Batch Loss: 0.1177140474319458\n",
            "mini Batch Loss: 2.022407054901123\n",
            "mini Batch Loss: 1.1412127017974854\n",
            "mini Batch Loss: 1.4797850847244263\n",
            "mini Batch Loss: 0.8632912635803223\n",
            "mini Batch Loss: 0.18885064125061035\n",
            "mini Batch Loss: 1.1176273822784424\n",
            "Training Batch: 541 | Training Loss: 1.1176273822784424\n",
            "Training Batch: 541 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_541.pt\n",
            "dist a: tensor([ 9.3424, 13.5597, 11.4867, 13.1114, 13.8882, 14.1243, 13.4170, 14.9656,\n",
            "        11.2606, 19.2596, 13.7611, 15.3974, 18.9810, 11.5896, 16.8753, 16.4644,\n",
            "        17.2475,  7.3730, 11.9236, 14.2190, 19.5539, 11.2872, 13.7315, 11.9632,\n",
            "        16.8298, 24.8264, 12.4625, 16.0414, 18.9103, 11.4802, 12.1492, 11.0436,\n",
            "        19.5184, 13.3234, 19.8163, 15.5898, 14.4015, 11.6562, 11.2227, 17.9882,\n",
            "        13.9337, 12.6409, 14.9851, 12.4323, 21.4661, 12.5198, 16.7194, 18.1952,\n",
            "        13.8534,  9.3790, 24.2836, 13.7174, 19.1533, 10.9180, 17.8367, 12.5925,\n",
            "        13.6370, 18.3137, 13.9584, 17.1323, 19.0054, 15.0017, 16.7512, 16.4834],\n",
            "       device='cuda:0'), dist b: tensor([24.0957, 23.3678, 23.3893, 12.2667, 17.4578, 19.1645, 21.2034, 17.1293,\n",
            "        25.2962, 27.5474, 13.1303, 19.0210, 21.1114, 15.8141, 24.5371, 13.3167,\n",
            "        20.8658, 20.0209, 20.2783, 22.5793, 25.7123, 22.2811, 19.7487, 16.0103,\n",
            "        17.5637, 27.2523, 22.7318, 22.2224, 14.7316, 13.2524, 20.4237, 18.7964,\n",
            "        23.8273, 18.0756, 22.4033, 18.6239, 20.0740, 12.1487, 15.2350, 24.9109,\n",
            "        20.4335, 18.6459, 17.1309, 18.3230, 17.8932, 24.7137, 26.0429, 18.9923,\n",
            "        18.4282, 18.2223, 27.4824, 16.0980, 15.2056, 14.8682,  9.5262, 19.8056,\n",
            "        18.6780, 20.2066, 14.5855, 27.5134, 21.5237, 14.9212, 25.6564, 18.4376],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 0.5868961811065674\n",
            "mini Batch Loss: 0.3007352650165558\n",
            "mini Batch Loss: 0.6587225198745728\n",
            "mini Batch Loss: 0.5864022374153137\n",
            "mini Batch Loss: 1.205283284187317\n",
            "mini Batch Loss: 1.756987452507019\n",
            "mini Batch Loss: 0.44528308510780334\n",
            "mini Batch Loss: 1.127280592918396\n",
            "mini Batch Loss: 1.4206290245056152\n",
            "mini Batch Loss: 0.380443811416626\n",
            "mini Batch Loss: 0.48683053255081177\n",
            "mini Batch Loss: 0.31285056471824646\n",
            "mini Batch Loss: 0.2346835881471634\n",
            "mini Batch Loss: 1.114226222038269\n",
            "mini Batch Loss: 0.44012144207954407\n",
            "mini Batch Loss: 0.3999224901199341\n",
            "mini Batch Loss: 2.0975732803344727\n",
            "mini Batch Loss: 1.9729459285736084\n",
            "mini Batch Loss: 1.1812903881072998\n",
            "mini Batch Loss: 1.1392122507095337\n",
            "mini Batch Loss: 0.4283701181411743\n",
            "mini Batch Loss: 0.03579726815223694\n",
            "mini Batch Loss: 0.8451207876205444\n",
            "mini Batch Loss: 0.24284227192401886\n",
            "mini Batch Loss: 0.5062864422798157\n",
            "mini Batch Loss: 0.5934711694717407\n",
            "mini Batch Loss: 0.009847819805145264\n",
            "mini Batch Loss: 0.1806519329547882\n",
            "mini Batch Loss: 0.7257400751113892\n",
            "mini Batch Loss: 0.43896496295928955\n",
            "Training Batch: 571 | Training Loss: 0.43896496295928955\n",
            "Training Batch: 571 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_571.pt\n",
            "dist a: tensor([11.2736, 13.3245, 11.4284, 12.4032, 12.1418, 15.1755, 13.9572, 15.6187,\n",
            "        10.6862, 16.4493, 14.3481, 14.6114, 20.5163, 12.5257, 15.6976, 16.1696,\n",
            "        20.1529,  8.5025, 12.3927, 15.3343, 19.2971, 10.6806, 13.8072, 11.9732,\n",
            "        17.1685, 24.3607, 11.5566, 16.2617, 16.2254, 12.5409, 11.2790, 11.2285,\n",
            "        18.4822, 13.7284, 20.3344, 15.0342, 12.8252, 12.0961, 11.9850, 18.0092,\n",
            "        12.4950, 14.7744, 13.9085, 10.9687, 19.1642, 13.3111, 16.7523, 18.0370,\n",
            "        12.1325,  9.3410, 23.9421, 13.3025, 20.6757, 10.1559, 19.4305, 12.8097,\n",
            "        12.6196, 17.3138, 14.5541, 16.7754, 17.9757, 14.0055, 17.4812, 17.3444],\n",
            "       device='cuda:0'), dist b: tensor([25.9016, 21.1025, 22.7892, 11.9573, 15.8441, 18.0555, 20.9930, 17.8043,\n",
            "        27.4162, 26.4919, 12.5610, 19.0159, 20.5443, 15.2229, 24.3836, 13.6019,\n",
            "        22.8348, 21.6765, 19.3166, 26.8850, 24.4154, 23.7872, 20.4541, 16.6874,\n",
            "        16.4224, 27.0373, 22.6078, 23.6127, 15.4344, 13.5362, 20.9131, 20.0098,\n",
            "        23.9771, 22.2188, 22.3119, 18.0918, 20.2386, 13.2671, 14.8832, 25.7594,\n",
            "        20.7937, 19.5856, 16.9813, 19.5057, 18.7860, 25.1003, 24.7314, 18.2400,\n",
            "        20.2613, 18.5317, 27.6013, 15.8201, 15.2286, 14.1531,  9.2411, 18.6518,\n",
            "        19.0173, 18.9330, 12.7031, 28.4010, 21.7581, 15.6105, 26.0439, 19.5225],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 1.35141921043396\n",
            "mini Batch Loss: 0.38795173168182373\n",
            "mini Batch Loss: 1.122423529624939\n",
            "mini Batch Loss: 1.0863738059997559\n",
            "mini Batch Loss: 0.229131817817688\n",
            "mini Batch Loss: 0.3698790669441223\n",
            "mini Batch Loss: 0.17870163917541504\n",
            "mini Batch Loss: 1.3936011791229248\n",
            "mini Batch Loss: 0.4350488483905792\n",
            "mini Batch Loss: 0.6107302308082581\n",
            "mini Batch Loss: 1.1384711265563965\n",
            "mini Batch Loss: 0.6678165793418884\n",
            "mini Batch Loss: 0.24387389421463013\n",
            "mini Batch Loss: 0.015773892402648926\n",
            "mini Batch Loss: 0.07903626561164856\n",
            "mini Batch Loss: 2.703873634338379\n",
            "mini Batch Loss: 0.5038467645645142\n",
            "mini Batch Loss: 0.3027619421482086\n",
            "mini Batch Loss: 1.2738349437713623\n",
            "mini Batch Loss: 2.509247303009033\n",
            "mini Batch Loss: 0.6980679035186768\n",
            "mini Batch Loss: 0.13195443153381348\n",
            "mini Batch Loss: 1.3085490465164185\n",
            "mini Batch Loss: 0.4516042172908783\n",
            "mini Batch Loss: 0.10438597202301025\n",
            "mini Batch Loss: 1.1515419483184814\n",
            "mini Batch Loss: 1.6313035488128662\n",
            "mini Batch Loss: 1.055643081665039\n",
            "mini Batch Loss: 0.30649304389953613\n",
            "mini Batch Loss: 0.4754820466041565\n",
            "Training Batch: 601 | Training Loss: 0.4754820466041565\n",
            "Training Batch: 601 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_601.pt\n",
            "dist a: tensor([12.5956, 14.2669, 13.6228, 12.6948, 12.0801, 16.1203, 12.9106, 15.5941,\n",
            "        11.5323, 17.7943, 14.0260, 14.6538, 18.6926, 12.3177, 17.2634, 17.2615,\n",
            "        17.4358,  9.7347, 13.2866, 16.6207, 20.0888, 11.2193, 13.8962, 11.9631,\n",
            "        19.0610, 24.9933, 11.6173, 18.2900, 17.0547, 13.3696, 11.5708, 11.6234,\n",
            "        21.4615, 15.3398, 23.0581, 17.0032, 13.6905, 12.8442, 12.9214, 19.5419,\n",
            "        14.9943, 14.2385, 13.8002, 12.3084, 22.3034, 13.7276, 16.8980, 19.0935,\n",
            "        13.0173,  9.6724, 24.0817, 13.7399, 18.7432, 11.3522, 20.7562, 12.9967,\n",
            "        14.0244, 17.8928, 15.1922, 19.1312, 20.6815, 16.1736, 18.0709, 18.0752],\n",
            "       device='cuda:0'), dist b: tensor([28.5639, 24.2490, 24.5288, 11.5510, 15.9111, 18.0801, 24.5753, 17.5221,\n",
            "        27.5236, 26.8248, 12.0970, 23.6011, 19.0800, 14.5675, 23.7891, 14.1399,\n",
            "        20.1830, 22.5608, 19.2567, 28.8128, 23.5791, 23.5549, 21.7227, 17.5101,\n",
            "        17.5822, 27.9267, 23.4042, 24.0005, 16.9755, 15.9859, 22.7806, 19.8334,\n",
            "        26.5164, 22.2488, 24.1829, 18.8494, 21.9376, 14.1081, 16.9940, 25.6260,\n",
            "        18.6047, 19.9261, 19.1208, 22.4628, 17.7285, 26.1437, 26.2613, 18.6885,\n",
            "        20.4866, 19.3779, 29.1549, 16.4732, 17.7481, 15.7370, 11.3345, 21.7559,\n",
            "        19.9580, 20.6398, 13.9915, 28.7093, 24.4728, 15.3295, 26.7493, 18.8266],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.03430497646331787\n",
            "mini Batch Loss: 1.136474609375\n",
            "mini Batch Loss: 0.9682995676994324\n",
            "mini Batch Loss: 3.0017552375793457\n",
            "mini Batch Loss: 1.1089295148849487\n",
            "mini Batch Loss: 2.34112811088562\n",
            "mini Batch Loss: 1.6018837690353394\n",
            "mini Batch Loss: 0.9910847544670105\n",
            "mini Batch Loss: 2.455453872680664\n",
            "mini Batch Loss: 1.6922451257705688\n",
            "mini Batch Loss: 1.9139418601989746\n",
            "mini Batch Loss: 2.2825522422790527\n",
            "mini Batch Loss: 0.8993871212005615\n",
            "mini Batch Loss: 1.369369626045227\n",
            "mini Batch Loss: 1.8883615732192993\n",
            "mini Batch Loss: 1.6258654594421387\n",
            "mini Batch Loss: 2.0598039627075195\n",
            "mini Batch Loss: 2.1462507247924805\n",
            "mini Batch Loss: 1.1805591583251953\n",
            "mini Batch Loss: 2.2107999324798584\n",
            "mini Batch Loss: 1.3957774639129639\n",
            "mini Batch Loss: 1.242504596710205\n",
            "mini Batch Loss: 0.9294329881668091\n",
            "mini Batch Loss: 1.2741663455963135\n",
            "mini Batch Loss: 1.9176265001296997\n",
            "mini Batch Loss: 0.9367704391479492\n",
            "mini Batch Loss: 1.9000372886657715\n",
            "mini Batch Loss: 1.0402237176895142\n",
            "mini Batch Loss: 1.6512682437896729\n",
            "mini Batch Loss: 1.2265851497650146\n",
            "Training Batch: 631 | Training Loss: 1.2265851497650146\n",
            "Training Batch: 631 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_631.pt\n",
            "dist a: tensor([19.7496, 20.2990, 21.4725, 19.1765, 19.3569, 25.4429, 20.4011, 23.6437,\n",
            "        16.9881, 23.8369, 22.5035, 17.6016, 26.9458, 22.0421, 19.9024, 24.1983,\n",
            "        28.0020, 16.7705, 18.4133, 23.4392, 29.7780, 16.9544, 19.9554, 16.8794,\n",
            "        25.0740, 31.4622, 20.1091, 25.2359, 25.4604, 20.1819, 16.1928, 16.1164,\n",
            "        26.9974, 18.4165, 30.8562, 22.7595, 18.4543, 19.6015, 15.6368, 26.5428,\n",
            "        20.7896, 21.4764, 21.7890, 18.0465, 27.0545, 20.1494, 24.7017, 24.7167,\n",
            "        19.7552, 13.8236, 34.6629, 16.6975, 26.9149, 16.2514, 30.1839, 16.9429,\n",
            "        20.7448, 25.4207, 28.5914, 23.9330, 27.3802, 16.6866, 23.6575, 23.9137],\n",
            "       device='cuda:0'), dist b: tensor([40.7687, 31.6098, 33.0642, 19.1453, 22.7219, 26.3363, 38.0875, 30.8790,\n",
            "        37.5200, 44.0627, 21.5027, 28.7455, 26.9181, 21.2666, 36.3006, 21.5882,\n",
            "        33.4058, 31.9151, 25.2413, 39.8627, 36.6199, 34.5565, 31.2558, 27.4124,\n",
            "        25.0764, 36.2267, 29.7436, 30.4798, 22.9118, 22.7333, 31.2481, 29.7822,\n",
            "        38.7197, 32.4018, 31.7563, 25.2107, 33.2484, 20.3726, 25.0174, 34.8870,\n",
            "        26.4703, 27.7077, 29.7223, 28.0575, 23.9368, 36.9312, 34.1730, 22.5316,\n",
            "        30.8894, 29.7473, 37.2865, 23.3852, 22.6636, 22.7194, 15.9360, 30.3066,\n",
            "        26.6645, 26.9629, 17.5396, 39.2802, 32.5251, 33.0851, 41.4265, 28.2447],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 1.6051445007324219\n",
            "mini Batch Loss: 1.1016534566879272\n",
            "mini Batch Loss: 1.462438941001892\n",
            "mini Batch Loss: 1.3282145261764526\n",
            "mini Batch Loss: 0.9988431334495544\n",
            "mini Batch Loss: 0.9355694055557251\n",
            "mini Batch Loss: 0.3943476676940918\n",
            "mini Batch Loss: 0.5186737179756165\n",
            "mini Batch Loss: 0.013204500079154968\n",
            "mini Batch Loss: 2.5367674827575684\n",
            "mini Batch Loss: 2.26434063911438\n",
            "mini Batch Loss: 0.1521780639886856\n",
            "mini Batch Loss: 1.5717658996582031\n",
            "mini Batch Loss: 0.5256223678588867\n",
            "mini Batch Loss: 0.5010122656822205\n",
            "mini Batch Loss: 0.8162562251091003\n",
            "mini Batch Loss: 1.730911135673523\n",
            "mini Batch Loss: 0.6771242618560791\n",
            "mini Batch Loss: 1.9378952980041504\n",
            "mini Batch Loss: 0.5062425136566162\n",
            "mini Batch Loss: 1.6941289901733398\n",
            "mini Batch Loss: 1.1096405982971191\n",
            "mini Batch Loss: 1.7111878395080566\n",
            "mini Batch Loss: 1.2873461246490479\n",
            "mini Batch Loss: 0.20129431784152985\n",
            "mini Batch Loss: 0.39317917823791504\n",
            "mini Batch Loss: 0.0561409592628479\n",
            "mini Batch Loss: 0.6044273972511292\n",
            "mini Batch Loss: 0.32974955439567566\n",
            "mini Batch Loss: 1.064727544784546\n",
            "Training Batch: 661 | Training Loss: 1.064727544784546\n",
            "Training Batch: 661 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_661.pt\n",
            "dist a: tensor([12.0594, 12.9204, 12.1485, 12.0254, 13.3069, 13.5433, 12.9411, 13.5276,\n",
            "        10.9225, 14.9964, 14.9437, 12.2662, 17.0349, 11.9285, 13.3125, 15.3667,\n",
            "        21.5825,  9.9712, 12.2685, 12.5858, 16.5465, 10.3278, 13.4313, 11.4283,\n",
            "        18.9307, 20.2872, 11.9354, 13.3549, 15.2493, 10.5746,  9.9959, 10.7281,\n",
            "        18.3003, 12.3327, 20.3333, 14.9479, 11.8368, 10.7911, 10.5608, 16.5347,\n",
            "        12.7395, 14.8090, 12.0288, 11.9505, 22.1463, 13.0072, 15.4953, 16.1192,\n",
            "        12.5035,  8.4865, 21.9745, 13.0683, 18.9022, 11.3218, 21.9820, 10.6177,\n",
            "        11.6467, 14.2972, 13.3186, 16.5691, 19.0461, 13.5534, 14.1773, 13.9733],\n",
            "       device='cuda:0'), dist b: tensor([25.6658, 21.4281, 23.0835, 12.2258, 13.7052, 18.0287, 20.3284, 19.3922,\n",
            "        25.2874, 26.3656, 12.2056, 16.4111, 18.2553, 14.2089, 23.2349, 12.5771,\n",
            "        22.9416, 21.6896, 17.5359, 23.3868, 24.2747, 22.5941, 19.9430, 15.7383,\n",
            "        16.8639, 26.6271, 18.4540, 20.6884, 14.9364, 12.6170, 19.5999, 17.0076,\n",
            "        24.5181, 20.2345, 25.6701, 15.5094, 19.2828, 13.0045, 16.6481, 24.3299,\n",
            "        17.2606, 19.2361, 16.6585, 19.1835, 14.0202, 21.1624, 24.1297, 17.5331,\n",
            "        19.3695, 19.7845, 29.3287, 17.3090, 14.0220, 14.0995, 10.5987, 20.6075,\n",
            "        19.0823, 16.9943, 13.0911, 27.6719, 23.8600, 18.9148, 26.1451, 17.0485],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.875 \n",
            "tensor(0.8750)\n",
            "mini Batch Loss: 0.4036558270454407\n",
            "mini Batch Loss: 1.4984657764434814\n",
            "mini Batch Loss: 1.4464116096496582\n",
            "mini Batch Loss: 1.3637182712554932\n",
            "mini Batch Loss: 1.0282663106918335\n",
            "mini Batch Loss: 0.504003643989563\n",
            "mini Batch Loss: 0.15900421142578125\n",
            "mini Batch Loss: 1.6488173007965088\n",
            "mini Batch Loss: 0.44940072298049927\n",
            "mini Batch Loss: 0.9817924499511719\n",
            "mini Batch Loss: 0.12714748084545135\n",
            "mini Batch Loss: 0.01312190294265747\n",
            "mini Batch Loss: 0.38137173652648926\n",
            "mini Batch Loss: 4.756760597229004\n",
            "mini Batch Loss: 2.917667865753174\n",
            "mini Batch Loss: 0.7926949262619019\n",
            "mini Batch Loss: 0.02043771743774414\n",
            "mini Batch Loss: 0.47709405422210693\n",
            "mini Batch Loss: 0.4205296039581299\n",
            "mini Batch Loss: 0.47767043113708496\n",
            "mini Batch Loss: 2.1756770610809326\n",
            "mini Batch Loss: 0.6146535873413086\n",
            "mini Batch Loss: 0.24065828323364258\n",
            "mini Batch Loss: 2.267735481262207\n",
            "mini Batch Loss: 0.5909850597381592\n",
            "mini Batch Loss: 0.7009707093238831\n",
            "mini Batch Loss: 0.8887335062026978\n",
            "mini Batch Loss: 0.5027493834495544\n",
            "mini Batch Loss: 2.9748291969299316\n",
            "mini Batch Loss: 1.2442035675048828\n",
            "Training Batch: 691 | Training Loss: 1.2442035675048828\n",
            "Training Batch: 691 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_691.pt\n",
            "dist a: tensor([12.0120, 13.1443, 11.1411, 13.1769, 13.1614, 15.8686, 14.0163, 15.4880,\n",
            "        13.8088, 16.9236, 16.2648, 13.4020, 19.0942, 13.5245, 14.0529, 15.3178,\n",
            "        19.8312, 11.0858, 12.2746, 13.8638, 19.2761, 11.0440, 15.2122, 10.9947,\n",
            "        19.2465, 22.3185, 13.8046, 13.9033, 17.2999, 12.1747, 11.5903, 11.5284,\n",
            "        21.2629, 11.9124, 22.1234, 13.8652, 13.9042, 11.3661, 11.5251, 15.6071,\n",
            "        13.2295, 14.2911, 13.7417, 13.5442, 24.5161, 12.7838, 17.8305, 17.9501,\n",
            "        12.2856,  8.2727, 24.2518, 13.1477, 21.0270, 11.5385, 22.6691, 11.7202,\n",
            "        12.9132, 16.4448, 14.4644, 16.5240, 18.9827, 13.2514, 15.9331, 15.4014],\n",
            "       device='cuda:0'), dist b: tensor([26.1472, 22.8242, 23.0578, 11.7012, 15.6162, 18.1429, 23.0797, 20.3270,\n",
            "        25.3845, 27.8713, 13.7774, 18.2007, 19.7487, 17.6909, 21.8515, 12.8523,\n",
            "        21.7669, 24.6431, 19.0496, 29.4933, 26.0843, 22.2371, 20.8207, 15.2557,\n",
            "        17.0181, 28.0093, 18.2760, 21.2168, 15.5823, 14.1036, 19.7485, 18.2893,\n",
            "        27.3712, 20.9145, 23.7223, 17.9663, 19.6660, 13.0559, 17.1088, 24.5717,\n",
            "        16.3229, 19.7214, 18.2838, 18.9214, 17.3969, 22.0059, 23.1001, 19.2030,\n",
            "        20.8021, 19.2420, 31.0044, 16.9656, 14.5782, 16.6538,  9.9901, 19.5590,\n",
            "        19.7031, 19.3933, 12.7160, 31.9188, 22.9316, 19.2747, 28.2410, 18.8483],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.9200986623764038\n",
            "mini Batch Loss: 1.3059895038604736\n",
            "mini Batch Loss: 0.4368966221809387\n",
            "mini Batch Loss: 0.9295510053634644\n",
            "mini Batch Loss: 1.1139346361160278\n",
            "mini Batch Loss: 0.5142739415168762\n",
            "mini Batch Loss: 0.7199916839599609\n",
            "mini Batch Loss: 0.966938853263855\n",
            "mini Batch Loss: 1.5653258562088013\n",
            "mini Batch Loss: 0.3730049133300781\n",
            "mini Batch Loss: 0.40615254640579224\n",
            "mini Batch Loss: 0.059045419096946716\n",
            "mini Batch Loss: 0.13164205849170685\n",
            "mini Batch Loss: 2.340512752532959\n",
            "mini Batch Loss: 0.7755405902862549\n",
            "mini Batch Loss: 0.25607624650001526\n",
            "mini Batch Loss: 0.42957890033721924\n",
            "mini Batch Loss: 0.2521883249282837\n",
            "mini Batch Loss: 0.08171188831329346\n",
            "mini Batch Loss: 0.1686987280845642\n",
            "mini Batch Loss: 0.1668112725019455\n",
            "mini Batch Loss: 1.6642053127288818\n",
            "mini Batch Loss: 1.357342004776001\n",
            "mini Batch Loss: 0.17092019319534302\n",
            "mini Batch Loss: 0.41072437167167664\n",
            "mini Batch Loss: 1.6277410984039307\n",
            "mini Batch Loss: 0.03875705599784851\n",
            "mini Batch Loss: 0.1498289555311203\n",
            "mini Batch Loss: 2.064887523651123\n",
            "mini Batch Loss: 0.4008488059043884\n",
            "Training Batch: 721 | Training Loss: 0.4008488059043884\n",
            "Training Batch: 721 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_721.pt\n",
            "dist a: tensor([13.7808, 16.5352, 12.8137, 13.9281, 11.8108, 16.3785, 15.2940, 18.6444,\n",
            "        13.8058, 18.0801, 16.0216, 15.4821, 20.9290, 15.5369, 16.5091, 15.8682,\n",
            "        25.6047, 10.6958, 12.5693, 17.0871, 22.3621, 12.6580, 16.0169, 13.3872,\n",
            "        20.0174, 22.7289, 12.9279, 15.9125, 19.8185, 12.1855, 13.5270, 13.0659,\n",
            "        21.4281, 11.7239, 23.0606, 14.1969, 13.9193, 13.9430, 13.3625, 18.0421,\n",
            "        14.4071, 18.3615, 16.6707, 14.3139, 27.5933, 14.0626, 19.7419, 20.8361,\n",
            "        13.9916,  8.6595, 25.7076, 15.8480, 21.1917, 13.4934, 23.4353, 12.9017,\n",
            "        14.2157, 19.8884, 16.5984, 19.0886, 21.6714, 14.9401, 18.3146, 15.9794],\n",
            "       device='cuda:0'), dist b: tensor([25.5995, 25.1087, 24.1405, 13.0449, 16.7711, 22.4228, 24.9173, 23.7658,\n",
            "        27.8788, 31.5228, 14.5260, 20.8942, 25.1459, 16.4272, 25.3522, 14.7277,\n",
            "        25.2908, 23.7398, 19.8683, 32.5001, 29.4443, 22.6344, 23.5617, 17.5744,\n",
            "        18.4185, 29.3181, 20.9242, 21.0976, 18.6691, 15.2986, 21.9529, 22.5803,\n",
            "        24.5564, 22.0531, 24.4237, 17.0270, 20.1415, 15.0487, 17.4500, 25.2924,\n",
            "        17.1940, 19.4026, 21.6899, 21.1556, 19.8283, 22.1719, 27.7200, 19.0042,\n",
            "        23.4349, 20.6993, 30.1086, 22.4429, 17.5517, 19.3253, 11.7670, 22.4887,\n",
            "        17.9051, 23.2657, 14.2699, 31.0186, 25.4689, 25.5593, 29.8242, 18.2706],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.8551896214485168\n",
            "mini Batch Loss: 1.7684553861618042\n",
            "mini Batch Loss: 0.4263893961906433\n",
            "mini Batch Loss: 1.6138434410095215\n",
            "mini Batch Loss: 6.240827560424805\n",
            "mini Batch Loss: 0.36738869547843933\n",
            "mini Batch Loss: 1.7473928928375244\n",
            "mini Batch Loss: 0.9631217122077942\n",
            "mini Batch Loss: 0.8576263189315796\n",
            "mini Batch Loss: 2.1374411582946777\n",
            "mini Batch Loss: 0.2582065463066101\n",
            "mini Batch Loss: 1.2548637390136719\n",
            "mini Batch Loss: 0.44786399602890015\n",
            "mini Batch Loss: 0.21990397572517395\n",
            "mini Batch Loss: 1.5251414775848389\n",
            "mini Batch Loss: 2.573031425476074\n",
            "mini Batch Loss: 1.1596653461456299\n",
            "mini Batch Loss: 0.41974925994873047\n",
            "mini Batch Loss: 1.2416647672653198\n",
            "mini Batch Loss: 0.27964362502098083\n",
            "mini Batch Loss: 0.1365133821964264\n",
            "mini Batch Loss: 1.308693528175354\n",
            "mini Batch Loss: 0.5840493440628052\n",
            "mini Batch Loss: 0.25374990701675415\n",
            "mini Batch Loss: 1.943118929862976\n",
            "mini Batch Loss: 0.15043441951274872\n",
            "mini Batch Loss: 0.7420274019241333\n",
            "mini Batch Loss: 0.9414830207824707\n",
            "mini Batch Loss: 1.4788057804107666\n",
            "mini Batch Loss: 1.471400499343872\n",
            "Training Batch: 751 | Training Loss: 1.471400499343872\n",
            "Training Batch: 751 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_751.pt\n",
            "dist a: tensor([13.4162, 14.3706, 12.1333, 14.3660, 11.7460, 17.1255, 16.9019, 16.1883,\n",
            "        14.1192, 16.1151, 16.0225, 14.9505, 17.3852, 13.5677, 15.1548, 18.9003,\n",
            "        24.3349, 13.4422, 12.4653, 15.7854, 22.0568, 11.4471, 16.2963, 13.0303,\n",
            "        19.9517, 25.1192, 11.3439, 17.2467, 19.5246, 12.4192, 12.7776, 14.0248,\n",
            "        22.1146, 13.4653, 20.7693, 14.5448, 13.3777, 14.6076, 12.7551, 18.3171,\n",
            "        15.2133, 18.0800, 14.5145, 13.2539, 27.6867, 15.9531, 16.8530, 21.8918,\n",
            "        16.4129,  9.1095, 25.8103, 15.8196, 24.3226, 12.5346, 21.6210, 11.0374,\n",
            "        13.5258, 19.8479, 16.9326, 19.0414, 23.4645, 15.8128, 16.9481, 17.2790],\n",
            "       device='cuda:0'), dist b: tensor([30.5257, 22.1986, 25.4987, 12.6927, 16.6787, 22.7449, 25.8626, 22.3744,\n",
            "        31.1505, 27.5276, 13.3018, 19.3448, 22.3918, 19.0316, 27.8683, 14.2694,\n",
            "        21.9654, 28.7048, 17.4499, 29.2583, 27.3741, 25.9156, 20.8364, 15.3127,\n",
            "        16.9461, 28.1777, 22.4324, 25.6977, 17.4865, 13.7774, 19.5683, 23.7084,\n",
            "        27.3048, 24.0788, 22.4567, 17.6769, 23.1988, 17.3291, 18.0931, 27.9311,\n",
            "        19.7222, 19.6410, 20.0947, 20.2173, 17.3953, 25.3831, 25.1540, 21.9957,\n",
            "        27.4482, 22.4163, 32.8462, 20.9296, 18.0735, 16.3076, 11.6039, 23.1637,\n",
            "        20.5811, 21.5085, 14.5015, 39.5618, 28.1123, 20.4675, 31.8079, 19.3264],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.3405056595802307\n",
            "mini Batch Loss: 0.46306002140045166\n",
            "mini Batch Loss: 0.0\n",
            "mini Batch Loss: 0.107734814286232\n",
            "mini Batch Loss: 0.0\n",
            "mini Batch Loss: 0.39028501510620117\n",
            "mini Batch Loss: 0.8013452291488647\n",
            "mini Batch Loss: 0.2501240372657776\n",
            "mini Batch Loss: 0.021918952465057373\n",
            "mini Batch Loss: 0.10797181725502014\n",
            "mini Batch Loss: 1.4919929504394531\n",
            "mini Batch Loss: 0.0427493155002594\n",
            "mini Batch Loss: 1.1780890226364136\n",
            "mini Batch Loss: 0.2198333740234375\n",
            "mini Batch Loss: 0.7932283878326416\n",
            "mini Batch Loss: 0.46655482053756714\n",
            "mini Batch Loss: 2.864746570587158\n",
            "mini Batch Loss: 0.25674349069595337\n",
            "mini Batch Loss: 4.223289489746094\n",
            "mini Batch Loss: 1.6056667566299438\n",
            "mini Batch Loss: 1.6454296112060547\n",
            "mini Batch Loss: 2.068341016769409\n",
            "mini Batch Loss: 0.6186520457267761\n",
            "mini Batch Loss: 1.2101589441299438\n",
            "mini Batch Loss: 1.2115776538848877\n",
            "mini Batch Loss: 1.3464124202728271\n",
            "mini Batch Loss: 1.5794315338134766\n",
            "mini Batch Loss: 1.8676731586456299\n",
            "mini Batch Loss: 1.8163548707962036\n",
            "mini Batch Loss: 1.8812580108642578\n",
            "Training Batch: 781 | Training Loss: 1.8812580108642578\n",
            "Training Batch: 781 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_781.pt\n",
            "dist a: tensor([19.2553, 20.6680, 18.5241, 20.5128, 13.6863, 26.4156, 20.2603, 21.9627,\n",
            "        19.5029, 25.0376, 20.1223, 20.3778, 29.0164, 18.6541, 22.2755, 27.1529,\n",
            "        28.7572, 14.2817, 16.7629, 25.2444, 31.1773, 14.0414, 21.8914, 18.9329,\n",
            "        27.9917, 37.9319, 15.9261, 24.6530, 31.1254, 16.8107, 16.0858, 19.3229,\n",
            "        28.6002, 19.2622, 30.5008, 23.4606, 21.0383, 19.8828, 16.7953, 24.2046,\n",
            "        18.8553, 24.0590, 23.0394, 19.2646, 35.0115, 20.0084, 23.3587, 30.8888,\n",
            "        23.9228, 10.8459, 35.1942, 20.4259, 31.6033, 17.9025, 26.9823, 16.2345,\n",
            "        21.1340, 27.9456, 26.3025, 24.7964, 31.9328, 21.9164, 21.4633, 23.8529],\n",
            "       device='cuda:0'), dist b: tensor([41.0541, 30.7749, 33.1812, 16.6658, 23.5025, 35.2827, 33.8820, 31.9665,\n",
            "        43.5242, 40.5428, 16.7148, 23.9236, 32.9024, 22.0108, 37.4478, 19.4206,\n",
            "        29.5992, 33.2822, 22.9404, 39.1408, 36.4401, 36.3324, 29.8103, 25.0185,\n",
            "        22.9960, 38.1990, 27.7871, 33.5098, 27.9789, 21.2885, 31.1651, 32.9030,\n",
            "        35.4616, 32.9633, 33.5232, 27.1364, 30.1592, 23.2472, 25.0892, 35.4316,\n",
            "        26.9132, 25.2629, 30.1075, 27.1856, 27.3953, 34.4299, 34.7117, 26.2016,\n",
            "        36.2415, 30.7941, 39.3468, 29.1700, 25.1799, 23.7980, 13.6666, 29.0624,\n",
            "        24.5640, 28.6115, 18.8777, 53.1842, 35.4630, 25.2243, 39.7271, 28.2548],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.8770942687988281\n",
            "mini Batch Loss: 1.3789855241775513\n",
            "mini Batch Loss: 1.728029489517212\n",
            "mini Batch Loss: 2.189690351486206\n",
            "mini Batch Loss: 1.3454869985580444\n",
            "mini Batch Loss: 1.3699994087219238\n",
            "mini Batch Loss: 1.6625434160232544\n",
            "mini Batch Loss: 1.5550386905670166\n",
            "mini Batch Loss: 2.0347986221313477\n",
            "mini Batch Loss: 0.6177574396133423\n",
            "mini Batch Loss: 2.2299046516418457\n",
            "mini Batch Loss: 0.8708661794662476\n",
            "mini Batch Loss: 1.8822174072265625\n",
            "mini Batch Loss: 1.347548007965088\n",
            "mini Batch Loss: 1.448782205581665\n",
            "mini Batch Loss: 0.9230135083198547\n",
            "mini Batch Loss: 1.235737681388855\n",
            "mini Batch Loss: 1.3764784336090088\n",
            "mini Batch Loss: 1.1949766874313354\n",
            "mini Batch Loss: 1.1297986507415771\n",
            "mini Batch Loss: 2.087141513824463\n",
            "mini Batch Loss: 0.8559801578521729\n",
            "mini Batch Loss: 1.201254963874817\n",
            "mini Batch Loss: 0.8451915979385376\n",
            "mini Batch Loss: 0.2860519289970398\n",
            "mini Batch Loss: 0.9557043313980103\n",
            "mini Batch Loss: 0.7666351795196533\n",
            "mini Batch Loss: 1.0906974077224731\n",
            "mini Batch Loss: 1.5310916900634766\n",
            "mini Batch Loss: 0.22005373239517212\n",
            "Training Batch: 811 | Training Loss: 0.22005373239517212\n",
            "Training Batch: 811 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_811.pt\n",
            "dist a: tensor([13.1399, 16.2330, 12.9852, 13.8908,  9.8970, 15.3608, 14.5347, 13.7142,\n",
            "        12.8080, 15.0691, 17.2401, 12.4086, 14.6913, 13.2705, 16.5154, 20.7828,\n",
            "        21.5353, 11.2470, 12.1344, 17.6584, 20.2002, 11.3358, 15.9379, 13.0420,\n",
            "        21.4080, 24.0651, 12.5294, 15.8932, 18.5150, 13.1809, 12.7786, 14.0971,\n",
            "        22.7280, 13.5979, 20.6708, 15.2633, 13.9443, 12.4170, 11.7639, 16.9797,\n",
            "        15.2413, 15.3904, 15.3076, 13.6441, 21.9684, 12.1406, 16.1705, 19.7520,\n",
            "        15.9931,  9.0401, 24.8266, 14.4872, 20.1967, 11.9733, 21.7689, 11.4908,\n",
            "        12.3792, 16.9131, 17.0132, 17.8043, 21.0542, 17.1364, 15.4459, 19.1717],\n",
            "       device='cuda:0'), dist b: tensor([26.0253, 24.6992, 25.4819, 14.1282, 14.7743, 23.3332, 19.7145, 20.5178,\n",
            "        27.7469, 27.2005, 15.1277, 18.7384, 18.1419, 18.1989, 29.2403, 16.7445,\n",
            "        21.3381, 24.0830, 18.6603, 28.2066, 23.4602, 21.8370, 18.6404, 14.8821,\n",
            "        18.5746, 26.3868, 22.8183, 25.8493, 16.7480, 14.4923, 23.2510, 21.0531,\n",
            "        24.6704, 19.8591, 23.4291, 19.4994, 16.0355, 16.5121, 18.7942, 24.2122,\n",
            "        17.5159, 19.1605, 22.8299, 19.9765, 15.4280, 22.0834, 25.8981, 18.5350,\n",
            "        23.4646, 23.8693, 29.0501, 19.8067, 16.0651, 17.9402, 11.2131, 20.9094,\n",
            "        19.3007, 21.0814, 15.2788, 31.7743, 24.5125, 20.2744, 30.1006, 21.2217],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.84375 \n",
            "tensor(0.8438)\n",
            "mini Batch Loss: 0.3583291471004486\n",
            "mini Batch Loss: 0.30961304903030396\n",
            "mini Batch Loss: 0.6579649448394775\n",
            "mini Batch Loss: 2.0074212551116943\n",
            "mini Batch Loss: 0.5949651002883911\n",
            "mini Batch Loss: 0.3540281653404236\n",
            "mini Batch Loss: 0.8715234994888306\n",
            "mini Batch Loss: 1.1574167013168335\n",
            "mini Batch Loss: 0.40009063482284546\n",
            "mini Batch Loss: 0.10018880665302277\n",
            "mini Batch Loss: 0.6190841197967529\n",
            "mini Batch Loss: 1.4347984790802002\n",
            "mini Batch Loss: 0.28010404109954834\n",
            "mini Batch Loss: 0.25733375549316406\n",
            "mini Batch Loss: 1.0251641273498535\n",
            "mini Batch Loss: 0.24479608237743378\n",
            "mini Batch Loss: 0.5374000072479248\n",
            "mini Batch Loss: 1.6288124322891235\n",
            "mini Batch Loss: 0.20599417388439178\n",
            "mini Batch Loss: 0.7533448338508606\n",
            "mini Batch Loss: 1.3635884523391724\n",
            "mini Batch Loss: 0.17652484774589539\n",
            "mini Batch Loss: 0.015634864568710327\n",
            "mini Batch Loss: 1.6338112354278564\n",
            "mini Batch Loss: 0.34364545345306396\n",
            "mini Batch Loss: 3.1850745677948\n",
            "mini Batch Loss: 0.7145748734474182\n",
            "mini Batch Loss: 1.2714691162109375\n",
            "mini Batch Loss: 2.4549434185028076\n",
            "mini Batch Loss: 0.4017459750175476\n",
            "Training Batch: 841 | Training Loss: 0.4017459750175476\n",
            "Training Batch: 841 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_841.pt\n",
            "dist a: tensor([15.6911, 18.4795, 15.9857, 16.1993, 14.4302, 18.3426, 17.2043, 16.8998,\n",
            "        14.6611, 18.0408, 19.8498, 15.4751, 21.2340, 15.4844, 18.5566, 22.9401,\n",
            "        25.9530, 11.3767, 16.2411, 20.3178, 25.0131, 11.1392, 21.1899, 14.3407,\n",
            "        24.2569, 29.1062, 14.6510, 18.9234, 23.4447, 16.0810, 14.3510, 14.3298,\n",
            "        24.5534, 18.0872, 24.7103, 19.2775, 16.4043, 14.4296, 13.5551, 20.1766,\n",
            "        18.3413, 22.0111, 15.7358, 16.6434, 28.3577, 17.5587, 21.3788, 22.7775,\n",
            "        17.1439,  9.0328, 28.1961, 17.9008, 31.3792, 15.0692, 25.1096, 14.3398,\n",
            "        16.9658, 20.3670, 19.9226, 20.5264, 24.7937, 18.5023, 20.6243, 21.7266],\n",
            "       device='cuda:0'), dist b: tensor([31.6088, 27.1910, 28.8265, 16.4501, 18.0147, 24.4480, 23.7393, 26.2196,\n",
            "        35.6069, 31.4075, 17.1317, 21.7763, 23.3088, 23.2430, 34.9528, 18.3560,\n",
            "        23.9431, 27.6601, 23.8150, 32.1338, 33.0324, 29.5949, 24.9162, 19.8335,\n",
            "        21.3074, 33.5501, 25.9844, 27.7170, 21.4361, 20.3342, 27.1941, 24.5926,\n",
            "        31.2879, 27.1939, 27.9652, 21.1963, 20.3217, 18.4126, 24.5159, 29.7400,\n",
            "        21.6626, 24.5060, 23.6644, 21.7318, 20.3375, 27.4668, 28.0239, 24.6488,\n",
            "        24.9944, 25.6632, 32.1017, 24.3936, 20.1274, 21.6266, 11.2607, 22.7826,\n",
            "        22.3556, 23.3830, 18.2016, 39.2677, 25.6349, 25.2115, 34.3380, 24.0324],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.700446605682373\n",
            "mini Batch Loss: 3.136219024658203\n",
            "mini Batch Loss: 0.7248747944831848\n",
            "mini Batch Loss: 1.0817316770553589\n",
            "mini Batch Loss: 0.14809861779212952\n",
            "mini Batch Loss: 0.9109939336776733\n",
            "mini Batch Loss: 2.5031847953796387\n",
            "mini Batch Loss: 0.9969242811203003\n",
            "mini Batch Loss: 0.46569210290908813\n",
            "mini Batch Loss: 1.8940773010253906\n",
            "mini Batch Loss: 1.2216904163360596\n",
            "mini Batch Loss: 0.9272339344024658\n",
            "mini Batch Loss: 1.2712420225143433\n",
            "mini Batch Loss: 1.4894757270812988\n",
            "mini Batch Loss: 0.7700549364089966\n",
            "mini Batch Loss: 0.9404207468032837\n",
            "mini Batch Loss: 0.16975809633731842\n",
            "mini Batch Loss: 0.5304708480834961\n",
            "mini Batch Loss: 0.9233413934707642\n",
            "mini Batch Loss: 1.0438538789749146\n",
            "mini Batch Loss: 1.3916349411010742\n",
            "mini Batch Loss: 0.7118365168571472\n",
            "mini Batch Loss: 0.3536190390586853\n",
            "mini Batch Loss: 0.3127557635307312\n",
            "mini Batch Loss: 2.034092664718628\n",
            "mini Batch Loss: 1.0907021760940552\n",
            "mini Batch Loss: 1.2834858894348145\n",
            "mini Batch Loss: 0.2193695306777954\n",
            "mini Batch Loss: 1.5177068710327148\n",
            "mini Batch Loss: 1.4574586153030396\n",
            "Training Batch: 871 | Training Loss: 1.4574586153030396\n",
            "Training Batch: 871 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_871.pt\n",
            "dist a: tensor([15.9824, 12.5460, 14.6645, 17.5566, 13.3738, 16.9833, 15.7931, 17.5157,\n",
            "        15.8525, 15.4039, 18.6727, 13.2974, 25.4932, 15.4340, 16.8676, 22.5034,\n",
            "        24.2106, 12.7969, 16.2404, 20.1999, 24.6768, 10.4431, 20.1040, 14.0616,\n",
            "        24.0589, 27.9918, 13.8923, 18.4489, 18.3436, 15.4835, 11.5049, 13.9745,\n",
            "        21.0068, 16.6684, 22.1801, 16.8100, 15.5934, 14.6502, 12.7846, 20.2146,\n",
            "        18.6305, 17.3564, 13.7219, 14.7435, 24.1912, 15.0751, 22.2412, 24.2739,\n",
            "        16.3822, 10.4554, 28.3169, 15.8002, 29.5394, 15.1381, 23.0041, 14.1279,\n",
            "        17.0327, 18.9939, 19.5844, 19.9211, 20.5810, 16.4737, 23.0905, 19.5298],\n",
            "       device='cuda:0'), dist b: tensor([32.6933, 20.4836, 26.1511, 16.9165, 19.3649, 22.6847, 22.7263, 22.7612,\n",
            "        32.4633, 28.8403, 17.1555, 23.0185, 25.2948, 21.4614, 29.5344, 18.1767,\n",
            "        22.7412, 26.5260, 26.2327, 28.2090, 29.4014, 28.1653, 25.9614, 19.3599,\n",
            "        20.5663, 31.3298, 26.3607, 27.6144, 19.7863, 18.7557, 24.0902, 24.2065,\n",
            "        27.8357, 23.4505, 26.3730, 21.0386, 19.3723, 18.9055, 20.8565, 29.1267,\n",
            "        21.2569, 22.3501, 21.2591, 20.8525, 21.6497, 22.2875, 26.1784, 23.8709,\n",
            "        27.0452, 23.6600, 34.3231, 21.0888, 18.6294, 18.9795, 10.7722, 20.7477,\n",
            "        19.9827, 20.9373, 16.5649, 38.0342, 24.9235, 22.8510, 32.3125, 22.0132],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.828125 \n",
            "tensor(0.8281)\n",
            "mini Batch Loss: 0.5147209167480469\n",
            "mini Batch Loss: 0.92856764793396\n",
            "mini Batch Loss: 1.0999168157577515\n",
            "mini Batch Loss: 1.0415024757385254\n",
            "mini Batch Loss: 1.092722773551941\n",
            "mini Batch Loss: 1.230084776878357\n",
            "mini Batch Loss: 1.2286930084228516\n",
            "mini Batch Loss: 2.144423484802246\n",
            "mini Batch Loss: 0.7972241044044495\n",
            "mini Batch Loss: 0.2225264608860016\n",
            "mini Batch Loss: 0.6222103834152222\n",
            "mini Batch Loss: 0.12686362862586975\n",
            "mini Batch Loss: 0.8622166514396667\n",
            "mini Batch Loss: 1.0564351081848145\n",
            "mini Batch Loss: 0.8161370158195496\n",
            "mini Batch Loss: 2.614342451095581\n",
            "mini Batch Loss: 1.4673212766647339\n",
            "mini Batch Loss: 0.6438574194908142\n",
            "mini Batch Loss: 0.1609526127576828\n",
            "mini Batch Loss: 1.1304395198822021\n",
            "mini Batch Loss: 0.7943262457847595\n",
            "mini Batch Loss: 0.44135329127311707\n",
            "mini Batch Loss: 0.6231205463409424\n",
            "mini Batch Loss: 0.6951440572738647\n",
            "mini Batch Loss: 0.5846587419509888\n",
            "mini Batch Loss: 1.6535675525665283\n",
            "mini Batch Loss: 0.7439051866531372\n",
            "mini Batch Loss: 0.6513278484344482\n",
            "mini Batch Loss: 0.3339093327522278\n",
            "mini Batch Loss: 0.7889976501464844\n",
            "Training Batch: 901 | Training Loss: 0.7889976501464844\n",
            "Training Batch: 901 | Model saved to: /content/drive/My Drive/test4/model_epoch_5_batch_901.pt\n",
            "dist a: tensor([14.1660, 11.6419, 12.4608, 13.3891, 11.1612, 15.4731, 12.7290, 13.4970,\n",
            "        11.6517, 16.1305, 15.9827, 11.4183, 19.8110, 14.0042, 14.7724, 19.5476,\n",
            "        20.6515,  9.0532, 14.9732, 17.3398, 21.2162, 10.2296, 17.2267, 11.3542,\n",
            "        19.7902, 25.0141, 12.2539, 16.6485, 15.4026, 13.1835, 11.4992, 13.8146,\n",
            "        20.0883, 16.6095, 20.1384, 14.8219, 14.1573, 13.0424, 10.2089, 17.7322,\n",
            "        17.8435, 14.3201, 11.3526, 12.2383, 20.9913, 13.3599, 19.4089, 19.1558,\n",
            "        15.6081,  9.9578, 25.7881, 13.0753, 24.0321, 12.6448, 21.7961, 12.5736,\n",
            "        15.5620, 19.9084, 16.0042, 16.0434, 18.9055, 14.0820, 15.4367, 18.1676],\n",
            "       device='cuda:0'), dist b: tensor([28.1354, 22.3884, 25.0315, 13.2954, 17.5898, 20.7189, 20.1265, 17.0333,\n",
            "        26.0946, 28.1195, 13.6735, 21.0893, 21.9921, 16.7010, 26.4409, 16.2427,\n",
            "        19.9104, 23.0506, 18.9932, 25.7521, 24.9080, 24.5386, 22.2196, 17.4560,\n",
            "        18.5863, 27.3507, 22.3436, 25.7808, 17.1846, 15.9048, 20.3946, 21.8204,\n",
            "        27.0285, 20.9822, 24.1243, 16.1830, 17.2328, 16.3750, 19.9383, 24.5777,\n",
            "        18.3885, 17.9618, 19.2172, 18.5493, 14.2335, 20.0287, 25.3150, 18.0462,\n",
            "        24.1842, 20.5668, 31.2549, 18.2830, 17.5251, 16.3906, 10.2255, 18.5917,\n",
            "        17.3598, 21.0662, 16.3632, 30.9680, 22.4469, 20.8886, 28.1717, 18.8832],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "mini Batch Loss: 0.48576468229293823\n",
            "mini Batch Loss: 0.6493779420852661\n",
            "mini Batch Loss: 0.11847060918807983\n",
            "mini Batch Loss: 0.7526804804801941\n",
            "mini Batch Loss: 0.5147425532341003\n",
            "mini Batch Loss: 0.8483538627624512\n",
            "mini Batch Loss: 1.4765596389770508\n",
            "mini Batch Loss: 0.32467958331108093\n",
            "mini Batch Loss: 1.3188235759735107\n",
            "mini Batch Loss: 0.6095703840255737\n",
            "mini Batch Loss: 0.7146169543266296\n",
            "mini Batch Loss: 0.004483699798583984\n",
            "mini Batch Loss: 1.249889850616455\n",
            "mini Batch Loss: 0.5169827342033386\n",
            "mini Batch Loss: 0.9353141188621521\n",
            "mini Batch Loss: 1.1896965503692627\n",
            "mini Batch Loss: 0.24309229850769043\n",
            "mini Batch Loss: 0.6155747175216675\n",
            "mini Batch Loss: 0.19217100739479065\n",
            "mini Batch Loss: 0.10892491787672043\n",
            "[5] average loss per epoch: 1.070\n",
            "Saved model checkpoint to /content/drive/My Drive/test4/model_epoch_5.pt\n",
            "dist a: tensor([13.9675, 11.5221, 12.2898, 12.4814, 11.3084, 15.2520, 13.5860, 12.9594,\n",
            "        11.8475, 14.1466, 16.6678, 10.4638, 19.5930, 14.6643, 15.3214, 18.5072,\n",
            "        20.8414,  9.2124, 14.7256, 17.3397, 20.2050,  9.8645, 16.2104, 10.2119,\n",
            "        19.8743, 22.1271, 13.1894, 16.1058, 15.5334, 13.2260, 12.1942, 14.6579,\n",
            "        19.8556, 14.4009, 18.6636, 13.5702, 12.9423, 13.9019, 10.8540, 15.9572,\n",
            "        17.3957, 13.0467, 10.4399, 11.5963, 21.5887, 13.0727, 19.1825, 16.2006,\n",
            "        13.8231,  9.4398, 24.2878, 14.1476, 22.4661, 11.7252, 20.3545, 10.8353,\n",
            "        15.0687, 20.1416, 16.3702, 16.1278, 19.4236, 14.3603, 17.4255, 16.3330],\n",
            "       device='cuda:0'), dist b: tensor([25.8641, 21.5256, 23.4535, 12.2269, 17.2406, 19.3874, 19.8585, 18.0981,\n",
            "        25.6924, 26.7689, 15.7253, 18.4979, 22.8720, 18.1512, 25.4203, 16.4886,\n",
            "        20.3063, 22.4334, 17.5299, 24.7619, 24.0301, 22.0240, 21.8730, 18.4663,\n",
            "        17.3998, 24.8174, 21.5436, 24.7875, 17.1826, 16.7430, 19.0932, 20.1221,\n",
            "        25.9953, 19.5556, 21.2679, 14.6136, 17.8197, 17.1421, 19.7362, 21.9049,\n",
            "        17.4186, 16.4887, 17.1894, 17.8605, 14.3137, 20.0216, 23.3472, 16.9615,\n",
            "        22.4840, 19.2245, 27.6696, 19.1041, 16.2573, 15.7093, 10.1300, 18.7122,\n",
            "        17.4056, 21.0942, 15.5627, 29.5937, 22.8041, 21.1117, 27.7312, 16.8079],\n",
            "       device='cuda:0')\n",
            "random batch accuracy: 0.859375 \n",
            "tensor(0.8594)\n",
            "================== START PREDICTION ==================\n",
            "batch:  0\n",
            "batch:  1\n",
            "batch:  2\n",
            "batch:  3\n",
            "batch:  4\n",
            "batch:  5\n",
            "batch:  6\n",
            "batch:  7\n",
            "batch:  8\n",
            "batch:  9\n",
            "batch:  10\n",
            "batch:  11\n",
            "batch:  12\n",
            "batch:  13\n",
            "batch:  14\n",
            "batch:  15\n",
            "batch:  16\n",
            "batch:  17\n",
            "batch:  18\n",
            "batch:  19\n",
            "batch:  20\n",
            "batch:  21\n",
            "batch:  22\n",
            "batch:  23\n",
            "batch:  24\n",
            "batch:  25\n",
            "batch:  26\n",
            "batch:  27\n",
            "batch:  28\n",
            "batch:  29\n",
            "batch:  30\n",
            "batch:  31\n",
            "batch:  32\n",
            "batch:  33\n",
            "batch:  34\n",
            "batch:  35\n",
            "batch:  36\n",
            "batch:  37\n",
            "batch:  38\n",
            "batch:  39\n",
            "batch:  40\n",
            "batch:  41\n",
            "batch:  42\n",
            "batch:  43\n",
            "batch:  44\n",
            "batch:  45\n",
            "batch:  46\n",
            "batch:  47\n",
            "batch:  48\n",
            "batch:  49\n",
            "batch:  50\n",
            "batch:  51\n",
            "batch:  52\n",
            "batch:  53\n",
            "batch:  54\n",
            "batch:  55\n",
            "batch:  56\n",
            "batch:  57\n",
            "batch:  58\n",
            "batch:  59\n",
            "batch:  60\n",
            "batch:  61\n",
            "batch:  62\n",
            "batch:  63\n",
            "batch:  64\n",
            "batch:  65\n",
            "batch:  66\n",
            "batch:  67\n",
            "batch:  68\n",
            "batch:  69\n",
            "batch:  70\n",
            "batch:  71\n",
            "batch:  72\n",
            "batch:  73\n",
            "batch:  74\n",
            "batch:  75\n",
            "batch:  76\n",
            "batch:  77\n",
            "batch:  78\n",
            "batch:  79\n",
            "batch:  80\n",
            "batch:  81\n",
            "batch:  82\n",
            "batch:  83\n",
            "batch:  84\n",
            "batch:  85\n",
            "batch:  86\n",
            "batch:  87\n",
            "batch:  88\n",
            "batch:  89\n",
            "batch:  90\n",
            "batch:  91\n",
            "batch:  92\n",
            "batch:  93\n",
            "batch:  94\n",
            "batch:  95\n",
            "batch:  96\n",
            "batch:  97\n",
            "batch:  98\n",
            "batch:  99\n",
            "batch:  100\n",
            "batch:  101\n",
            "batch:  102\n",
            "batch:  103\n",
            "batch:  104\n",
            "batch:  105\n",
            "batch:  106\n",
            "batch:  107\n",
            "batch:  108\n",
            "batch:  109\n",
            "batch:  110\n",
            "batch:  111\n",
            "batch:  112\n",
            "batch:  113\n",
            "batch:  114\n",
            "batch:  115\n",
            "batch:  116\n",
            "batch:  117\n",
            "batch:  118\n",
            "batch:  119\n",
            "batch:  120\n",
            "batch:  121\n",
            "batch:  122\n",
            "batch:  123\n",
            "batch:  124\n",
            "batch:  125\n",
            "batch:  126\n",
            "batch:  127\n",
            "batch:  128\n",
            "batch:  129\n",
            "batch:  130\n",
            "batch:  131\n",
            "batch:  132\n",
            "batch:  133\n",
            "batch:  134\n",
            "batch:  135\n",
            "batch:  136\n",
            "batch:  137\n",
            "batch:  138\n",
            "batch:  139\n",
            "batch:  140\n",
            "batch:  141\n",
            "batch:  142\n",
            "batch:  143\n",
            "batch:  144\n",
            "batch:  145\n",
            "batch:  146\n",
            "batch:  147\n",
            "batch:  148\n",
            "batch:  149\n",
            "batch:  150\n",
            "batch:  151\n",
            "batch:  152\n",
            "batch:  153\n",
            "batch:  154\n",
            "batch:  155\n",
            "batch:  156\n",
            "batch:  157\n",
            "batch:  158\n",
            "batch:  159\n",
            "batch:  160\n",
            "batch:  161\n",
            "batch:  162\n",
            "batch:  163\n",
            "batch:  164\n",
            "batch:  165\n",
            "batch:  166\n",
            "batch:  167\n",
            "batch:  168\n",
            "batch:  169\n",
            "batch:  170\n",
            "batch:  171\n",
            "batch:  172\n",
            "batch:  173\n",
            "batch:  174\n",
            "batch:  175\n",
            "batch:  176\n",
            "batch:  177\n",
            "batch:  178\n",
            "batch:  179\n",
            "batch:  180\n",
            "batch:  181\n",
            "batch:  182\n",
            "batch:  183\n",
            "batch:  184\n",
            "batch:  185\n",
            "batch:  186\n",
            "batch:  187\n",
            "batch:  188\n",
            "batch:  189\n",
            "batch:  190\n",
            "batch:  191\n",
            "batch:  192\n",
            "batch:  193\n",
            "batch:  194\n",
            "batch:  195\n",
            "batch:  196\n",
            "batch:  197\n",
            "batch:  198\n",
            "batch:  199\n",
            "batch:  200\n",
            "batch:  201\n",
            "batch:  202\n",
            "batch:  203\n",
            "batch:  204\n",
            "batch:  205\n",
            "batch:  206\n",
            "batch:  207\n",
            "batch:  208\n",
            "batch:  209\n",
            "batch:  210\n",
            "batch:  211\n",
            "batch:  212\n",
            "batch:  213\n",
            "batch:  214\n",
            "batch:  215\n",
            "batch:  216\n",
            "batch:  217\n",
            "batch:  218\n",
            "batch:  219\n",
            "batch:  220\n",
            "batch:  221\n",
            "batch:  222\n",
            "batch:  223\n",
            "batch:  224\n",
            "batch:  225\n",
            "batch:  226\n",
            "batch:  227\n",
            "batch:  228\n",
            "batch:  229\n",
            "batch:  230\n",
            "batch:  231\n",
            "batch:  232\n",
            "batch:  233\n",
            "batch:  234\n",
            "batch:  235\n",
            "batch:  236\n",
            "batch:  237\n",
            "batch:  238\n",
            "batch:  239\n",
            "batch:  240\n",
            "batch:  241\n",
            "batch:  242\n",
            "batch:  243\n",
            "batch:  244\n",
            "batch:  245\n",
            "batch:  246\n",
            "batch:  247\n",
            "batch:  248\n",
            "batch:  249\n",
            "batch:  250\n",
            "batch:  251\n",
            "batch:  252\n",
            "batch:  253\n",
            "batch:  254\n",
            "batch:  255\n",
            "batch:  256\n",
            "batch:  257\n",
            "batch:  258\n",
            "batch:  259\n",
            "batch:  260\n",
            "batch:  261\n",
            "batch:  262\n",
            "batch:  263\n",
            "batch:  264\n",
            "batch:  265\n",
            "batch:  266\n",
            "batch:  267\n",
            "batch:  268\n",
            "batch:  269\n",
            "batch:  270\n",
            "batch:  271\n",
            "batch:  272\n",
            "batch:  273\n",
            "batch:  274\n",
            "batch:  275\n",
            "batch:  276\n",
            "batch:  277\n",
            "batch:  278\n",
            "batch:  279\n",
            "batch:  280\n",
            "batch:  281\n",
            "batch:  282\n",
            "batch:  283\n",
            "batch:  284\n",
            "batch:  285\n",
            "batch:  286\n",
            "batch:  287\n",
            "batch:  288\n",
            "batch:  289\n",
            "batch:  290\n",
            "batch:  291\n",
            "batch:  292\n",
            "batch:  293\n",
            "batch:  294\n",
            "batch:  295\n",
            "batch:  296\n",
            "batch:  297\n",
            "batch:  298\n",
            "batch:  299\n",
            "batch:  300\n",
            "batch:  301\n",
            "batch:  302\n",
            "batch:  303\n",
            "batch:  304\n",
            "batch:  305\n",
            "batch:  306\n",
            "batch:  307\n",
            "batch:  308\n",
            "batch:  309\n",
            "batch:  310\n",
            "batch:  311\n",
            "batch:  312\n",
            "batch:  313\n",
            "batch:  314\n",
            "batch:  315\n",
            "batch:  316\n",
            "batch:  317\n",
            "batch:  318\n",
            "batch:  319\n",
            "batch:  320\n",
            "batch:  321\n",
            "batch:  322\n",
            "batch:  323\n",
            "batch:  324\n",
            "batch:  325\n",
            "batch:  326\n",
            "batch:  327\n",
            "batch:  328\n",
            "batch:  329\n",
            "batch:  330\n",
            "batch:  331\n",
            "batch:  332\n",
            "batch:  333\n",
            "batch:  334\n",
            "batch:  335\n",
            "batch:  336\n",
            "batch:  337\n",
            "batch:  338\n",
            "batch:  339\n",
            "batch:  340\n",
            "batch:  341\n",
            "batch:  342\n",
            "batch:  343\n",
            "batch:  344\n",
            "batch:  345\n",
            "batch:  346\n",
            "batch:  347\n",
            "batch:  348\n",
            "batch:  349\n",
            "batch:  350\n",
            "batch:  351\n",
            "batch:  352\n",
            "batch:  353\n",
            "batch:  354\n",
            "batch:  355\n",
            "batch:  356\n",
            "batch:  357\n",
            "batch:  358\n",
            "batch:  359\n",
            "batch:  360\n",
            "batch:  361\n",
            "batch:  362\n",
            "batch:  363\n",
            "batch:  364\n",
            "batch:  365\n",
            "batch:  366\n",
            "batch:  367\n",
            "batch:  368\n",
            "batch:  369\n",
            "batch:  370\n",
            "batch:  371\n",
            "batch:  372\n",
            "batch:  373\n",
            "batch:  374\n",
            "batch:  375\n",
            "batch:  376\n",
            "batch:  377\n",
            "batch:  378\n",
            "batch:  379\n",
            "batch:  380\n",
            "batch:  381\n",
            "batch:  382\n",
            "batch:  383\n",
            "batch:  384\n",
            "batch:  385\n",
            "batch:  386\n",
            "batch:  387\n",
            "batch:  388\n",
            "batch:  389\n",
            "batch:  390\n",
            "batch:  391\n",
            "batch:  392\n",
            "batch:  393\n",
            "batch:  394\n",
            "batch:  395\n",
            "batch:  396\n",
            "batch:  397\n",
            "batch:  398\n",
            "batch:  399\n",
            "batch:  400\n",
            "batch:  401\n",
            "batch:  402\n",
            "batch:  403\n",
            "batch:  404\n",
            "batch:  405\n",
            "batch:  406\n",
            "batch:  407\n",
            "batch:  408\n",
            "batch:  409\n",
            "batch:  410\n",
            "batch:  411\n",
            "batch:  412\n",
            "batch:  413\n",
            "batch:  414\n",
            "batch:  415\n",
            "batch:  416\n",
            "batch:  417\n",
            "batch:  418\n",
            "batch:  419\n",
            "batch:  420\n",
            "batch:  421\n",
            "batch:  422\n",
            "batch:  423\n",
            "batch:  424\n",
            "batch:  425\n",
            "batch:  426\n",
            "batch:  427\n",
            "batch:  428\n",
            "batch:  429\n",
            "batch:  430\n",
            "batch:  431\n",
            "batch:  432\n",
            "batch:  433\n",
            "batch:  434\n",
            "batch:  435\n",
            "batch:  436\n",
            "batch:  437\n",
            "batch:  438\n",
            "batch:  439\n",
            "batch:  440\n",
            "batch:  441\n",
            "batch:  442\n",
            "batch:  443\n",
            "batch:  444\n",
            "batch:  445\n",
            "batch:  446\n",
            "batch:  447\n",
            "batch:  448\n",
            "batch:  449\n",
            "batch:  450\n",
            "batch:  451\n",
            "batch:  452\n",
            "batch:  453\n",
            "batch:  454\n",
            "batch:  455\n",
            "batch:  456\n",
            "batch:  457\n",
            "batch:  458\n",
            "batch:  459\n",
            "batch:  460\n",
            "batch:  461\n",
            "batch:  462\n",
            "batch:  463\n",
            "batch:  464\n",
            "batch:  465\n",
            "batch:  466\n",
            "batch:  467\n",
            "batch:  468\n",
            "batch:  469\n",
            "batch:  470\n",
            "batch:  471\n",
            "batch:  472\n",
            "batch:  473\n",
            "batch:  474\n",
            "batch:  475\n",
            "batch:  476\n",
            "batch:  477\n",
            "batch:  478\n",
            "batch:  479\n",
            "batch:  480\n",
            "batch:  481\n",
            "batch:  482\n",
            "batch:  483\n",
            "batch:  484\n",
            "batch:  485\n",
            "batch:  486\n",
            "batch:  487\n",
            "batch:  488\n",
            "batch:  489\n",
            "batch:  490\n",
            "batch:  491\n",
            "batch:  492\n",
            "batch:  493\n",
            "batch:  494\n",
            "batch:  495\n",
            "batch:  496\n",
            "batch:  497\n",
            "batch:  498\n",
            "batch:  499\n",
            "batch:  500\n",
            "batch:  501\n",
            "batch:  502\n",
            "batch:  503\n",
            "batch:  504\n",
            "batch:  505\n",
            "batch:  506\n",
            "batch:  507\n",
            "batch:  508\n",
            "batch:  509\n",
            "batch:  510\n",
            "batch:  511\n",
            "batch:  512\n",
            "batch:  513\n",
            "batch:  514\n",
            "batch:  515\n",
            "batch:  516\n",
            "batch:  517\n",
            "batch:  518\n",
            "batch:  519\n",
            "batch:  520\n",
            "batch:  521\n",
            "batch:  522\n",
            "batch:  523\n",
            "batch:  524\n",
            "batch:  525\n",
            "batch:  526\n",
            "batch:  527\n",
            "batch:  528\n",
            "batch:  529\n",
            "batch:  530\n",
            "batch:  531\n",
            "batch:  532\n",
            "batch:  533\n",
            "batch:  534\n",
            "batch:  535\n",
            "batch:  536\n",
            "batch:  537\n",
            "batch:  538\n",
            "batch:  539\n",
            "batch:  540\n",
            "batch:  541\n",
            "batch:  542\n",
            "batch:  543\n",
            "batch:  544\n",
            "batch:  545\n",
            "batch:  546\n",
            "batch:  547\n",
            "batch:  548\n",
            "batch:  549\n",
            "batch:  550\n",
            "batch:  551\n",
            "batch:  552\n",
            "batch:  553\n",
            "batch:  554\n",
            "batch:  555\n",
            "batch:  556\n",
            "batch:  557\n",
            "batch:  558\n",
            "batch:  559\n",
            "batch:  560\n",
            "batch:  561\n",
            "batch:  562\n",
            "batch:  563\n",
            "batch:  564\n",
            "batch:  565\n",
            "batch:  566\n",
            "batch:  567\n",
            "batch:  568\n",
            "batch:  569\n",
            "batch:  570\n",
            "batch:  571\n",
            "batch:  572\n",
            "batch:  573\n",
            "batch:  574\n",
            "batch:  575\n",
            "batch:  576\n",
            "batch:  577\n",
            "batch:  578\n",
            "batch:  579\n",
            "batch:  580\n",
            "batch:  581\n",
            "batch:  582\n",
            "batch:  583\n",
            "batch:  584\n",
            "batch:  585\n",
            "batch:  586\n",
            "batch:  587\n",
            "batch:  588\n",
            "batch:  589\n",
            "batch:  590\n",
            "batch:  591\n",
            "batch:  592\n",
            "batch:  593\n",
            "batch:  594\n",
            "batch:  595\n",
            "batch:  596\n",
            "batch:  597\n",
            "batch:  598\n",
            "batch:  599\n",
            "batch:  600\n",
            "batch:  601\n",
            "batch:  602\n",
            "batch:  603\n",
            "batch:  604\n",
            "batch:  605\n",
            "batch:  606\n",
            "batch:  607\n",
            "batch:  608\n",
            "batch:  609\n",
            "batch:  610\n",
            "batch:  611\n",
            "batch:  612\n",
            "batch:  613\n",
            "batch:  614\n",
            "batch:  615\n",
            "batch:  616\n",
            "batch:  617\n",
            "batch:  618\n",
            "batch:  619\n",
            "batch:  620\n",
            "batch:  621\n",
            "batch:  622\n",
            "batch:  623\n",
            "batch:  624\n",
            "batch:  625\n",
            "batch:  626\n",
            "batch:  627\n",
            "batch:  628\n",
            "batch:  629\n",
            "batch:  630\n",
            "batch:  631\n",
            "batch:  632\n",
            "batch:  633\n",
            "batch:  634\n",
            "batch:  635\n",
            "batch:  636\n",
            "batch:  637\n",
            "batch:  638\n",
            "batch:  639\n",
            "batch:  640\n",
            "batch:  641\n",
            "batch:  642\n",
            "batch:  643\n",
            "batch:  644\n",
            "batch:  645\n",
            "batch:  646\n",
            "batch:  647\n",
            "batch:  648\n",
            "batch:  649\n",
            "batch:  650\n",
            "batch:  651\n",
            "batch:  652\n",
            "batch:  653\n",
            "batch:  654\n",
            "batch:  655\n",
            "batch:  656\n",
            "batch:  657\n",
            "batch:  658\n",
            "batch:  659\n",
            "batch:  660\n",
            "batch:  661\n",
            "batch:  662\n",
            "batch:  663\n",
            "batch:  664\n",
            "batch:  665\n",
            "batch:  666\n",
            "batch:  667\n",
            "batch:  668\n",
            "batch:  669\n",
            "batch:  670\n",
            "batch:  671\n",
            "batch:  672\n",
            "batch:  673\n",
            "batch:  674\n",
            "batch:  675\n",
            "batch:  676\n",
            "batch:  677\n",
            "batch:  678\n",
            "batch:  679\n",
            "batch:  680\n",
            "batch:  681\n",
            "batch:  682\n",
            "batch:  683\n",
            "batch:  684\n",
            "batch:  685\n",
            "batch:  686\n",
            "batch:  687\n",
            "batch:  688\n",
            "batch:  689\n",
            "batch:  690\n",
            "batch:  691\n",
            "batch:  692\n",
            "batch:  693\n",
            "batch:  694\n",
            "batch:  695\n",
            "batch:  696\n",
            "batch:  697\n",
            "batch:  698\n",
            "batch:  699\n",
            "batch:  700\n",
            "batch:  701\n",
            "batch:  702\n",
            "batch:  703\n",
            "batch:  704\n",
            "batch:  705\n",
            "batch:  706\n",
            "batch:  707\n",
            "batch:  708\n",
            "batch:  709\n",
            "batch:  710\n",
            "batch:  711\n",
            "batch:  712\n",
            "batch:  713\n",
            "batch:  714\n",
            "batch:  715\n",
            "batch:  716\n",
            "batch:  717\n",
            "batch:  718\n",
            "batch:  719\n",
            "batch:  720\n",
            "batch:  721\n",
            "batch:  722\n",
            "batch:  723\n",
            "batch:  724\n",
            "batch:  725\n",
            "batch:  726\n",
            "batch:  727\n",
            "batch:  728\n",
            "batch:  729\n",
            "batch:  730\n",
            "batch:  731\n",
            "batch:  732\n",
            "batch:  733\n",
            "batch:  734\n",
            "batch:  735\n",
            "batch:  736\n",
            "batch:  737\n",
            "batch:  738\n",
            "batch:  739\n",
            "batch:  740\n",
            "batch:  741\n",
            "batch:  742\n",
            "batch:  743\n",
            "batch:  744\n",
            "batch:  745\n",
            "batch:  746\n",
            "batch:  747\n",
            "batch:  748\n",
            "batch:  749\n",
            "batch:  750\n",
            "batch:  751\n",
            "batch:  752\n",
            "batch:  753\n",
            "batch:  754\n",
            "batch:  755\n",
            "batch:  756\n",
            "batch:  757\n",
            "batch:  758\n",
            "batch:  759\n",
            "batch:  760\n",
            "batch:  761\n",
            "batch:  762\n",
            "batch:  763\n",
            "batch:  764\n",
            "batch:  765\n",
            "batch:  766\n",
            "batch:  767\n",
            "batch:  768\n",
            "batch:  769\n",
            "batch:  770\n",
            "batch:  771\n",
            "batch:  772\n",
            "batch:  773\n",
            "batch:  774\n",
            "batch:  775\n",
            "batch:  776\n",
            "batch:  777\n",
            "batch:  778\n",
            "batch:  779\n",
            "batch:  780\n",
            "batch:  781\n",
            "batch:  782\n",
            "batch:  783\n",
            "batch:  784\n",
            "batch:  785\n",
            "batch:  786\n",
            "batch:  787\n",
            "batch:  788\n",
            "batch:  789\n",
            "batch:  790\n",
            "batch:  791\n",
            "batch:  792\n",
            "batch:  793\n",
            "batch:  794\n",
            "batch:  795\n",
            "batch:  796\n",
            "batch:  797\n",
            "batch:  798\n",
            "batch:  799\n",
            "batch:  800\n",
            "batch:  801\n",
            "batch:  802\n",
            "batch:  803\n",
            "batch:  804\n",
            "batch:  805\n",
            "batch:  806\n",
            "batch:  807\n",
            "batch:  808\n",
            "batch:  809\n",
            "batch:  810\n",
            "batch:  811\n",
            "batch:  812\n",
            "batch:  813\n",
            "batch:  814\n",
            "batch:  815\n",
            "batch:  816\n",
            "batch:  817\n",
            "batch:  818\n",
            "batch:  819\n",
            "batch:  820\n",
            "batch:  821\n",
            "batch:  822\n",
            "batch:  823\n",
            "batch:  824\n",
            "batch:  825\n",
            "batch:  826\n",
            "batch:  827\n",
            "batch:  828\n",
            "batch:  829\n",
            "batch:  830\n",
            "batch:  831\n",
            "batch:  832\n",
            "batch:  833\n",
            "batch:  834\n",
            "batch:  835\n",
            "batch:  836\n",
            "batch:  837\n",
            "batch:  838\n",
            "batch:  839\n",
            "batch:  840\n",
            "batch:  841\n",
            "batch:  842\n",
            "batch:  843\n",
            "batch:  844\n",
            "batch:  845\n",
            "batch:  846\n",
            "batch:  847\n",
            "batch:  848\n",
            "batch:  849\n",
            "batch:  850\n",
            "batch:  851\n",
            "batch:  852\n",
            "batch:  853\n",
            "batch:  854\n",
            "batch:  855\n",
            "batch:  856\n",
            "batch:  857\n",
            "batch:  858\n",
            "batch:  859\n",
            "batch:  860\n",
            "batch:  861\n",
            "batch:  862\n",
            "batch:  863\n",
            "batch:  864\n",
            "batch:  865\n",
            "batch:  866\n",
            "batch:  867\n",
            "batch:  868\n",
            "batch:  869\n",
            "batch:  870\n",
            "batch:  871\n",
            "batch:  872\n",
            "batch:  873\n",
            "batch:  874\n",
            "batch:  875\n",
            "batch:  876\n",
            "batch:  877\n",
            "batch:  878\n",
            "batch:  879\n",
            "batch:  880\n",
            "batch:  881\n",
            "batch:  882\n",
            "batch:  883\n",
            "batch:  884\n",
            "batch:  885\n",
            "batch:  886\n",
            "batch:  887\n",
            "batch:  888\n",
            "batch:  889\n",
            "batch:  890\n",
            "batch:  891\n",
            "batch:  892\n",
            "batch:  893\n",
            "batch:  894\n",
            "batch:  895\n",
            "batch:  896\n",
            "batch:  897\n",
            "batch:  898\n",
            "batch:  899\n",
            "batch:  900\n",
            "batch:  901\n",
            "batch:  902\n",
            "batch:  903\n",
            "batch:  904\n",
            "batch:  905\n",
            "batch:  906\n",
            "batch:  907\n",
            "batch:  908\n",
            "batch:  909\n",
            "batch:  910\n",
            "batch:  911\n",
            "batch:  912\n",
            "batch:  913\n",
            "batch:  914\n",
            "batch:  915\n",
            "batch:  916\n",
            "batch:  917\n",
            "batch:  918\n",
            "batch:  919\n",
            "batch:  920\n",
            "batch:  921\n",
            "batch:  922\n",
            "batch:  923\n",
            "batch:  924\n",
            "batch:  925\n",
            "batch:  926\n",
            "batch:  927\n",
            "batch:  928\n",
            "batch:  929\n",
            "batch:  930\n",
            "[1 0 0 ... 1 0 1]\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "trained_net = train(net, criterion, optimizer, 5, train_loader, val_loader, test_loader)"
      ],
      "id": "8b7f8cdf"
    },
    {
      "cell_type": "code",
      "source": [
        "trained_net.eval()"
      ],
      "metadata": {
        "id": "1LEvesrMqQac"
      },
      "id": "1LEvesrMqQac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# load model from checkpoint\n",
        "model_loaded = TripletNet(FeatureExtractNET())\n",
        "checkpoint = torch.load('/content/drive/MyDrive/test3/model_epoch_5.pt')\n",
        "model_loaded.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "model_loaded.to(device)\n",
        "model_loaded.eval()"
      ],
      "metadata": {
        "id": "8uUh7Yk99ecP"
      },
      "id": "8uUh7Yk99ecP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFZPwoPglJFH"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  valloader_iterator = iter(train_loader)\n",
        "  mean_accuracy = val_accuracy(model_loaded, train_loader, valloader_iterator)\n",
        "  print(mean_accuracy)"
      ],
      "id": "dFZPwoPglJFH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB9aCp_jT2OH"
      },
      "outputs": [],
      "source": [
        "print('================== START PREDICTION ==================')\n",
        "\n",
        "# Change to evaluation mode\n",
        "model_loaded.eval()\n",
        "\n",
        "redicted_labels = np.zeros(59544)\n",
        "pred_test=[]\n",
        "\n",
        "#Predict labels 1 or 0 for each test triplet\n",
        "for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
        "\n",
        "    data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
        "\n",
        "    # wrap in torch.autograd.Variable\n",
        "    data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # compute output and loss\n",
        "        embedded_x, embedded_y, embedded_z = trained_net(data1, data2, data3)\n",
        "\n",
        "    dist_a = F.pairwise_distance(embedded_x, embedded_y, 2)\n",
        "    dist_b = F.pairwise_distance(embedded_x, embedded_z, 2)\n",
        "    #print(np.squeeze(embedded_a.cpu().detach().numpy()).shape)\n",
        "    \n",
        "\n",
        "    pred_test.append(1*(dist_a <= dist_b))\n",
        "\n",
        "\n",
        "    print('batch: ', batch_idx)"
      ],
      "id": "iB9aCp_jT2OH"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_np = []\n",
        "for i in range(len(pred_test)):\n",
        "  pred_test_cpu = pred_test[i].cpu().detach().numpy()\n",
        "  pred_test_np += list(pred_test_cpu)\n",
        "len(pred_test_np)\n",
        "predicted_labels = np.hstack(pred_test_np)\n",
        "print(predicted_labels)\n",
        "\n",
        "#Write submisison file\n",
        "df = pd.DataFrame(predicted_labels)\n",
        "df.to_csv('/content/drive/MyDrive/test3/submission_1epoch.txt', index=False, header=None) #write CSV"
      ],
      "metadata": {
        "id": "eqMINu0rwiv2"
      },
      "id": "eqMINu0rwiv2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = len(open(\"/content/drive/MyDrive/test3/submission_1epoch.txt\",'rU').readlines())\n",
        "print(count)"
      ],
      "metadata": {
        "id": "rc5c6182Do0U"
      },
      "id": "rc5c6182Do0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nq4tku-jQRuh"
      },
      "id": "nq4tku-jQRuh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Task 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}